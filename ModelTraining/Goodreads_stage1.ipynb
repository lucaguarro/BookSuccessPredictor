{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Goodreads_stage1.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["lq8akyvCLgHA","tI83z4OAhYpi"],"mount_file_id":"17P7vkejHtBSgHb_vbRCYK_xTCVffMWId","authorship_tag":"ABX9TyOTnDgEnXxYdHs5Ja4ipDFL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gp5L_VXbtxAi"},"source":["# Installs, Imports, Drive Connection, WandB Connection"]},{"cell_type":"markdown","metadata":{"id":"fTDqhCRUt3HT"},"source":["##### Installs"]},{"cell_type":"code","metadata":{"id":"MYJcwbartY2A"},"source":["!pip install transformers==4.10.0\n","# !pip install git+https://github.com/huggingface/transformers.git\n","!pip install datasets==1.9.0\n","!pip install -U PyYAML\n","!pip install \"ray[default]\"\n","!pip install wandb\n","!pip install tensorboardX"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCsyqPset6dy"},"source":["##### Imports"]},{"cell_type":"code","metadata":{"id":"CqYDmipPt5vR"},"source":["import sys\n","import os\n","import numpy as np\n","# from transformers import pipeline\n","from datasets import concatenate_datasets, load_dataset\n","import torch\n","from pathlib import Path\n","import pickle\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from transformers import Trainer, TrainingArguments\n","from scipy.special import softmax\n","from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","import math\n","from transformers import AutoModelForSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wQn6xfRzRhH"},"source":["import os\n","import pickle\n","import numpy as np\n","\n","import ray\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import PopulationBasedTraining, ASHAScheduler\n","from transformers import Trainer, TrainingArguments\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLoggerCallback, WandbLogger\n","\n","from transformers import DistilBertConfig\n","\n","from ray.tune.integration.wandb import WandbLoggerCallback\n","# from MultiTaskExtensions import DistilBERTForMultipleSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlNbZBmyt_FS"},"source":["##### Drive Connection"]},{"cell_type":"code","metadata":{"id":"9jEJvuPit9vd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUtmDSYEfQ0M"},"source":["##### Get Configuration"]},{"cell_type":"code","metadata":{"id":"zMYGmKuqfTK5"},"source":["import configparser\n","import sys\n","from pathlib import Path\n","\n","config = configparser.ConfigParser()\n","config.read('/content/drive/MyDrive/Thesis/BookSuccessPredictor/config.ini')\n","\n","drive_base_path = Path(config['Drive']['drive_base_path'])\n","\n","sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / '_utils'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BovuphPCqcPU"},"source":["print(\"Using Model:\", config['Model']['name'])\n","print(\"With NERed Dataset:\", config['Model']['use_ner'])\n","print(\"In multi-task setting:\", config['Model']['multi_task'])\n","print(\"Using overlap tokenizer:\", config['Tokenizer']['overlap'])\n","print(\"Using pretrained model:\", config['WandB']['use_WandB_pretrained'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ftr_j828j4Sc"},"source":["##### WandB Connection"]},{"cell_type":"code","metadata":{"id":"TK_BtgJrj8-F"},"source":["# saves our models to artifacts in WandB\n","import wandb\n","%env WANDB_LOG_MODEL=true\n","%env WANDB_PROJECT=goodreads_success_predictor_80_20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDrXDpMIbyct"},"source":["wandb.login(key = config['WandB']['api_key'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lyW-LH6nvHBY"},"source":["# Dataset Generator"]},{"cell_type":"markdown","metadata":{"id":"saQOgEd7hjWf"},"source":["### Load Text Data"]},{"cell_type":"markdown","metadata":{"id":"Z-L4TuE8g8VC"},"source":["#### goodreads_maharjan"]},{"cell_type":"code","metadata":{"id":"_GU2ZkecvIlY"},"source":["base_path = Path(config['Datasets']['nered_goodreads_maharjan_path'])\n","dataset = load_dataset(str(base_path / 'goodreadsnered.py'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0yvGFjMhAbX"},"source":["#### goodreads_guarro"]},{"cell_type":"code","metadata":{"id":"i_kWFv5jhCpV"},"source":["base_path = config['Datasets']['nered_goodreads_guarro_path']\n","dataset = load_dataset(base_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVtdWBx0h09i"},"source":["### Custom Tokenization Process"]},{"cell_type":"markdown","metadata":{"id":"xAAeka1DNAV9"},"source":["##### Get Tokenizer"]},{"cell_type":"code","metadata":{"id":"cUZHpyH4JScW"},"source":["from transformers import AutoTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owH2-U4-JYzw"},"source":["print(\"Using tokenizer of \", config['Model']['name'])\n","if eval(config['Model']['use_ner']):\n","  print(\"adding special token for [CHARACTER]\")\n","  # tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base', additional_special_tokens = ['[CHARACTER]'])\n","  tokenizer = AutoTokenizer.from_pretrained(config['Model']['name'], additional_special_tokens = ['[CHARACTER]'])\n","else:\n","  tokenizer = AutoTokenizer.from_pretrained(config['Model']['name'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2k2gNmd0V5k"},"source":["##### Do actual tokenizing and uploading"]},{"cell_type":"code","metadata":{"id":"yfqceoNYm9Xa"},"source":["from tokenization_algos import chunk_and_encode_examples_w_complete_sentences, chunk_and_encode_examples_w_overlap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfGdetaTjWtp"},"source":["from functools import partial\n","encode_algo = partial(chunk_and_encode_examples_w_overlap, tokenizer=tokenizer, stride=384)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tfRMxPspRIT"},"source":["chunked_encoded_dataset = dataset.map(encode_algo, remove_columns=dataset.column_names['train'], batched = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmq3ttP2xdtR"},"source":["When uploading the tokenized datasets to Drive, we may need to break them up into as many pieces as is necessary. Otherwise the serialization and uploading fails. In my case I had to split each subset (train, val, test) into 2 parts. If using tokenization with complete sentences, we can usually avoid this hack. Otherwise with overlap, the dataset will most likely be too large and this trick may be necessary."]},{"cell_type":"code","metadata":{"id":"5GkA1_OPm_uH"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfIFipREwjkZ"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('train_dataset1.pkl', 'wb') as output_file:\n","  pickle.dump(chunked_encoded_dataset['train'][0:chunked_encoded_dataset['train'].num_rows//2], output_file)\n","\n","with open('train_dataset2.pkl', 'wb') as output_file:\n","  pickle.dump(chunked_encoded_dataset['train'][chunked_encoded_dataset['train'].num_rows//2:chunked_encoded_dataset['train'].num_rows], output_file)\n","\n","with open('val_dataset1.pkl', 'wb') as output_file:\n","  pickle.dump(chunked_encoded_dataset['validation'][0:chunked_encoded_dataset['validation'].num_rows//2], output_file)\n","\n","with open('val_dataset2.pkl', 'wb') as output_file:\n","  pickle.dump(chunked_encoded_dataset['validation'][chunked_encoded_dataset['validation'].num_rows//2:chunked_encoded_dataset['validation'].num_rows], output_file)\n","\n","with open('test_dataset1.pkl', 'wb') as output_file:\n","  pickle.dump(chunked_encoded_dataset['test'][0:chunked_encoded_dataset['test'].num_rows//2], output_file)\n","\n","with open('test_dataset2.pkl', 'wb') as output_file:\n","  pickle.dump(chunked_encoded_dataset['test'][chunked_encoded_dataset['test'].num_rows//2:chunked_encoded_dataset['test'].num_rows], output_file)\n","\n","folder_id = '1vQKkl_9SeshGXavfz4fcgiPtmfks5vx9'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_dataset1.pkl')\n","file.Upload() \n","\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_dataset2.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_dataset1.pkl')\n","file.Upload() \n","\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_dataset2.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_dataset1.pkl')\n","file.Upload() \n","\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_dataset2.pkl')\n","file.Upload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBZgml9JiIil"},"source":["#### Loading"]},{"cell_type":"code","metadata":{"id":"-I2ys-gbOvXn"},"source":["load_path = Path(config['Drive']['drive_base_path']) / 'BookSuccessPredictor' / 'datasets' / 'goodreads_maharjan_super' / 'already_tokenized'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyHsdlQBD-vE"},"source":["if config['Datasets']['split'] == '80_20':\n","  load_path = load_path / '80_20'\n","else:\n","  load_path = load_path / '60_40'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ehOdQnyPiq5"},"source":["if (config['Model']['name'] == 'albert-base-v2'):\n","  load_path = load_path / 'ALBERT_NER_512'\n","\n","elif (config['Model']['name'] == 'bert-base-uncased'):\n","  if (config['Tokenizer']['max_len'] == '512'):\n","    load_path = load_path / 'BERT_UNCASED_NER_512'\n","  elif (config['Tokenizer']['max_len'] == '256'):\n","    load_path = load_path / 'BERT_UNCASED_NER_256'\n","\n","elif (config['Model']['name'] == 'distilbert-base-uncased'):\n","  if (eval(config['Tokenizer']['overlap'])):\n","    load_path = load_path / 'DistilBERT_UNCASED_NER_512_w50overlap'\n","  else:\n","    load_path = load_path / 'DistilBERT_UNCASED_NER_512'\n","\n","elif (config['Model']['name'] == 'microsoft/deberta-base'):\n","  load_path = load_path / 'DeBERTa'\n","\n","elif (config['Model']['name'] == 'roberta-base'):\n","  load_path = load_path / 'ROBERTA_NER_512'\n","\n","elif (config['Model']['name'] == 'google/bigbird-roberta-base'):\n","  load_path = load_path / 'BIGBIRD_NER_4096'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nppRfvDyZGof"},"source":["load_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHXEWzPkUHo-"},"source":["from datasets import DatasetDict, Dataset, concatenate_datasets\n","train_paths = [f for f in os.listdir(load_path) if f.startswith('train')]\n","val_paths = [f for f in os.listdir(load_path) if f.startswith('val')]\n","test_paths = [f for f in os.listdir(load_path) if f.startswith('test')]\n","\n","train_datasets = []\n","val_datasets = []\n","test_datasets = []\n","\n","for trainp in train_paths:\n","  with open(load_path / trainp, \"rb\") as input_file:\n","    train_datasets.append(Dataset.from_dict(pickle.load(input_file)))\n","\n","for valp in val_paths:\n","  with open(load_path / valp, \"rb\") as input_file:\n","    val_datasets.append(Dataset.from_dict(pickle.load(input_file)))\n","\n","for testp in test_paths:\n","  with open(load_path / testp, \"rb\") as input_file:\n","    test_datasets.append(Dataset.from_dict(pickle.load(input_file)))\n","\n","train_dataset = concatenate_datasets(train_datasets)\n","del train_datasets\n","\n","val_dataset = concatenate_datasets(val_datasets)\n","del val_datasets\n","\n","test_dataset = concatenate_datasets(test_datasets)\n","del test_datasets\n","\n","chunked_encoded_dataset = DatasetDict({'train': train_dataset, 'validation': val_dataset, 'test': test_dataset})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mcFeMQgLEOP"},"source":["if (eval(config['Model']['multi_task'])):\n","  print('multitask')\n","  # When batched = True, we take in multiple examples\n","  def group_success_and_genre(examples):\n","    examples['labels'] = np.vstack((examples['success_label'], examples['genre'])).T\n","    return examples\n","\n","  chunked_encoded_dataset = chunked_encoded_dataset.map(group_success_and_genre, batched = True, remove_columns=['genre', 'success_label'])\n","else:\n","  print('single task')\n","  chunked_encoded_dataset = chunked_encoded_dataset.rename_column('success_label', 'labels')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNtzoZdGJKF7"},"source":["#Dataset Exploration"]},{"cell_type":"code","metadata":{"id":"m35LUkPSJmNS"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlDjDuXSORXo"},"source":["import matplotlib.pyplot as plt\n","import itertools\n","\n","num_segments_per_book = [len(list(g[1])) for g in itertools.groupby(chunked_encoded_dataset['train']['book_title'])]\n","\n","plt.hist(num_segments_per_book, density=True, bins=30)  # density=False would make counts\n","plt.ylabel('Probability')\n","plt.xlabel('Data');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1TsnXeKJqzz"},"source":["# [(g[0], len(list(g[1]))) for g in itertools.groupby(chunked_encoded_dataset['train']['book_title'])]\n","start_of_segmented_book = {}\n","last_idx = 0\n","for g in itertools.groupby(chunked_encoded_dataset['train']['book_title']):\n","  start_of_segmented_book[g[0]] = last_idx\n","  last_idx = len(list(g[1])) + last_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZf01knMQxBC"},"source":["# When batched = True, we take in multiple examples\n","def filter_segments(example, idx):\n","  if (idx - start_of_segmented_book[example['book_title']] < 25):\n","    return True\n","  else:\n","    return False\n","\n","test = chunked_encoded_dataset['train'].filter(filter_segments, with_indices = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKhzZkSfR9Bk"},"source":["num_segments_per_book = [len(list(g[1])) for g in itertools.groupby(test['book_title'])]\n","\n","plt.hist(num_segments_per_book, density=True, bins=30)  # density=False would make counts\n","plt.ylabel('Probability')\n","plt.xlabel('Data');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYj8PW1JSedZ"},"source":["chunked_encoded_dataset['train'] = chunked_encoded_dataset['train'].filter(filter_segments, with_indices = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5umzDNpkQuo"},"source":["# GoodReads Success Prediction"]},{"cell_type":"markdown","metadata":{"id":"a3G544UU9O2O"},"source":["## Transformer --> Classification"]},{"cell_type":"code","metadata":{"id":"Z0YL3VwHIWzf"},"source":["# db_config_base = DistilBertConfig.from_pretrained('/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/DistilBertPretrained')\n","# # db_config_base.update({'_name_or_path': '/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/DistilBertPretrained', 'alpha': 0.2, 'dropout': 0.8})\n","# return DistilBERTForMultipleSequenceClassification.from_pretrained(pretrained_model_name_or_path = db_config_base._name_or_path, config = db_config_base)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAlIUr2KUlhm"},"source":["from MultiTaskExtensions import DistilBERTForMultipleSequenceClassification\n","from transformers import DistilBertConfig\n","# pretrained_model_name_or_path = config['Model']['name']\n","# pretrained_model_name_or_path = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/DistilBertPretrained'\n","pretrained_model_name_or_path = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/3vvi0uoqDistilBertModel'\n","\n","# if (eval(config['WandB']['use_WandB_pretrained'])):\n","#   run = wandb.init()\n","#   artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-nlpbosie:v0', type='model')\n","#   # artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-3d16pz7v:v0', type='model')\n","#   pretrained_model_name_or_path = artifact.download()\n","\n","if (eval(config['Model']['multi_task'])):\n","  metric_for_best_model = 'eval_s_f1'\n","  if (config['Model']['name'] == 'distilbert-base-uncased'):\n","    from MultiTaskExtensions import DistilBERTForMultipleSequenceClassification\n","    db_config = DistilBertConfig.from_pretrained(pretrained_model_name_or_path)\n","    db_config.update({'_name_or_path': pretrained_model_name_or_path, 'alpha': 0.5928, 'attention_dropout': 0.2436, 'dropout': 0.3}) # 'dropout': 0.3877\n","    print(db_config)\n","    model = DistilBERTForMultipleSequenceClassification.from_pretrained(pretrained_model_name_or_path = db_config._name_or_path, config = db_config)\n","  else:\n","    from MultiTaskExtensions import BertForMultipleSequenceClassification\n","    model = BertForMultipleSequenceClassification.from_pretrained(pretrained_model_name_or_path)\n","\n","else:\n","  print(\"standard model\", pretrained_model_name_or_path)\n","  metric_for_best_model = 'eval_f1'\n","  model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)\n","  model.resize_token_embeddings(len(tokenizer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LtotUtaaeNI-"},"source":["model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KPJrDBdWEaV8"},"source":["# from scores import compute_metrics_multi, compute_metrics_single\n","# if eval(config['Model']['multi_task']):\n","#   print('multi')\n","#   # from scores import compute_metrics_multi\n","#   compute_metrics = compute_metrics_multi\n","# else:\n","#   print('single')\n","#   # from scores import compute_metrics_single\n","#   compute_metrics = compute_metrics_single"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_L9eX3a6NBS"},"source":["%env WANDB_PROJECT=goodreads_success_predictor_80_20\n","\n","training_args = TrainingArguments(\n","    'gsp_80_20_DistilBERT_25segs_wtfwillthiswork',\n","    num_train_epochs=1,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    learning_rate=0.00003969,\n","    per_device_eval_batch_size=32,   # batch size for evaluation\n","    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.1,               # strength of weight decay\n","    # logging_dir='./logs',            # directory for storing logs\n","    logging_steps=5,\n","    # gradient_accumulation_steps=2,\n","    evaluation_strategy = \"steps\",\n","    eval_steps = 5, # prob better if set to 601 that way it evenly divides into the epochs\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'eval_s_f1',\n","    greater_is_better = True,\n","    # report_to = \"wandb\",\n","    save_total_limit = 5\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=chunked_encoded_dataset['train'],         # training dataset             # evaluation dataset\n","    eval_dataset=chunked_encoded_dataset['validation'], \n","    compute_metrics = compute_metrics\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpBjjjgA9oEM"},"source":["chunked_encoded_dataset['train']['labels'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lq8akyvCLgHA"},"source":["## Scorer"]},{"cell_type":"code","metadata":{"id":"bHVlCz50L9Q-"},"source":["from MultiTaskExtensions import DistilBERTForMultipleSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrSQd1b-ELB3"},"source":["run = wandb.init()\n","# artifact = run.use_artifact('lucaguarro/DistilbertMultitaskHPSearch/model-3igwy2id:v0', type='model')\n","artifact = run.use_artifact('lucaguarro/DistilbertMultitaskHPSearch/model-3vvi0uoq:v0', type='model')\n","artifact_dir = artifact.download()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWM8FcKeL5Dq"},"source":["model = DistilBERTForMultipleSequenceClassification.from_pretrained(artifact_dir)\n","trainer = Trainer(\n","    model=model\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adteyeUzMQS4"},"source":["from scores import ModelScorer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33qPNFMTMVUr"},"source":["m_scorer = ModelScorer(trainer, chunked_encoded_dataset, for_multitask=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdWwDnkNMi1D"},"source":["m_scorer.get_segmented_f1_scores()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ApGSxaK_IUdR"},"source":["m_scorer.get_book_f1_scores()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2FCxHJtLFUOP"},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"2hd8wn9jdjfV"},"source":["multitask hyperparameter tuning"]},{"cell_type":"code","metadata":{"id":"fwq2YV_dqo5j"},"source":["# saves our models to artifacts in WandB\n","%env WANDB_LOG_MODEL=true\n","wandb.login()\n","%env WANDB_PROJECT=DistilbertMultitaskHPSearch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgFdAxweCbfj"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IObtn-HBeB68"},"source":["def compute_metrics_single(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","def compute_metrics_multi(pred):\n","    preds = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    success_labels = label_ids[:, 0]\n","    genre_labels = label_ids[:, 1]\n","\n","    success_preds = preds[:, 0:2].argmax(1)\n","    genre_preds = preds[:, 2:6].argmax(1)\n","\n","    s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(success_labels, success_preds, average='weighted')\n","    s_acc = accuracy_score(success_labels, success_preds)\n","\n","    g_precision, g_recall, g_f1, _ = precision_recall_fscore_support(genre_labels, genre_preds, average='weighted')\n","    g_acc = accuracy_score(success_labels, success_preds)\n","\n","    return {\n","        's_accuracy': s_acc,\n","        's_f1': s_f1,\n","        's_precision': s_precision,\n","        's_recall': s_recall,\n","        'g_accuracy': g_acc,\n","        'g_f1': g_f1,\n","        'g_precision': g_precision,\n","        'g_recall': g_recall\n","    }\n","\n","if eval(config['Model']['multi_task']):\n","  print('multi')\n","  # from scores import compute_metrics_multi\n","  compute_metrics = compute_metrics_multi\n","else:\n","  print('single')\n","  # from scores import compute_metrics_single\n","  compute_metrics = compute_metrics_single\n","\n","def my_objective(metrics):\n","    # Your elaborate computation here\n","    if eval(config['Model']['multi_task']):\n","      return metrics['eval_s_f1']\n","    else:\n","      return metrics['eval_f1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cgd-ZnB6VFv8"},"source":["from transformers import DistilBertConfig\n","db_config_base = DistilBertConfig.from_pretrained('/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/DistilBertPretrained')\n","db_config_base.update({'_name_or_path': '/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/DistilBertPretrained', 'alpha': 0.2, 'dropout': 0.8})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRbq_rU8Rovj"},"source":["db_config_base"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqQMbfUM1mmK"},"source":["test = DistilBERTForMultipleSequenceClassification.from_pretrained(pretrained_model_name_or_path = db_config_base._name_or_path, config = db_config_base)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zG1m6btWJ2jg"},"source":["# tune_config_pop_based = {\n","#     \"per_device_train_batch_size\": 16,\n","#     \"per_device_eval_batch_size\": 32,\n","#     \"num_train_epochs\": 1,\n","#     \"max_steps\": 1 if smoke_test else -1,  # Used for smoke test.\n","#     \"wandb\": {\n","#         \"project\": \"DistilbertMultitaskHPSearch\",\n","#         \"group\": \"Search1\",\n","#         \"api_key\": \"XXXXXXXX\",\n","#         \"log_config\": True\n","#     }\n","# }\n","\n","# scheduler = PopulationBasedTraining(\n","#     time_attr=\"training_iteration\",\n","#     metric=\"eval_s_f1\",\n","#     mode=\"max\",\n","#     perturbation_interval=60,\n","#     hyperparam_mutations={\n","#         \"weight_decay\": tune.uniform(0.0, 0.3),\n","#         \"learning_rate\": tune.uniform(1e-5, 5e-5),\n","#         \"per_device_train_batch_size\": [16],\n","#     })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPueEVqH18lD"},"source":["from transformers.modeling_outputs import SequenceClassifierOutput\n","from torch import nn\n","import torch\n","from torch.nn import CrossEntropyLoss, MSELoss\n","\n","from transformers import DistilBertPreTrainedModel, DistilBertModel\n","class DistilBERTForMultipleSequenceClassification(DistilBertPreTrainedModel):\n","    def __init__(self, config, num_labels1 = 2, num_labels2 = 8):\n","        super().__init__(config)\n","        self.num_labels1 = num_labels1\n","        self.num_labels2 = num_labels2\n","        print(self.num_labels1, self.num_labels2)\n","        self.alpha = config.alpha\n","        self.config = config\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier1 = nn.Linear(config.dim, self.num_labels1)\n","        self.classifier2 = nn.Linear(config.dim, self.num_labels2)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n","            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n","            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        distilbert_output = self.distilbert(\n","              input_ids=input_ids,\n","              attention_mask=attention_mask,\n","              head_mask=head_mask,\n","              inputs_embeds=inputs_embeds,\n","              output_attentions=output_attentions,\n","              output_hidden_states=output_hidden_states,\n","              return_dict=return_dict,\n","          )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits1 = self.classifier1(pooled_output)\n","        logits2 = self.classifier2(pooled_output)\n","        logits = torch.cat([logits1, logits2], 1)\n","\n","        loss = None\n","        if labels is not None:\n","            #if self.config.problem_type is None:\n","            #self.config.problem_type = \"single_label_classification\"\n","            \n","            if self.num_labels1 > 1:\n","                loss_fct1 = CrossEntropyLoss()\n","                loss1 = loss_fct1(logits1.view(-1, self.num_labels1), labels[:, 0].view(-1))\n","            else:\n","                loss_fct1 = MSELoss()\n","                loss1 = loss_fct1(logits1.view(-1), labels[:, 0].view(-1))\n","\n","            if self.num_labels2 > 1:\n","                loss_fct2 = CrossEntropyLoss()\n","                loss2 = loss_fct2(logits2.view(-1, self.num_labels2), labels[:, 1].view(-1))\n","            else:\n","                loss_fct2 = MSELoss()\n","                loss2 = loss_fct2(logits2.view(-1), labels[:, 1].view(-1))\n","            loss = self.alpha*loss1 + (1-self.alpha)*loss2 \n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:] #not sure if this works\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=distilbert_output.hidden_states, #hidden_states,\n","            attentions=distilbert_output.attentions, #attentions,\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8HR2WV3iFRz"},"source":["def tune_transformer(num_samples=8, gpus_per_trial=0, smoke_test=False):\n","    data_dir_name = \"./data\" if not smoke_test else \"./test_data\"\n","    data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n","    if not os.path.exists(data_dir):\n","        os.mkdir(data_dir, 0o755)\n","\n","    def get_model(params):\n","        db_config = db_config_base\n","        print(\"printing params\", params)\n","        if params is not None:\n","          db_config.update({'_name_or_path': '/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/DistilBertPretrained', 'alpha': params['alpha'], 'attention_dropout': params['attention_dropout'], 'dropout': params['dropout']})\n","        return DistilBERTForMultipleSequenceClassification.from_pretrained(pretrained_model_name_or_path = db_config_base._name_or_path, config = db_config_base)\n","\n","    train_dataset = chunked_encoded_dataset['train']\n","    eval_dataset = chunked_encoded_dataset['validation']\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"DistilBertMultitask_HPsearch\",\n","        learning_rate=1e-5,  # config\n","        do_train=True,\n","        do_eval=True,\n","        no_cuda=gpus_per_trial <= 0,\n","        evaluation_strategy=\"steps\",\n","        save_total_limit = 5,\n","        logging_strategy=\"steps\",\n","        logging_steps=5,\n","        eval_steps=5,\n","        load_best_model_at_end=True,\n","        # metric_for_best_model='eval_s_f1',\n","        # greater_is_better=True,\n","        num_train_epochs=0.9,  # config\n","        per_device_train_batch_size=16,  # config\n","        per_device_eval_batch_size=16,  # config\n","        warmup_steps=0,\n","        weight_decay=0.1,  # config\n","        logging_dir=\"./logs\",\n","        skip_memory_metrics=True)\n","\n","    tune_config_ASHA = {\n","        \"attention_dropout\": tune.uniform(0.15,0.4),\n","        \"dropout\": tune.uniform(0.15, 0.4),\n","        \"alpha\": tune.uniform(0.3,0.7),\n","        \"learning_rate\": tune.loguniform(1e-5, 1e-4),\n","        \"per_device_train_batch_size\": tune.choice([16]),\n","        \"num_train_epochs\": tune.choice([0.9]),\n","        \"wandb\": {\n","            \"project\": \"DistilbertMultitaskHPSearch\",\n","            \"group\": \"Search1\",\n","            \"api_key\": config['WandB']['api_key'],\n","            \"log_config\": True\n","        }\n","    }\n","\n","    trainer = Trainer(\n","        model_init=get_model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics)\n","    \n","\n","    # scheduler = ASHAScheduler(\n","    #     metric=\"eval_s_f1\",\n","    #     mode=\"max\",\n","    #     max_t=1000,\n","    #     grace_period=30,\n","    #     reduction_factor=1.5)\n","\n","    # scheduler = ASHAScheduler(\n","    #     metric=\"eval_s_f1\",\n","    #     mode=\"max\",\n","    #     max_t=1,\n","    #     grace_period=1,\n","    #     reduction_factor=2)\n","\n","    reporter = CLIReporter(\n","        parameter_columns={\n","            \"weight_decay\": \"w_decay\",\n","            \"learning_rate\": \"lr\",\n","            \"dropout\": \"dropout\",\n","            \"alpha\": \"alpha\",\n","            \"per_device_train_batch_size\": \"train_bs/gpu\",\n","            \"num_train_epochs\": \"num_epochs\"\n","        },\n","        metric_columns=[\n","            \"eval_s_accuracy\", \"eval_loss\", \"eval_s_f1\", \"steps\", \"training_iteration\"\n","        ])\n","\n","    trainer.hyperparameter_search(\n","        hp_space=lambda _: tune_config_ASHA,\n","        backend=\"ray\",\n","        # compute_objective=my_objective,\n","        direction=\"maximize\",\n","        n_trials=num_samples,\n","        resources_per_trial={\n","            \"cpu\": 2,\n","            \"gpu\": gpus_per_trial\n","        },\n","        # scheduler=scheduler,\n","        keep_checkpoints_num=1,\n","        checkpoint_score_attr=\"training_iteration\",\n","        stop={\"training_iteration\": 1} if smoke_test else None,\n","        progress_reporter=reporter,\n","        local_dir=\"~/ray_results/\",\n","        name=\"tune_transformer\",\n","        loggers=DEFAULT_LOGGERS + (WandbLogger,))\n","        # time_budget_s=60*60*10) # 10 hours"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"R56EET-8kvC3"},"source":["tune_transformer(num_samples=3, gpus_per_trial=1, smoke_test=False)\n","# tune_transformer(num_samples=1, gpus_per_trial=0, smoke_test=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yngPQIlDdmKd"},"source":["standard hyperparameter tuning"]},{"cell_type":"code","metadata":{"id":"cUSOFFnGPDyp"},"source":["# saves our models to artifacts in WandB\n","import wandb\n","%env WANDB_LOG_MODEL=true\n","wandb.login()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqc0rD6rVp2f"},"source":["import os\n","import pickle\n","\n","import ray\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import PopulationBasedTraining\n","from transformers import AutoConfig, \\\n","    AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLoggerCallback, WandbLogger\n","from transformers import ElectraTokenizerFast, ElectraForSequenceClassification, BertTokenizer, BertForSequenceClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xa578LQbSUYj"},"source":["model_name = 'bert-base-uncased'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F90QttEzFxT7"},"source":["project_name = \"BERT-base-uncased-HP-Tuning\"\n","%env WANDB_PROJECT=BERT-base-uncased-HP-Tuning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsVonOvqTpZe"},"source":["from datasets import DatasetDict\n","\n","with open(r\"/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/PreTokenized/BERT_UNCASED_NER_512/train_dataset.pkl\", \"rb\") as input_file:\n","  train_dataset = pickle.load(input_file)\n","\n","with open(r\"/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/PreTokenized/BERT_UNCASED_NER_512/val_dataset.pkl\", \"rb\") as input_file:\n","  val_dataset = pickle.load(input_file)\n","\n","with open(r\"/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/PreTokenized/BERT_UNCASED_NER_512/test_dataset.pkl\", \"rb\") as input_file:\n","  test_dataset = pickle.load(input_file)\n","\n","chunked_encoded_dataset = DatasetDict({'train': train_dataset, 'validation': val_dataset, 'test': test_dataset})\n","chunked_encoded_dataset = chunked_encoded_dataset.rename_column('success_label', 'labels')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za_8B6C9KOTM"},"source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","def tune_transformer(num_samples=8,\n","                     gpus_per_trial=0,\n","                     smoke_test=False,\n","                     ray_address=None):\n","    ray.init(ray_address, log_to_driver=True)\n","    data_dir_name = \"./data\" if not smoke_test else \"./test_data\"\n","    data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n","    if not os.path.exists(data_dir):\n","        os.mkdir(data_dir, 0o755)\n","\n","    # Change these as needed.\n","    # model_name = 'google/electra-small-discriminator' if not smoke_test \\\n","    #     else 'google/electra-small-discriminator'\n","    task_name = \"grs\"\n","\n","    task_data_dir = os.path.join(data_dir, task_name.upper())\n","\n","    num_labels = 2\n","\n","    # config = AutoConfig.from_pretrained(\n","    #     model_name, num_labels=num_labels, finetuning_task=task_name)\n","\n","    # Download and cache tokenizer, model, and features\n","    print(\"Downloading and caching Tokenizer\")\n","    # tokenizer = ElectraTokenizerFast.from_pretrained('google/electra-small-discriminator', additional_special_tokens = ['[CHARACTER]'])\n","    tokenizer = BertTokenizer.from_pretrained(model_name, additional_special_tokens = ['[CHARACTER]'])\n","    \n","    # Triggers tokenizer download to cache\n","    # print(\"Downloading and caching pre-trained model\")\n","    # AutoModelForSequenceClassification.from_pretrained(\n","    #     model_name,\n","    #     config=config,\n","    # )\n","\n","    def get_model():\n","        # model = ElectraForSequenceClassification.from_pretrained('/content/drive/MyDrive/Thesis/Models/ELECTRA_small_pretrained', num_labels=2)\n","        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","        model.resize_token_embeddings(len(tokenizer))\n","        return model\n","\n","    train_dataset = chunked_encoded_dataset['train']\n","    eval_dataset = chunked_encoded_dataset['validation']\n","\n","    training_args = TrainingArguments(\n","        project_name,\n","        # output_dir=\".\",\n","        learning_rate=1e-5,  # config\n","        do_train=True,\n","        do_eval=True,\n","        no_cuda=gpus_per_trial <= 0,\n","        evaluation_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        num_train_epochs=2,  # config\n","        max_steps=-1,\n","        per_device_train_batch_size=16,  # config\n","        per_device_eval_batch_size=16,  # config\n","        warmup_steps=0,\n","        weight_decay=0.1,  # config\n","        # logging_dir=\"./logs\",\n","    )\n","\n","    training_args._n_gpu = gpus_per_trial\n","\n","    trainer = Trainer(\n","        model_init=get_model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics)\n","\n","    tune_config = {\n","        # \"per_device_train_batch_size\": 16,\n","        \"per_device_eval_batch_size\": 32,\n","        \"num_train_epochs\": tune.choice([2, 3, 4, 5]),\n","        \"max_steps\": 1 if smoke_test else -1,  # Used for smoke test.\n","        \"wandb\": {\n","            \"project\": project_name,\n","            \"api_key\": config['WandB']['api_key'],\n","            \"log_config\": True\n","        }\n","    }\n","\n","    scheduler = PopulationBasedTraining(\n","        time_attr=\"training_iteration\",\n","        metric=\"eval_f1\",\n","        mode=\"max\",\n","        perturbation_interval=1,\n","        hyperparam_mutations={\n","            \"weight_decay\": tune.uniform(0.0, 0.3),\n","            \"warmup_steps\": tune.choice([0, 50, 100, 500, 1000]),\n","            \"learning_rate\": tune.uniform(1e-5, 4e-5),\n","            \"per_device_train_batch_size\": [8, 16],\n","        })\n","\n","    reporter = CLIReporter(\n","        parameter_columns={\n","            \"weight_decay\": \"w_decay\",\n","            \"learning_rate\": \"lr\",\n","            \"per_device_train_batch_size\": \"train_bs/gpu\",\n","            \"num_train_epochs\": \"num_epochs\"\n","        })\n","\n","    trainer.hyperparameter_search(\n","        hp_space=lambda _: tune_config,\n","        backend=\"ray\",\n","        n_trials=num_samples,\n","        resources_per_trial={\n","            \"cpu\": 1,\n","            \"gpu\": gpus_per_trial\n","        },\n","        scheduler=scheduler,\n","        keep_checkpoints_num=1,\n","        checkpoint_score_attr=\"training_iteration\",\n","        stop={\"training_iteration\": 1} if smoke_test else None,\n","        progress_reporter=reporter,\n","        local_dir=\"~/ray_results/\",\n","        name=\"tune_transformer_pbt\",\n","        # log_to_file=True,\n","        loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n","        time_budget_s=60*15\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VD1-EmsGKY9l"},"source":["import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\n","    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n","parser.add_argument(\n","    \"--ray-address\",\n","    type=str,\n","    default=None,\n","    help=\"Address to use for Ray. \"\n","    \"Use \\\"auto\\\" for cluster. \"\n","    \"Defaults to None for local.\")\n","args, _ = parser.parse_known_args()\n","\n","if args.smoke_test:\n","    tune_transformer(\n","        num_samples=1,\n","        gpus_per_trial=0,\n","        smoke_test=True,\n","        ray_address=args.ray_address)\n","else:\n","    # You can change the number of GPUs here:\n","    tune_transformer(\n","        num_samples=15, gpus_per_trial=1, ray_address=args.ray_address)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVQNsDDuLuG_"},"source":["ray.shutdown()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tI83z4OAhYpi"},"source":["# UNNEEDED CODE"]},{"cell_type":"code","metadata":{"id":"0qPERDTBVqA-"},"source":["def tokenize_w_overlap(example, tokenizer, chunk_len = 512, overlap_len = 50):\n","    data_tokenize = tokenizer(example['text'], \n","                    max_length = chunk_len,\n","                    add_special_tokens=True,\n","                    return_attention_mask=True,\n","                    return_token_type_ids=True,\n","                    return_overflowing_tokens = True,\n","                    return_tensors = 'np')\n","\n","    long_terms_token = []\n","    input_ids_list = []\n","    attention_mask_list = []\n","    token_type_ids_list = []\n","    targets_list = []\n","\n","    previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1)\n","    previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1)\n","    previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1)\n","    remain = data_tokenize[\"overflowing_tokens\"].reshape(-1)\n","    \n","    input_ids_list.append(previous_input_ids)\n","    attention_mask_list.append(previous_attention_mask)\n","    token_type_ids_list.append(previous_token_type_ids)\n","\n","    if remain is not None:\n","      idxs = range(len(remain)+chunk_len)\n","      idxs = idxs[(chunk_len-overlap_len-2)\n","                    ::(chunk_len-overlap_len-2)]\n","      input_ids_first_overlap = previous_input_ids[-(\n","          overlap_len+1):-1]\n","      start_token = np.array([101])\n","      end_token = np.array([102])\n","\n","      for i, idx in enumerate(idxs):\n","          if i == 0:\n","              input_ids = np.concatenate((input_ids_first_overlap, remain[:idx])) # building the 2nd chunk\n","          elif i == len(idxs):\n","              input_ids = remain[idx:]\n","          elif previous_idx >= len(remain):\n","              break\n","          else:\n","              input_ids = remain[(previous_idx-overlap_len):idx]\n","\n","          previous_idx = idx\n","\n","          nb_token = len(input_ids)+2\n","          attention_mask = np.ones(chunk_len)\n","          attention_mask[nb_token:chunk_len] = 0 # only will take effect on the last chunk\n","          token_type_ids = np.zeros(chunk_len)\n","          input_ids = np.concatenate((start_token, input_ids, end_token))\n","\n","          if chunk_len-nb_token > 0: # add padding, only can pass on last chunk\n","              padding = np.zeros(chunk_len-nb_token)\n","              input_ids = np.concatenate((input_ids, padding))\n","\n","          input_ids_list.append(input_ids)\n","          attention_mask_list.append(attention_mask)\n","          token_type_ids_list.append(token_type_ids)\n","\n","      print(input_ids_list)\n","\n","      return {\n","          'input_ids': input_ids_list,  # torch.tensor(ids, dtype=torch.long),\n","          'attention_mask': attention_mask_list,\n","          'token_type_ids': token_type_ids_list,\n","          'success_label': np.array([example['success_label']] * len(input_ids_list)),\n","          'genre': np.array([example['genre']] * len(input_ids_list))\n","          # 'len': [np.array(len(targets_list), dtype=torch.long)]\n","      }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cM0zn9NVqgf"},"source":["# When batched = True, we take in multiple examples\n","def chunk_and_encode_examples_w_overlap(examples):\n","  mega_dict = {'attention_mask': [], 'genre': [], 'input_ids': [], 'success_label': [], 'token_type_ids': [], 'book_title': []}\n","  for i in range(len(examples['text'])):\n","    book_sample = {'text': examples['text'][i], 'genre': examples['genre'][i], 'success_label': examples['success_label'][i], 'book_title':examples['book_title'][i]}\n","    dictOfTokenizedChunks = tokenize_w_overlap(book_sample, tokenizer)\n","    for key, value in dictOfTokenizedChunks.items():\n","      mega_dict[key].extend(value)\n","  return mega_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qL8HlV6LXRtu"},"source":["!transformers-cli env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN57wiBZXVrl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9avcpW6dPni"},"source":["from transformers import DistilBertModel\n","import torch\n","\n","run = wandb.init()\n","artifact = run.use_artifact('lucaguarro/DistilbertMultitaskHPSearch/model-3vvi0uoq:v0', type='model')\n","artifact_dir = artifact.download()\n","db_model = DistilBertModel.from_pretrained(artifact_dir)\n","\n","db_model.save_pretrained('/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/3vvi0uoqDistilBertModel')"],"execution_count":null,"outputs":[]}]}