{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stager.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPPwE0o1xqWle0jFi6Ic0V2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PktHVgJIDch9","executionInfo":{"status":"ok","timestamp":1626904330819,"user_tz":420,"elapsed":6082,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"0d96c3bf-86da-4787-fb05-2208e53a5205"},"source":["!pip install datasets==1.9.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting datasets==1.9.0\n","  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n","\u001b[K     |████████████████████████████████| 262 kB 33.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.14-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 709 kB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (1.19.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (2.23.0)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 43.9 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (0.70.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (1.1.5)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (3.0.0)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 62.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (4.6.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (0.3.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.9.0) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.9.0) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.9.0) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.9.0) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.9.0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.9.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.9.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.9.0) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.9.0) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.9.0) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.9.0) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.9.0) (1.15.0)\n","Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n","Successfully installed datasets-1.9.0 fsspec-2021.7.0 huggingface-hub-0.0.14 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RBBMldhV1msf"},"source":["##### This is the Keras implementation from the researches themselves"]},{"cell_type":"code","metadata":{"id":"SvKztdP21FWF"},"source":["class AttentionMLP(Layer):\n","    \"\"\"\n","    Genre Aware Attention Model\n","\n","    \"\"\"\n","    def __init__(self,\n","                 units, # what does units mean\n","                 activation=None,\n","                 use_bias=True,\n","                 kernel_initializer='glorot_uniform',\n","                 bias_initializer='ones',\n","                 v_initializer='glorot_uniform',\n","                 Wg_initializer='glorot_uniform',\n","                 **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        self.units = units\n","        self.activation = activations.get(activation) # \"selu\"\n","        self.use_bias = use_bias\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.v_initializer = initializers.get(v_initializer)\n","        self.Wg_initializer = initializers.get(Wg_initializer)\n","        self.supports_masking = True\n","        super(AttentionMLP, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert type(input_shape) is list and len(input_shape) == 2\n","        # W: (EMBED_SIZE, units)\n","        # Wg:(GENRE_EMB_SIZE, units)\n","        # b: (units,)\n","        # v: (units,)\n","\n","        self.W = self.add_weight(name=\"W_{:s}\".format(self.name),\n","                                 shape=(input_shape[0][-1], self.units),\n","                                 initializer=self.kernel_initializer,\n","                                 trainable=True)\n","\n","        self.Wg = self.add_weight(name=\"W_g{:s}\".format(self.name),\n","                                  shape=(input_shape[1][-1], self.units),\n","                                  initializer=self.Wg_initializer,\n","                                  trainable=True)\n","\n","        self.b = self.add_weight(name=\"b_{:s}\".format(self.name), # b_a in the paper\n","                                 shape=(self.units,),\n","                                 initializer=self.bias_initializer,\n","                                 trainable=True)\n","\n","        self.v = self.add_weight(name=\"v_{:s}\".format(self.name),\n","                                 shape=(self.units,),\n","                                 initializer=self.v_initializer,\n","                                 trainable=True)\n","\n","        super(AttentionMLP, self).build(input_shape)\n","\n","    def call(self, xs, mask=None):\n","        # input: [x, u]\n","        # x: (BATCH_SIZE, MAX_TIMESTEPS, EMBED_SIZE)\n","        # g: (BATCH_SIZE, 1,GENRE_EMB_SIZE)\n","\n","        # all the modalities are concatenated together into x. However, it seems that they are using x as if they have all already gone through a dense layer.. \n","        # Also W_h in the paper makes no sense. How can you multiply all modalities by the same dimensional weight matrix when each modality itself has a different dimension?\n","        x, g = xs \n","        g=K.squeeze(g, axis=1)\n","        atten_g = K.expand_dims(K.dot(g, self.Wg), axis=1) # dot product between genre vector and genre Weights\n","        \n","        # computes score(x_i, g) and NOT h_i as said in the paper\n","        et = self.activation(K.dot(x, self.W) + atten_g + self.b) # this is h (all concatenated together)\n","        et = K.dot(et, self.v)\n","\n","        at = K.softmax(et)  # softmaxed to get us the alpha scores. Not clear if these values correspond to each modality as expressed in the paper or... ???\n","        if mask is not None and mask[0] is not None:\n","            at *= K.cast(mask, K.floatx())\n","        # ot: (BATCH_SIZE, MAX_TIMESTEPS, EMBED_SIZE)\n","        atx = K.expand_dims(at, axis=-1)\n","        ot = atx * x\n","        # output: (BATCH_SIZE, EMBED_SIZE)\n","        # print(ot.eval())\n","        return K.sum(ot, axis=1) # returns r\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def compute_output_shape(self, input_shape):\n","        # output shape: (BATCH_SIZE, EMBED_SIZE)\n","        return (input_shape[0][0], input_shape[0][-1])\n","\n","    def get_config(self):\n","        return super(AttentionMLP, self).get_config()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQCVETxMLeSs"},"source":["### From Paper to Pytorch"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05cMpASCqoJ4","executionInfo":{"status":"ok","timestamp":1626998481827,"user_tz":420,"elapsed":4218,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"bf212a09-ae3d-4de5-8b15-71e11f791f90"},"source":["!pip install torch"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S-Hw4pbqqZyQ","executionInfo":{"status":"ok","timestamp":1626998638075,"user_tz":420,"elapsed":132,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["import torch.nn as nn\n","import torch"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBzmjejZLhbT","executionInfo":{"status":"ok","timestamp":1627012698171,"user_tz":420,"elapsed":136,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["class Genre_Aware_Attention_Model(nn.Module):\n","\n","  def __init__(self, num_units):\n","    super(Genre_Aware_Attention_Model, self).__init__()\n","    self.num_units = num_units\n","    self.hidden_trans = nn.Linear(768, 100)\n","    self.hidden_c5g = nn.Linear(376417, 100)\n","    \n","    self.activation = nn.SELU()\n","\n","    # self.Wa\n","\n","    self.v = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,1)),\n","        requires_grad=True\n","    )\n","\n","    # self.Wa = nn.linear(num_units,num_units)\n","\n","    self.Wa = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.Wg = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(8, num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.softmax = nn.Softmax(dim=0)\n","\n","    self.out = nn.Linear(100, 2)\n","\n","  def forward(self, x1, x2, g):\n","    x1_dense = self.hidden_trans(x1)\n","    x2_dense = self.hidden_c5g(x2)\n","\n","    atten_g = torch.mm(torch.unsqueeze(g, 0), self.Wg)\n","\n","    h1 = self.activation(x1_dense)\n","    h2 = self.activation(x2_dense)\n","\n","    h1_score = torch.dot(torch.squeeze(self.activation(torch.mm(h1, self.Wa) + atten_g)), torch.squeeze(self.v))\n","    h2_score = torch.dot(torch.squeeze(self.activation(torch.mm(h2, self.Wa) + atten_g)), torch.squeeze(self.v))\n","\n","    alphas = self.softmax(torch.stack([h1_score, h2_score]))\n","\n","    x1_scaled = x1_dense * alphas[0]\n","    x2_scaled = x2_dense * alphas[1]\n","\n","    r = torch.sum(torch.stack([torch.squeeze(x1_scaled), torch.squeeze(x2_scaled)]), axis = 0)\n","    return self.out(r)"],"execution_count":265,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vwwH7wBXsbp","executionInfo":{"status":"ok","timestamp":1627012698709,"user_tz":420,"elapsed":3,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["sample_g = torch.Tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"],"execution_count":267,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEQ3wffurY0-","executionInfo":{"status":"ok","timestamp":1627012699208,"user_tz":420,"elapsed":151,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"27f2e3f3-d707-4627-b7fc-77f017856c23"},"source":["oy = the_model.forward(sample_x1,sample_x2,sample_g)"],"execution_count":268,"outputs":[{"output_type":"stream","text":["tensor([0.2521, 0.7479], grad_fn=<SoftmaxBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCR2tiacgK_y","executionInfo":{"status":"ok","timestamp":1627012511891,"user_tz":420,"elapsed":147,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"3339b8d3-cd06-4694-96de-44bc9a982626"},"source":["oy"],"execution_count":264,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0459, 0.1587], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":264}]},{"cell_type":"code","metadata":{"id":"hn4rfdgl3c19"},"source":["def ga_model_train(model):\n","  # define the optimization\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = nn.Adam(model.parameters(), lr=0.01, momentum=0.9)"],"execution_count":null,"outputs":[]}]}