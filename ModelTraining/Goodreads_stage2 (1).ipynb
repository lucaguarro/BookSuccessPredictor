{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Goodreads_stage2.ipynb","provenance":[],"collapsed_sections":["_6gF1bvclYVR","p95ei-lSdr9i","BQXU-G-cjEkk","z8195Rq9rb6I","Xh-WGSe1doKZ","SEJUtjZv2lzc","DfbeQQk-Jt5V","Mj191iy1JxE3","FQO49DFsVmUr","nD_NTt5_RWbR"],"machine_shape":"hm","mount_file_id":"1wBVNh_5-j0lep-7H-6-hnXYKB4cztf3_","authorship_tag":"ABX9TyOVfJqGxOrlU36gqwPLv2O3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BjfCjM2xh9dE"},"source":["In this chapter we used already fine-tuned BERT models to extract the chunk embeddings of our books. The chunk embeddings correspond to either the meaned embeddings of all the words in the sequence or the embedding of the [CLS] token. We will explore the results of both. We will then run a variety of classifiers over these embeddings directly. \n","\n","1.   Meaned pooled output --> single layer NN\n","2.   SVM\n","3.   RoBERT\n","4.   ToBERT "]},{"cell_type":"markdown","metadata":{"id":"XLXM0vhe5MQ8"},"source":["# Installs, Imports, Configuration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttrgrV6NN3nB","executionInfo":{"status":"ok","timestamp":1630367585120,"user_tz":420,"elapsed":51535,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"18c04704-19a6-40b3-a844-e4036ea1491b"},"source":["!pip install datasets\n","!pip install \"ray[default]\"==1.5.2\n","!pip install wandb\n","!pip install tensorboardX\n","!pip install pytorch_lightning\n","!pip install transformers\n","\n","!pip install httplib2==0.15.0\n","!pip install google-api-python-client==1.6"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n","\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 41.4 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 133 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 143 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 153 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 163 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 174 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 184 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 194 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 204 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 215 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 225 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 235 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 245 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 256 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 264 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 79.5 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 89.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n","Successfully installed datasets-1.11.0 fsspec-2021.7.0 huggingface-hub-0.0.16 xxhash-2.0.2\n","Collecting ray[default]==1.5.2\n","  Downloading ray-1.5.2-cp37-cp37m-manylinux2014_x86_64.whl (51.0 MB)\n","\u001b[K     |████████████████████████████████| 51.0 MB 72 kB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (3.13)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 81.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (3.0.12)\n","Collecting aioredis<2\n","  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 5.5 MB/s \n","\u001b[?25hCollecting aiohttp-cors\n","  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (2.23.0)\n","Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (0.11.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (2.6.0)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting redis>=3.5.0\n","  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 743 kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (1.39.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (7.1.2)\n","Collecting py-spy>=0.2.0\n","  Downloading py_spy-0.3.8-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 97.6 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (1.0.2)\n","Collecting gpustat\n","  Downloading gpustat-0.6.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 9.7 MB/s \n","\u001b[?25hCollecting pydantic>=1.8\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 89.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (1.19.5)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.5.2) (3.17.3)\n","Collecting opencensus\n","  Downloading opencensus-0.7.13-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.9 MB/s \n","\u001b[?25hCollecting colorful\n","  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 54.2 MB/s \n","\u001b[?25hCollecting hiredis\n","  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 5.1 MB/s \n","\u001b[?25hCollecting async-timeout\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[default]==1.5.2) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8->ray[default]==1.5.2) (3.7.4.3)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 86.8 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]==1.5.2) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 81.5 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]==1.5.2) (3.0.4)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[default]==1.5.2) (2.10)\n","Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]==1.5.2) (7.352.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]==1.5.2) (5.4.8)\n","Collecting blessings>=1.6\n","  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n","Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]==1.5.2) (1.26.3)\n","Collecting opencensus-context==0.1.2\n","  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (57.4.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (21.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (2018.9)\n","Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (1.34.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (1.53.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (0.2.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]==1.5.2) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.5.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.5.2) (2021.5.30)\n","Building wheels for collected packages: gpustat\n","  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=8718f73c10532637c5c1082a036d56bc8e4d3d04ef902a52c5d25cfe9f8b6018\n","  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n","Successfully built gpustat\n","Installing collected packages: multidict, yarl, async-timeout, opencensus-context, hiredis, blessings, aiohttp, redis, pydantic, py-spy, opencensus, gpustat, colorama, aioredis, aiohttp-cors, ray, colorful\n","Successfully installed aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 gpustat-0.6.0 hiredis-2.0.0 multidict-5.1.0 opencensus-0.7.13 opencensus-context-0.1.2 py-spy-0.3.8 pydantic-1.8.2 ray-1.5.2 redis-3.5.3 yarl-1.6.3\n","Collecting wandb\n","  Downloading wandb-0.12.1-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 9.0 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 94.4 MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n","\u001b[K     |████████████████████████████████| 170 kB 101.7 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.5 MB/s \n","\u001b[?25hCollecting smmap<5,>=3.0.1\n","  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=435a4231b7dbf9aa9c1b7a068e0be74a3f48974b6340cd96b9b41f060cb9d6d7\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=a82121214bd52f988de20cc778bbfda0593156cb17f3d7bbedd7d6fbda30e444\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n","Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.1\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 8.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-1.4.4-py3-none-any.whl (918 kB)\n","\u001b[K     |████████████████████████████████| 918 kB 8.1 MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 67.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.7.4.3)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n","Collecting PyYAML>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 60.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.9.0+cu102)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2021.7.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.0)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.0)\n","Collecting torchmetrics>=0.4.0\n","  Downloading torchmetrics-0.5.0-py3-none-any.whl (272 kB)\n","\u001b[K     |████████████████████████████████| 272 kB 71.0 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4.post0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.5)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.34.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.39.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.5.0)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ee9d8102aee004477110c83b6c4430f3448697be19c22206f22472e1dbbb01f7\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built future\n","Installing collected packages: torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed PyYAML-5.4.1 future-0.18.2 pyDeprecate-0.3.1 pytorch-lightning-1.4.4 torchmetrics-0.5.0\n","Collecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 82.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 80.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.0.16\n","    Uninstalling huggingface-hub-0.0.16:\n","      Successfully uninstalled huggingface-hub-0.0.16\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n","Collecting httplib2==0.15.0\n","  Downloading httplib2-0.15.0-py3-none-any.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.6 MB/s \n","\u001b[?25hInstalling collected packages: httplib2\n","  Attempting uninstall: httplib2\n","    Found existing installation: httplib2 0.17.4\n","    Uninstalling httplib2-0.17.4:\n","      Successfully uninstalled httplib2-0.17.4\n","Successfully installed httplib2-0.15.0\n","Collecting google-api-python-client==1.6\n","  Downloading google_api_python_client-1.6.0-py2.py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (1.15.0)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (0.15.0)\n","Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (4.1.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (3.0.1)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.4.8)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.2.8)\n","Installing collected packages: google-api-python-client\n","  Attempting uninstall: google-api-python-client\n","    Found existing installation: google-api-python-client 1.12.8\n","    Uninstalling google-api-python-client-1.12.8:\n","      Successfully uninstalled google-api-python-client-1.12.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","firebase-admin 4.4.0 requires google-api-python-client>=1.7.8, but you have google-api-python-client 1.6.0 which is incompatible.\n","earthengine-api 0.1.278 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.6.0 which is incompatible.\u001b[0m\n","Successfully installed google-api-python-client-1.6.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7vwrYLDGtF-","executionInfo":{"status":"ok","timestamp":1630367855241,"user_tz":420,"elapsed":194,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"03e38adb-c859-4a63-8843-7bd8a6f2bef8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"e0AfA2H3URJt"},"source":["!cd drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QiCafSeXFWzf","executionInfo":{"status":"ok","timestamp":1630367861703,"user_tz":420,"elapsed":3504,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["import pickle\n","import numpy as np\n","import torch\n","import pytorch_lightning as pl\n","from pathlib import Path\n","import sys\n","import os\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n","from scipy.special import softmax\n","import math\n","\n","# from ray import tune\n","# from ray.tune.logger import DEFAULT_LOGGERS\n","# from ray.tune.integration.wandb import WandbLoggerCallback\n","# from ray.tune.schedulers import ASHAScheduler\n","# from functools import partial"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA5cgP2i5PB9","executionInfo":{"status":"ok","timestamp":1630367867599,"user_tz":420,"elapsed":967,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["import configparser\n","\n","config = configparser.ConfigParser()\n","config.read('/content/drive/MyDrive/Thesis/BookSuccessPredictor/config.ini')\n","\n","drive_base_path = Path(config['Drive']['drive_base_path'])\n","\n","sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / '_utils'))\n","sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / 'datasets' / 'goodreads_maharjan_super' / 'MultiModal' / 'dataset_loader'))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXJAPVyEBGWX","executionInfo":{"status":"ok","timestamp":1630367867783,"user_tz":420,"elapsed":2,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"782460f0-5e2f-477a-89ef-dff6fafa4ea6"},"source":["# saves our models to artifacts in WandB\n","import wandb\n","%env WANDB_LOG_MODEL=true\n","%env WANDB_PROJECT=goodreads_success_predictor"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["env: WANDB_LOG_MODEL=true\n","env: WANDB_PROJECT=goodreads_success_predictor\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPihJfU1By4W","executionInfo":{"status":"ok","timestamp":1630367875544,"user_tz":420,"elapsed":2120,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"f57c0c4b-6790-4d83-b33a-4457defcecc8"},"source":["wandb.login(key = config['WandB']['api_key'])"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"_6gF1bvclYVR"},"source":["# Get Transformer Model from Stage 1"]},{"cell_type":"code","metadata":{"id":"0HlhrDWTn-nD","executionInfo":{"status":"ok","timestamp":1630367880478,"user_tz":420,"elapsed":1797,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from transformers.modeling_outputs import SequenceClassifierOutput\n","from torch import nn\n","import torch\n","from torch.nn import CrossEntropyLoss, MSELoss\n","\n","from transformers import BertPreTrainedModel, BertModel\n","\n","from transformers import DistilBertPreTrainedModel, DistilBertModel\n","\n","class DistilBERTForMultipleSequenceClassification(DistilBertPreTrainedModel):\n","    def __init__(self, config, num_labels1 = 2, num_labels2 = 8):\n","        super().__init__(config)\n","        self.num_labels1 = num_labels1\n","        self.num_labels2 = num_labels2\n","        print(self.num_labels1, self.num_labels2)\n","        self.alpha = config.alpha\n","        self.config = config\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier1 = nn.Linear(config.dim, self.num_labels1)\n","        self.classifier2 = nn.Linear(config.dim, self.num_labels2)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n","            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n","            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        distilbert_output = self.distilbert(\n","              input_ids=input_ids,\n","              attention_mask=attention_mask,\n","              head_mask=head_mask,\n","              inputs_embeds=inputs_embeds,\n","              output_attentions=output_attentions,\n","              output_hidden_states=output_hidden_states,\n","              return_dict=return_dict,\n","          )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits1 = self.classifier1(pooled_output)\n","        logits2 = self.classifier2(pooled_output)\n","        logits = torch.cat([logits1, logits2], 1)\n","\n","        loss = None\n","        if labels is not None:\n","            #if self.config.problem_type is None:\n","            #self.config.problem_type = \"single_label_classification\"\n","            \n","            if self.num_labels1 > 1:\n","                loss_fct1 = CrossEntropyLoss()\n","                loss1 = loss_fct1(logits1.view(-1, self.num_labels1), labels[:, 0].view(-1))\n","            else:\n","                loss_fct1 = MSELoss()\n","                loss1 = loss_fct1(logits1.view(-1), labels[:, 0].view(-1))\n","\n","            if self.num_labels2 > 1:\n","                loss_fct2 = CrossEntropyLoss()\n","                loss2 = loss_fct2(logits2.view(-1, self.num_labels2), labels[:, 1].view(-1))\n","            else:\n","                loss_fct2 = MSELoss()\n","                loss2 = loss_fct2(logits2.view(-1), labels[:, 1].view(-1))\n","            loss = self.alpha*loss1 + (1-self.alpha)*loss2 \n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:] #not sure if this works\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=distilbert_output.hidden_states, #hidden_states,\n","            attentions=distilbert_output.attentions, #attentions,\n","        )"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Sot-nVzelfLN","executionInfo":{"status":"ok","timestamp":1630367894093,"user_tz":420,"elapsed":13622,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"bffcb537-c946-4c15-cf34-2b5e4ac18b7e"},"source":["import wandb\n","run = wandb.init()\n","\n","if config['Model']['name'] == 'distilbert-base-uncased':\n","  if eval(config['Tokenizer']['overlap']):\n","    # artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-nlpbosie:v0', type='model')\n","    artifact = run.use_artifact('lucaguarro/DistilbertMultitaskHPSearch/model-3vvi0uoq:v0', type='model')\n","    print(\"using model trained on overlap dataset\")\n","  else:\n","    print(\"using model trained on sentence tokenized dataset\")\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-2giwtwvy:v0', type='model')\n","    \n","artifact_dir = artifact.download()\n","\n","transformer_model = DistilBERTForMultipleSequenceClassification.from_pretrained(artifact_dir, num_labels1 = 2, num_labels2 = 8)\n","transformer_model.cuda()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucaguarro\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.12.1<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">dainty-gorge-244</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/lucaguarro/goodreads_success_predictor\" target=\"_blank\">https://wandb.ai/lucaguarro/goodreads_success_predictor</a><br/>\n","                Run page: <a href=\"https://wandb.ai/lucaguarro/goodreads_success_predictor/runs/2qhzq0my\" target=\"_blank\">https://wandb.ai/lucaguarro/goodreads_success_predictor/runs/2qhzq0my</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20210830_235759-2qhzq0my</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["using model trained on overlap dataset\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-3vvi0uoq:v0, 255.48MB. 3 files... Done. 0:0:0\n"]},{"output_type":"stream","name":"stdout","text":["2 8\n"]},{"output_type":"execute_result","data":{"text/plain":["DistilBERTForMultipleSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.24363502971184062, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.24363502971184062, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.24363502971184062, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.24363502971184062, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.24363502971184062, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.24363502971184062, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.38767857660247906, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier1): Linear(in_features=768, out_features=2, bias=True)\n","  (classifier2): Linear(in_features=768, out_features=8, bias=True)\n","  (dropout): Dropout(p=0.38767857660247906, inplace=False)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"oHUvF9GXrF_0"},"source":["# Getting the Data"]},{"cell_type":"markdown","metadata":{"id":"p95ei-lSdr9i"},"source":["## Getting the Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"idsWCeYad34q"},"source":["### Creating the Dataset"]},{"cell_type":"markdown","metadata":{"id":"YgoOFdif7bbj"},"source":["#### From script"]},{"cell_type":"markdown","metadata":{"id":"32MHnBCbFJTk"},"source":["first we have to get the tokenized dataset"]},{"cell_type":"code","metadata":{"id":"N1rEBRTGFPTQ"},"source":["load_path = Path(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/already_tokenized/80_20/DistilBERT_UNCASED_NER_512_w50overlap\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uBd3PYnFLSy"},"source":["from datasets import DatasetDict, Dataset, concatenate_datasets\n","train_paths = [f for f in os.listdir(load_path) if f.startswith('train')]\n","val_paths = [f for f in os.listdir(load_path) if f.startswith('val')]\n","test_paths = [f for f in os.listdir(load_path) if f.startswith('test')]\n","\n","train_datasets = []\n","val_datasets = []\n","test_datasets = []\n","\n","for trainp in train_paths:\n","  with open(load_path / trainp, \"rb\") as input_file:\n","    train_datasets.append(Dataset.from_dict(pickle.load(input_file)))\n","\n","for valp in val_paths:\n","  with open(load_path / valp, \"rb\") as input_file:\n","    val_datasets.append(Dataset.from_dict(pickle.load(input_file)))\n","\n","for testp in test_paths:\n","  with open(load_path / testp, \"rb\") as input_file:\n","    test_datasets.append(Dataset.from_dict(pickle.load(input_file)))\n","\n","train_dataset = concatenate_datasets(train_datasets)\n","del train_datasets\n","\n","val_dataset = concatenate_datasets(val_datasets)\n","del val_datasets\n","\n","test_dataset = concatenate_datasets(test_datasets)\n","del test_datasets\n","\n","chunked_encoded_dataset = DatasetDict({'train': train_dataset, 'validation': val_dataset, 'test': test_dataset})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5yO5PCDH-G4"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjBdxGKsFyZO"},"source":["import torch as th\n","import time\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def getPooledOutputs(model, encoded_dataset, batch_size = 32):\n","  model.eval()\n","\n","  # pooled_outputs = []\n","  pooled_outputs = torch.empty([0,768]).cuda()\n","\n","  num_iters = (len(encoded_dataset['input_ids']) - 1)//batch_size + 1\n","  print(\"total number of iters \", num_iters)\n","  \n","  for i in range(num_iters):\n","    print(i)\n","    up_to = i*batch_size + batch_size\n","    if len(encoded_dataset['input_ids']) < up_to:\n","      up_to = len(encoded_dataset['input_ids'])\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to]).cuda()\n","    attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to]).cuda()\n","\n","    with torch.no_grad():\n","      embeddings = model.forward(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)['hidden_states'][-1][:,0] # Pooled output\n","      pooled_outputs = th.cat([pooled_outputs, embeddings],0)\n","      th.cuda.empty_cache()\n","\n","  return pooled_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-AigWJRlMyx"},"source":["train_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjtsYx1YnBL3"},"source":["val_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xYnBmEAnHwp"},"source":["test_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyWQ8O-eEc1D"},"source":["from datasets import Dataset\n","train_set_embeddings = Dataset.from_dict({'pooled_outputs': train_set_embeddings})\n","val_set_embeddings = Dataset.from_dict({'pooled_outputs': val_set_embeddings})\n","test_set_embeddings = Dataset.from_dict({'pooled_outputs': test_set_embeddings})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8agtlOpzEigf"},"source":["from datasets import concatenate_datasets\n","dataset_w_embeddings = DatasetDict({\n","    'train': concatenate_datasets([chunked_encoded_dataset['train'], train_set_embeddings], axis = 1), \n","    'validation': concatenate_datasets([chunked_encoded_dataset['validation'], val_set_embeddings], axis = 1), \n","    'test': concatenate_datasets([chunked_encoded_dataset['test'], test_set_embeddings], axis = 1)\n","})\n","dataset_w_embeddings = dataset_w_embeddings.remove_columns(['attention_mask', 'input_ids', 'token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kK_Uf0Au3RFd"},"source":["dataset_w_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VhVV2CdEo56"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('train_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['train'], output_file)\n","\n","with open('val_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['validation'], output_file)\n","\n","with open('test_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['test'], output_file)\n","\n","folder_id = '1TVBJzrWhS-yLq0xic2eXnw0mA4lY-FU8'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKxTgPtQ7gn5"},"source":["### Loading the Dataset from Drive"]},{"cell_type":"code","metadata":{"id":"C2e0wfqflvz_"},"source":["# base_path = Path(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/60_40/DistilBERT_multitask_sentence_tokenized_dataset_embeddings\")\n","base_path = Path(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/80_20/3vvi0uoq:v0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MP8MbdD07vOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630084063783,"user_tz":420,"elapsed":12206,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"a32dd6bf-79bb-4221-c037-ab778565c25e"},"source":["from datasets import DatasetDict\n","with open(base_path / 'train_ds.pkl', \"rb\") as input_file:\n","  train_set_embeddings = pickle.load(input_file)\n","\n","with open(base_path / 'val_ds.pkl', \"rb\") as input_file:\n","  val_set_embeddings = pickle.load(input_file)\n","\n","with open(base_path / 'test_ds.pkl', \"rb\") as input_file:\n","  test_set_embeddings = pickle.load(input_file)\n","\n","dataset_w_embeddings = DatasetDict({'train': train_set_embeddings, 'validation': val_set_embeddings, 'test': test_set_embeddings})\n","dataset_w_embeddings"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 21539\n","    })\n","    validation: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 5236\n","    })\n","    test: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"ShlSp7x5LkxG"},"source":["train_set_embeddings['book_title'][0:50]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5NgXCJLhiz_4"},"source":["## Average Pooled Outputs for Shallow Neural Network and SVM"]},{"cell_type":"markdown","metadata":{"id":"TK4ZZFsWjZAU"},"source":["### Generating the Data from Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"BQXU-G-cjEkk"},"source":["#### From Script"]},{"cell_type":"code","metadata":{"id":"jEdxglmBKCSP"},"source":["def getAveragePooledOutputs(dataset_w_embeddings):\n","  book_embeddings_dataset = {'meaned_pooled_output': [], 'book_title': [], 'genre': [], 'success_label': []}\n","  book_changes = get_book_changes_idx(dataset_w_embeddings['book_title'])\n","  # print(len(book_changes))\n","  for i in range(len(book_changes)):\n","      start = book_changes[i]\n","      end = None\n","      if i != len(book_changes) - 1:\n","        end = book_changes[i+1]\n","      else:\n","        end = len(dataset_w_embeddings['pooled_outputs'])\n","\n","      segment_embeddings = dataset_w_embeddings['pooled_outputs'][start:end]\n","      book_embeddings = torch.mean(segment_embeddings, dim=0)\n","\n","      book_embeddings_dataset['meaned_pooled_output'].append(book_embeddings)\n","      book_embeddings_dataset['book_title'].append(dataset_w_embeddings['book_title'][start])\n","      book_embeddings_dataset['genre'].append(dataset_w_embeddings['genre'][start])\n","      book_embeddings_dataset['success_label'].append(dataset_w_embeddings['success_label'][start])\n","    \n","  return book_embeddings_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWhc8XKbAODQ","executionInfo":{"status":"ok","timestamp":1630034531697,"user_tz":420,"elapsed":120,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"13518f9d-e1d5-40f9-fd09-df87f4715245"},"source":["type(dataset_w_embeddings['train']['pooled_outputs'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"T4p5ieZaNm0O"},"source":["dataset_w_embeddings.set_format(type='pt', columns=['pooled_outputs', 'success_label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"161S_UpRi5Xq"},"source":["avg_pld_outs_train = getAveragePooledOutputs(dataset_w_embeddings['train'])\n","avg_pld_outs_val = getAveragePooledOutputs(dataset_w_embeddings['validation'])\n","avg_pld_outs_test = getAveragePooledOutputs(dataset_w_embeddings['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bo_TJPJRN4Zu","executionInfo":{"status":"ok","timestamp":1630035240301,"user_tz":420,"elapsed":118,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"86e06364-b8cb-4d38-c4f0-814f2eb0c8bc"},"source":["len(avg_pld_outs_hf_ds['train']['meaned_pooled_output'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["555"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"FHebc7P1TslI"},"source":["# full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","from datasets import Dataset\n","avg_pld_outs_hf_ds = DatasetDict({'train': Dataset.from_dict(avg_pld_outs_train), 'validation': Dataset.from_dict(avg_pld_outs_val), 'test': Dataset.from_dict(avg_pld_outs_test)})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQXjx63ljRis"},"source":["from datasets import DatasetDict, Dataset\n","# avg_pld_outs_hf_ds = DatasetDict({'train': Dataset.from_dict(avg_pld_outs_train), 'validation': Dataset.from_dict(avg_pld_outs_val), 'test': Dataset.from_dict(avg_pld_outs_test)})\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('avg_pld_outs_hf_ds.pkl', 'wb') as output_file:\n","  pickle.dump(avg_pld_outs_hf_ds, output_file)\n","\n","folder_id = '1TVBJzrWhS-yLq0xic2eXnw0mA4lY-FU8'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('avg_pld_outs_hf_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lavLmaCjjGfG"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHOd6dPpk-X-","executionInfo":{"status":"ok","timestamp":1630367897632,"user_tz":420,"elapsed":2910,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"d1860665-31aa-405e-cf3a-99f44adf1977"},"source":["!pip install datasets"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.11.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"zRmZRxEGjWJT","executionInfo":{"status":"ok","timestamp":1630367899272,"user_tz":420,"elapsed":1647,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from datasets import DatasetDict\n","with open(r\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/80_20/3vvi0uoq:v0/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","  avg_pld_outs_hf_ds = pickle.load(input_file)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BBuAJFyOF1D","executionInfo":{"status":"ok","timestamp":1630367904313,"user_tz":420,"elapsed":326,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"7548156f-1ab2-444a-8b1a-67dcfa7064ec"},"source":["len(avg_pld_outs_hf_ds['train']['meaned_pooled_output'])"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["555"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Ydr19fUDjo9i"},"source":["# Simple Shallow Neural Network"]},{"cell_type":"code","metadata":{"id":"Aw4ZV-yjjvYe","executionInfo":{"status":"ok","timestamp":1630367919463,"user_tz":420,"elapsed":328,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","class Net(nn.Module):\n","\n","    def __init__(self, pre_classifier_init, classifier_init, do_rate=0.1):\n","        super(Net, self).__init__()\n","\n","        self.pre_classifier = nn.Linear(768, 768)\n","        self.classifier = nn.Linear(768, 2)\n","        self.dropout = nn.Dropout(do_rate)\n","\n","        self.pre_classifier.weight.data.copy_(pre_classifier_init.weight.data)\n","        self.classifier.weight.data.copy_(classifier_init.weight.data)\n","\n","        # print(pre_classifier_init.bias.data)\n","        self.pre_classifier.bias.data.copy_(pre_classifier_init.bias.data)\n","        self.classifier.bias.data.copy_(classifier_init.bias.data)\n","\n","        # DOUBLE CHECK IF BIASES ARE BEING SET AS WELL\n","\n","    def forward(self, x, labels = None):\n","        # Max pooling over a (2, 2) window\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        return self.classifier(x)\n","\n","        # loss = None\n","        # if labels is not None:\n","        #   loss_fct = CrossEntropyLoss()\n","        #   loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","        # return SequenceClassifierOutput(\n","        #     loss = loss,\n","        #     logits = logits\n","        # )\n","\n","net = Net(transformer_model.pre_classifier, transformer_model.classifier1)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3kxvLBJj70p"},"source":["#### Results with no Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nhYqvBgkAkH","executionInfo":{"status":"ok","timestamp":1630367960365,"user_tz":420,"elapsed":136,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"7bc4d61b-0f94-445f-eed4-b682f455a622"},"source":["net.eval()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecoQ7GOOj_ml","executionInfo":{"elapsed":194,"status":"ok","timestamp":1630006427031,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"f6c0afab-f130-46a5-819e-09b8fbe95024"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['validation']['meaned_pooled_output']))\n","y_score = softmax(logits, axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['validation']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[{"data":{"text/plain":["0.7507210669380432"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6v9W_FydkEOR","executionInfo":{"elapsed":172,"status":"ok","timestamp":1630006427770,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"f8cedd10-4fe7-4596-a308-576fecafb738"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['test']['meaned_pooled_output']))\n","y_score = softmax(logits, axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['test']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[{"data":{"text/plain":["0.6645011319979305"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"nvjGT-vJkO2x"},"source":["#### Training w Hyperparameter Tuning and Results"]},{"cell_type":"code","metadata":{"id":"DSHANKmHkSzp","executionInfo":{"status":"ok","timestamp":1630369477749,"user_tz":420,"elapsed":146,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["def load_data():\n","  with open(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/80_20/3vvi0uoq:v0/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  trainset = avg_pld_outs_hf_ds['train']\n","  valset = avg_pld_outs_hf_ds['validation']\n","  return trainset, valset\n","\n","def load_test_data():\n","  with open(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/80_20/3vvi0uoq:v0/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  testset = avg_pld_outs_hf_ds['test']\n","  return testset"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QudQKzhlxfl","executionInfo":{"status":"ok","timestamp":1630369478170,"user_tz":420,"elapsed":3,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["trainset, valset = load_data()"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWgUU0IvmCTN","executionInfo":{"status":"ok","timestamp":1630369522448,"user_tz":420,"elapsed":204,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E3_qxxImG_D","executionInfo":{"status":"ok","timestamp":1630369522591,"user_tz":420,"elapsed":2,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"4311b127-6d4f-42ee-cebd-b5b201be868f"},"source":["len(trainloader)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3W-3NSL6vou","executionInfo":{"status":"ok","timestamp":1630367966754,"user_tz":420,"elapsed":307,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"fc252e4a-ecdf-4639-8bd0-f2f2a6099be6"},"source":["print(type(net))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["<class '__main__.Net'>\n"]}]},{"cell_type":"code","metadata":{"id":"1kM4cHtBkqaf","executionInfo":{"status":"ok","timestamp":1630369856454,"user_tz":420,"elapsed":173,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from ray import tune\n","# from ray.tune.integration.wandb import wandb_mixin\n","# '''@wandb_mixin\n","# run = wandb.init()\n","\n","def train_nn(config, checkpoint_dir=None, data_dir=None):\n","  net = Net(transformer_model.pre_classifier, transformer_model.classifier1, config['do_rate'])\n","  net.train()\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      if torch.cuda.device_count() > 1:\n","          net = nn.DataParallel(net)\n","  print(type(net))\n","  net.to(device)\n","  # net.cuda()\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], shuffle=True)\n","\n","  total_iter_steps = len(trainloader)\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","    for i, data in enumerate(trainloader, 0):\n","\n","      inputs = data['meaned_pooled_output']\n","      labels = data['success_label']\n","\n","      inputs, labels = inputs.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = net(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if i % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      net.eval()\n","      with torch.no_grad():\n","        for j, data in enumerate(valloader, 0):\n","\n","          inputs_cpu = data['meaned_pooled_output']\n","          labels_cpu = data['success_label']\n","\n","          inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","          # inputs.cuda()\n","          # labels.cuda()\n","\n","          outputs = net(inputs)\n","          _, predicted = torch.max(outputs.data, 1)\n","\n","          all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","          all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","          loss = criterion(outputs, labels)\n","          val_loss += loss.cpu().numpy()\n","          val_steps += 1\n","\n","      with tune.checkpoint_dir(total_iter_steps * epoch + i) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((net.state_dict(), optimizer.state_dict()), path)\n","\n","      net.train()\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      # s_acc = accuracy_score(all_labels, all_predictions)\n","      # wandb.log({\"val_loss\": val_loss / val_steps, \"val_accuracy\": correct / total})\n","      tune.report(loss=(val_loss / val_steps), accuracy=correct / total, f1=s_f1, precision=s_precision, recall=s_recall)\n","  print(\"Finished Training\")"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBaWnJvfkwCR","executionInfo":{"status":"ok","timestamp":1630369856896,"user_tz":420,"elapsed":5,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["def test_results(net, device=\"cpu\", for_test_set=True):\n","    testset = load_test_data()\n","\n","    if for_test_set:\n","      testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n","    else:\n","      _, valset = load_data()\n","      testloader = torch.utils.data.DataLoader(valset, batch_size=4, shuffle=False)\n","\n","    all_predictions = np.array([])\n","    all_labels = np.array([])\n","\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader, 0):\n","            inputs_cpu = data['meaned_pooled_output']\n","            labels_cpu = data['success_label']\n","\n","            inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","            all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","    s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","    return {\n","        'precision': s_precision,\n","        'recall': s_recall,\n","        'f1': s_f1\n","    }"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aom_a9B_kUL7","executionInfo":{"status":"ok","timestamp":1630369858875,"user_tz":420,"elapsed":145,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n","import torch.optim as optim\n","from functools import partial\n","\n","def main(num_samples = 15, max_num_epochs = 10):\n","  tune_config = {\n","      \"lr\": tune.loguniform(5e-3, 1e-2),\n","      \"batch_size\": tune.choice([16,32]),\n","      \"num_epochs\": tune.choice([2,3,5,10]),#,2,3]),#,2,3,5,10,20]),\n","      \"do_rate\": tune.uniform(0.1, 0.4),\n","    }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2,\n","    metric='f1',\n","    mode='max',)\n","\n","  # scheduler = PopulationBasedTraining(\n","  #     time_attr='time_total_s',\n","  #     metric='f1',\n","  #     mode='max',\n","  #     perturbation_interval=6.0,\n","  #     hyperparam_mutations={\n","  #         \"lr\": [5e-4, 1e-4, 5e-5, 1e-5]\n","  #     })\n","\n","  result = tune.run(\n","    run_or_experiment = partial(train_nn, checkpoint_dir='/tmp/ShallowNNModels'),\n","    config = tune_config,\n","    resources_per_trial={'gpu': 1},\n","    # metric = 'loss',\n","    # mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler)\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"all\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation f1: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = Net(transformer_model.pre_classifier, transformer_model.classifier1, best_trial.config['do_rate'])\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","\n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  print(\"best_checkpoint_dir\", best_checkpoint_dir)\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/ShallowNNModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","\n","  print(\"Test results\")\n","  print(test_results(best_trained_model, device))\n","  return result"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ANZwhwsqk4C9","executionInfo":{"status":"ok","timestamp":1630369894009,"user_tz":420,"elapsed":29973,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"e42499af-26cb-4bb8-cfb2-fce5c87160ca"},"source":["tune_run_result = main(num_samples=1)"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:03,521\tWARNING experiment.py:296 -- No name detected on trainable. Using DEFAULT.\n","2021-08-31 00:31:03,523\tINFO registry.py:67 -- Detected unknown callable for trainable. Converting to class.\n","2021-08-31 00:31:11,488\tWARNING worker.py:1189 -- Warning: The actor ImplicitFunc has size 267908437 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n","2021-08-31 00:31:11,826\tWARNING util.py:164 -- The `start_trial` operation took 3.811 s, which may be a performance bottleneck.\n"]},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.98 GiB heap, 0.0/7.49 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-31_00-31-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_ba97d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\"> 0.393788</td><td style=\"text-align: right;\">0.00679726</td><td style=\"text-align: right;\">          10</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m <class '__main__.Net'>\n","Result for DEFAULT_ba97d_00000:\n","  accuracy: 0.7769784172661871\n","  date: 2021-08-31_00-31-18\n","  done: false\n","  experiment_id: a53a105163d44c0aa27751fabd8afbe8\n","  f1: 0.7693014391519823\n","  hostname: 3cea2778e88a\n","  iterations_since_restore: 1\n","  loss: 0.5295071502526602\n","  node_ip: 172.28.0.2\n","  pid: 1491\n","  precision: 0.7726071357609067\n","  recall: 0.7769784172661871\n","  should_checkpoint: true\n","  time_since_restore: 0.21792078018188477\n","  time_this_iter_s: 0.21792078018188477\n","  time_total_s: 0.21792078018188477\n","  timestamp: 1630369878\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: ba97d_00000\n","  \n"]},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 5.6/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.7693014391519823<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.98 GiB heap, 0.0/7.49 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-31_00-31-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">      f1</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_ba97d_00000</td><td>RUNNING </td><td>172.28.0.2:1491</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\"> 0.393788</td><td style=\"text-align: right;\">0.00679726</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.217921</td><td style=\"text-align: right;\">0.529507</td><td style=\"text-align: right;\">  0.776978</td><td style=\"text-align: right;\">0.769301</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:19,666\tWARNING util.py:164 -- The `process_trial_save` operation took 1.529 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:21,149\tWARNING util.py:164 -- The `process_trial_save` operation took 1.426 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:22,636\tWARNING util.py:164 -- The `process_trial_save` operation took 1.431 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:24,114\tWARNING util.py:164 -- The `process_trial_save` operation took 1.419 s, which may be a performance bottleneck.\n"]},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 5.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: 0.7806158999868829 | Iter 2.000: 0.7775894117242093 | Iter 1.000: 0.7693014391519823<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.98 GiB heap, 0.0/7.49 GiB objects (0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 CPU_group_0_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 CPU_group_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 GPU_group_0_59e84edb2f2cc65818aa8760dbb371a9)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-31_00-31-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">      f1</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_ba97d_00000</td><td>RUNNING </td><td>172.28.0.2:1491</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\"> 0.393788</td><td style=\"text-align: right;\">0.00679726</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.78655</td><td style=\"text-align: right;\">0.540947</td><td style=\"text-align: right;\">  0.784173</td><td style=\"text-align: right;\">0.780616</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_ba97d_00000:\n","  accuracy: 0.7841726618705036\n","  date: 2021-08-31_00-31-24\n","  done: false\n","  experiment_id: a53a105163d44c0aa27751fabd8afbe8\n","  f1: 0.7806158999868829\n","  hostname: 3cea2778e88a\n","  iterations_since_restore: 5\n","  loss: 0.5288541350099776\n","  node_ip: 172.28.0.2\n","  pid: 1491\n","  precision: 0.7802200100384808\n","  recall: 0.7841726618705036\n","  should_checkpoint: true\n","  time_since_restore: 6.2629783153533936\n","  time_this_iter_s: 1.4764275550842285\n","  time_total_s: 6.2629783153533936\n","  timestamp: 1630369884\n","  timesteps_since_restore: 0\n","  training_iteration: 5\n","  trial_id: ba97d_00000\n","  \n","\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:25,598\tWARNING util.py:164 -- The `process_trial_save` operation took 1.424 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:27,063\tWARNING util.py:164 -- The `process_trial_save` operation took 1.409 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:28,671\tWARNING util.py:164 -- The `process_trial_save` operation took 1.553 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:30,179\tWARNING util.py:164 -- The `process_trial_save` operation took 1.440 s, which may be a performance bottleneck.\n"]},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 5.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: 0.7885778561109265 | Iter 4.000: 0.7806158999868829 | Iter 2.000: 0.7775894117242093 | Iter 1.000: 0.7693014391519823<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.98 GiB heap, 0.0/7.49 GiB objects (0.0/1.0 CPU_group_0_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_0_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 GPU_group_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 CPU_group_59e84edb2f2cc65818aa8760dbb371a9)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-31_00-31-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">      f1</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_ba97d_00000</td><td>RUNNING </td><td>172.28.0.2:1491</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\"> 0.393788</td><td style=\"text-align: right;\">0.00679726</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         10.8311</td><td style=\"text-align: right;\">0.527504</td><td style=\"text-align: right;\">  0.791367</td><td style=\"text-align: right;\">0.788578</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_ba97d_00000:\n","  accuracy: 0.7985611510791367\n","  date: 2021-08-31_00-31-30\n","  done: false\n","  experiment_id: a53a105163d44c0aa27751fabd8afbe8\n","  f1: 0.7938942463050835\n","  hostname: 3cea2778e88a\n","  iterations_since_restore: 9\n","  loss: 0.5215916236241659\n","  node_ip: 172.28.0.2\n","  pid: 1491\n","  precision: 0.7950947355604815\n","  recall: 0.7985611510791367\n","  should_checkpoint: true\n","  time_since_restore: 12.330201625823975\n","  time_this_iter_s: 1.4991514682769775\n","  time_total_s: 12.330201625823975\n","  timestamp: 1630369890\n","  timesteps_since_restore: 0\n","  training_iteration: 9\n","  trial_id: ba97d_00000\n","  \n","\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:31,666\tWARNING util.py:164 -- The `process_trial_save` operation took 1.425 s, which may be a performance bottleneck.\n"]},{"output_type":"stream","name":"stdout","text":["Result for DEFAULT_ba97d_00000:\n","  accuracy: 0.7841726618705036\n","  date: 2021-08-31_00-31-31\n","  done: true\n","  experiment_id: a53a105163d44c0aa27751fabd8afbe8\n","  f1: 0.7806158999868829\n","  hostname: 3cea2778e88a\n","  iterations_since_restore: 10\n","  loss: 0.5357038411829207\n","  node_ip: 172.28.0.2\n","  pid: 1491\n","  precision: 0.7802200100384808\n","  recall: 0.7841726618705036\n","  should_checkpoint: true\n","  time_since_restore: 13.812340021133423\n","  time_this_iter_s: 1.4821383953094482\n","  time_total_s: 13.812340021133423\n","  timestamp: 1630369891\n","  timesteps_since_restore: 0\n","  training_iteration: 10\n","  trial_id: ba97d_00000\n","  \n","\u001b[2m\u001b[36m(pid=1491)\u001b[0m [1,    10] loss: 0.498\n","\u001b[2m\u001b[36m(pid=1491)\u001b[0m saving in checkpoint dir\n"]},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:33,140\tWARNING util.py:164 -- The `process_trial_save` operation took 1.414 s, which may be a performance bottleneck.\n"]},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 5.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=1\n","Bracket: Iter 8.000: 0.7885778561109265 | Iter 4.000: 0.7806158999868829 | Iter 2.000: 0.7775894117242093 | Iter 1.000: 0.7693014391519823<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/14.98 GiB heap, 0.0/7.49 GiB objects (0.0/1.0 CPU_group_0_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_0_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 GPU_group_59e84edb2f2cc65818aa8760dbb371a9, 0.0/1.0 CPU_group_59e84edb2f2cc65818aa8760dbb371a9)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-31_00-31-04<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">      f1</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_ba97d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\"> 0.393788</td><td style=\"text-align: right;\">0.00679726</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         13.8123</td><td style=\"text-align: right;\">0.535704</td><td style=\"text-align: right;\">  0.784173</td><td style=\"text-align: right;\">0.780616</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2021-08-31 00:31:33,259\tINFO tune.py:550 -- Total run time: 29.74 seconds (27.26 seconds for the tuning loop).\n"]},{"output_type":"stream","name":"stdout","text":["Best trial config: {'lr': 0.006797255020711621, 'batch_size': 16, 'num_epochs': 10, 'do_rate': 0.3937879223831988}\n","Best trial final validation loss: 0.5357038411829207\n","Best trial final validation f1: 0.7806158999868829\n","best_checkpoint_dir /root/ray_results/DEFAULT_2021-08-31_00-31-04/DEFAULT_ba97d_00000_0_batch_size=16,do_rate=0.39379,lr=0.0067973,num_epochs=10_2021-08-31_00-31-08/checkpoint_000009/\n","Test results\n","{'precision': 0.6959298817805587, 'recall': 0.7034482758620689, 'f1': 0.6979866542244342}\n"]}]},{"cell_type":"code","metadata":{"id":"MiEcQ9rI7Bvk","executionInfo":{"status":"ok","timestamp":1630369906548,"user_tz":420,"elapsed":125,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["best_trial = tune_run_result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"all\")\n","best_ckpt = tune_run_result.get_best_checkpoint(best_trial, metric=\"f1\", mode=\"max\")"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xIP_s0qj_bGm","executionInfo":{"status":"ok","timestamp":1630369907088,"user_tz":420,"elapsed":6,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"4c0b4a83-a23b-4358-d903-c6fd4bcffff4"},"source":["best_trained_model = Net(transformer_model.pre_classifier, transformer_model.classifier1, 0)\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","    # if gpus_per_trial > 1:\n","    #     best_trained_model = nn.DataParallel(best_trained_model)\n","best_trained_model.to(device)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0, inplace=False)\n",")"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4t7YTdQ_Pdv","executionInfo":{"status":"ok","timestamp":1630369907874,"user_tz":420,"elapsed":150,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"0617c3d6-f7b9-4e7a-854b-e527dd76209b"},"source":["  model_state, optimizer_state = torch.load(os.path.join(\n","      best_ckpt, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lMOAdmb_jB4","executionInfo":{"status":"ok","timestamp":1630369908621,"user_tz":420,"elapsed":149,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"c0320af5-fc2d-4bce-9432-1cdabf394e1d"},"source":["test_results(best_trained_model, device)"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'f1': 0.7132518689832534,\n"," 'precision': 0.7124129162880418,\n"," 'recall': 0.7206896551724138}"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FSBq9ZFFzcZ","executionInfo":{"status":"ok","timestamp":1630369909711,"user_tz":420,"elapsed":135,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"c46cd24a-89bd-4a37-ea24-efadf2d049f7"},"source":["test_results(best_trained_model, device, False)"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'f1': 0.7938942463050835,\n"," 'precision': 0.7950947355604815,\n"," 'recall': 0.7985611510791367}"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2tlDefmFr3U","executionInfo":{"status":"ok","timestamp":1630369920105,"user_tz":420,"elapsed":150,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"9bd60670-8c96-4367-a10e-c52795b672c0"},"source":["test_results(best_trained_model, device, True)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'f1': 0.7132518689832534,\n"," 'precision': 0.7124129162880418,\n"," 'recall': 0.7206896551724138}"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"06yYAY6VGLbI","executionInfo":{"status":"ok","timestamp":1630370573342,"user_tz":420,"elapsed":136,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["path = os.path.join(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/ShallowNNModels\", \"f1_7939.pt\")\n","torch.save(best_trained_model.state_dict(), path)"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8195Rq9rb6I"},"source":["# SVM"]},{"cell_type":"code","metadata":{"id":"Psd_tsr7rwkw"},"source":["from sklearn import svm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RWgJonqrzzi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630035612622,"user_tz":420,"elapsed":5136,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"8c9dc68a-eae0-4d7f-dfd7-cba59e866f42"},"source":["cs = np.arange(6, 6.2, 0.02).tolist()\n","best_clf = None\n","best_score = 0\n","best_c = None\n","for c in cs:\n","  clf = svm.SVC(kernel='rbf', gamma='scale', C=c)\n","  clf.fit(avg_pld_outs_hf_ds['train']['meaned_pooled_output'], avg_pld_outs_hf_ds['train']['success_label'])\n","  predictions = clf.predict(avg_pld_outs_hf_ds['validation']['meaned_pooled_output'])\n","  (_, pred_counts) = np.unique(predictions, return_counts=True)\n","  val_score = f1_score(avg_pld_outs_hf_ds['validation']['success_label'], predictions, average = 'weighted')\n","  print('Clf with C = {} obtained val-score of {}'.format(c, val_score))\n","  if (val_score > best_score):\n","    best_score = val_score\n","    best_clf = clf\n","    best_c = c\n","\n","print('\\nBest C: {}; Val-score: {}'.format(best_c, best_score))\n","test_predictions = best_clf.predict(avg_pld_outs_hf_ds['test']['meaned_pooled_output'])\n","test_score = f1_score(avg_pld_outs_hf_ds['test']['success_label'], test_predictions, average = 'weighted')\n","print('Yields score of {} on test set'.format(test_score))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Clf with C = 6.0 obtained val-score of 0.7805388682566458\n","Clf with C = 6.02 obtained val-score of 0.7805388682566458\n","Clf with C = 6.039999999999999 obtained val-score of 0.7805388682566458\n","Clf with C = 6.059999999999999 obtained val-score of 0.7805388682566458\n","Clf with C = 6.079999999999998 obtained val-score of 0.7805388682566458\n","Clf with C = 6.099999999999998 obtained val-score of 0.7805388682566458\n","Clf with C = 6.119999999999997 obtained val-score of 0.7805388682566458\n","Clf with C = 6.139999999999997 obtained val-score of 0.7805388682566458\n","Clf with C = 6.159999999999997 obtained val-score of 0.7805388682566458\n","Clf with C = 6.179999999999996 obtained val-score of 0.7805388682566458\n","Clf with C = 6.199999999999996 obtained val-score of 0.7805388682566458\n","\n","Best C: 6.0; Val-score: 0.7805388682566458\n","Yields score of 0.7363360213723841 on test set\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xh-WGSe1doKZ"},"source":["# RoBERT"]},{"cell_type":"code","metadata":{"id":"XaGB0mqPdTCr"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNINcDJopd1-"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  return book_changes_idx\n","\n","def convert_to_LSTM_dataset_full(dataset):\n","  full_ds = {}\n","  full_ds['train'] = convert_to_LSTM_dataset_sub(dataset['train'])\n","  full_ds['validation'] = convert_to_LSTM_dataset_sub(dataset['validation'])\n","  full_ds['test'] = convert_to_LSTM_dataset_sub(dataset['test'])\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  return full_ds\n","\n","def convert_to_LSTM_dataset_sub(dataset):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  book_start_idx = get_book_changes_idx(dataset['book_title'])\n","  book_start_idx_w_end = np.append(book_start_idx, len(dataset['book_title']))\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = dataset['pooled_outputs'].split_with_sizes(list(book_lengths))\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  # print(type(book_grouped_embeddings))\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = np.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = np.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_slJ4X3Bpf_Q"},"source":["class RoBERT_Model(nn.Module):\n","\n","    def __init__(self, layer_size = 100):\n","        self.layer_size = layer_size\n","        super(RoBERT_Model, self).__init__()\n","        self.lstm = nn.LSTM(768, layer_size, num_layers=1, bidirectional=False)\n","        self.out = nn.Linear(layer_size, 2)\n","\n","    def forward(self, grouped_pooled_outs):\n","        \"\"\" Define how to performed each call\n","        Parameters\n","        __________\n","        pooled_output: array\n","            -\n","        lengt: int\n","            -\n","        Returns:\n","        _______\n","        -\n","        \"\"\"\n","        # chunks_emb = pooled_out.split_with_sizes(lengt) # splits the input tensor into a list of tensors where the length of each sublist is determined by lengt\n","\n","        seq_lengths = torch.LongTensor([x for x in map(len, grouped_pooled_outs)]) # gets the length of each sublist in chunks_emb and returns it as an array\n","\n","        batch_emb_pad = nn.utils.rnn.pad_sequence(grouped_pooled_outs, padding_value=-91, batch_first=True) # pads each sublist in chunks_emb to the largest sublist with value -91\n","        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n","        lstm_input = nn.utils.rnn.pack_padded_sequence(batch_emb, seq_lengths, batch_first=False, enforce_sorted=False) # seq_lengths.cpu().numpy()\n","\n","        packed_output, (h_t, h_c) = self.lstm(lstm_input, )  # (h_t, h_c))\n","        # output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=-91)\n","\n","        h_t = h_t.view(-1, self.layer_size) # (-1, 100)\n","\n","        return self.out(h_t) # logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEPdCiU6phS2"},"source":["def my_collate1(batches):\n","  # for batch in batches:\n","  #   print(type(batch['grouped_pooled_outs']), len(batch['grouped_pooled_outs']))\n","  #   print(type(torch.FloatTensor(batch['grouped_pooled_outs'])))\n","    return {\n","        'grouped_pooled_outs': [torch.stack(x['grouped_pooled_outs']) for x in batches],\n","        'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTNSTUQkpnQp"},"source":["from torch.optim import AdamW\n","import time\n","\n","def load_test_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  testset = full_ds['test']\n","  return testset\n","\n","def load_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  trainset = full_ds['train']\n","  valset = full_ds['validation']\n","  return trainset, valset\n","\n","# def loss_fun(outputs, targets):\n","#     loss = nn.CrossEntropyLoss()\n","#     return loss(outputs, targets)\n","\n","def rnn_train_fun1(config, checkpoint_dir='/tmp/LSTMModels'):\n","  model = RoBERT_Model(config[\"layer_size\"])\n","  model.train()\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #   device = \"cuda:0\"\n","  #   if torch.cuda.device_count() > 1:\n","  #       model = nn.DataParallel(model)\n","  # # print(type(model))\n","  # model.to(device)\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(trainloader):\n","      grouped_pooled_outs = batch['grouped_pooled_outs'] # .to(device)\n","      targets = batch['success_label'] #.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(grouped_pooled_outs)\n","      loss = loss_fun(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if batch_idx % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      with torch.no_grad():\n","          for i, data in enumerate(valloader, 0):\n","\n","              grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","              targets = data['success_label'] # .to(device)\n","\n","              outputs = model(grouped_pooled_outs)\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.numpy())\n","              all_labels = np.append(all_labels, targets.numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6beZxWFXpsXW"},"source":["def test_results(net, device=\"cpu\"):\n","  testset = load_test_data()\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=8, collate_fn=my_collate1)\n","\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.eval()\n","  with torch.no_grad():\n","    for i, data in enumerate(testloader, 0):\n","        grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","        targets = data['success_label'] # .to(device)\n","\n","        outputs = net(grouped_pooled_outs)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxp-MTSnpvZN"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","  config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([16,32,64]),\n","    \"num_epochs\": tune.choice([1,2,3,5]),\n","    \"layer_size\": tune.choice([100]),\n","    \"wandb\": {\n","      \"project\": \"LSTMClassifier\",\n","      \"api_key\": config['WandB']['api_key'],\n","      \"log_config\": True\n","    }\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(rnn_train_fun1, checkpoint_dir='/tmp/LSTMModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"LSTMClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = RoBERT_Model(best_trial.config['layer_size'])\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #     device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/LSTMModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETTb9Z0vpy00"},"source":["test_results = main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55x8tH3RG2o-"},"source":["# ToBERT"]},{"cell_type":"markdown","metadata":{"id":"2hsJkN0qJhyQ"},"source":["### ToBERT dataset"]},{"cell_type":"code","metadata":{"id":"gcqyMEWZn6ga"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = torch.from_numpy(np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0])\n","  book_changes_idx += 1\n","  return book_changes_idx\n","\n","def convert_to_transformer_dataset_full(dataset):\n","  full_ds = {}\n","\n","  full_ds['train'] = convert_to_transformer_dataset_sub(dataset['train'])\n","  full_ds['validation'] = convert_to_transformer_dataset_sub(dataset['validation'])\n","  full_ds['test'] = convert_to_transformer_dataset_sub(dataset['test'])\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  return full_ds\n","\n","def convert_to_transformer_dataset_sub(dataset):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  book_titles = dataset['book_title']\n","  book_start_idx = get_book_changes_idx(book_titles)\n","  book_start_idx_w_end = np.append(book_start_idx, len(book_titles))\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = dataset['pooled_outputs'].split_with_sizes(list(book_lengths))\n","  book_grouped_embeddings = pad_sequence(list(book_grouped_embeddings), batch_first=True)\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","  ds['book_lengths'] = torch.from_numpy(book_lengths)\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = torch.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = torch.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds\n","\n","# def get_max_seq_length(train_book_lengths, val_book_lengths, test_book_lengths):\n","#   return max(max(train_book_lengths),max(val_book_lengths),max(test_book_lengths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkqJ69AXln0L"},"source":["import pytorch_lightning as pl\n","from functools import partial\n","class GoodReadsDataModule(pl.LightningDataModule):\n","\n","  def prepare_data(self):\n","    dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])\n","    self.full_ds = convert_to_transformer_dataset_full(dataset_w_embeddings)\n","\n","  def train_dataloader(self):\n","    trainset = Dataset.from_dict(convert_to_transformer_dataset_sub(dataset_w_embeddings['train']))\n","    trainset.set_format('pytorch', columns=['book_lengths', 'grouped_pooled_outs', 'success_label', 'genre'])\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, collate_fn=partial(self.my_collate1, up_to=None))\n","    return trainloader\n","\n","  def val_dataloader(self):\n","    valset = Dataset.from_dict(convert_to_transformer_dataset_sub(dataset_w_embeddings['validation']))\n","    valset.set_format('pytorch', columns=['book_lengths', 'grouped_pooled_outs', 'success_label', 'genre'])\n","    valloader = torch.utils.data.DataLoader(valset, batch_size=128, collate_fn=partial(self.my_collate1, up_to=None))\n","    return valloader\n","\n","  def test_dataloader(self):\n","    testset = Dataset.from_dict(convert_to_transformer_dataset_sub(dataset_w_embeddings['test']))\n","    testset.set_format('pytorch', columns=['book_lengths', 'grouped_pooled_outs', 'success_label', 'genre'])\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=128, collate_fn=partial(self.my_collate1, up_to=None))\n","    return testloader\n","\n","  def get_batch_mask(self, max_seq_len, book_lens):\n","    mask = torch.zeros(len(book_lens),max_seq_len+1) # batch_size, seq_len\n","    mask[(torch.arange(len(book_lens)),book_lens)] = 1\n","    mask = mask.cumsum(dim=1)[:, :-1]\n","    return mask\n","\n","  def my_collate1(self, batches, up_to=None):\n","    max_seq_len = len(batches[0]['grouped_pooled_outs']) # all sequences were previously padded to the max length\n","    src_key_padding_mask = self.get_batch_mask(max_seq_len, [x['book_lengths'] for x in batches])\n","\n","    if up_to == None:\n","      up_to = max_seq_len\n","\n","    return {\n","        'src_key_padding_mask': src_key_padding_mask[:,:up_to],\n","        'grouped_pooled_outs': torch.stack([torch.stack(x['grouped_pooled_outs']) for x in batches])[:,:up_to,:],\n","        'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SEJUtjZv2lzc"},"source":["### Model Debugging"]},{"cell_type":"code","metadata":{"id":"Oi4FnoK3xMXa","colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"status":"error","timestamp":1630084198062,"user_tz":420,"elapsed":611,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"379ec69c-17d9-4fd8-fcac-0e4100362969"},"source":["gr_dm = GoodReadsDataModule()\n","gr_dm.prepare_data()\n","valloader = gr_dm.val_dataloader()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-4ffe072b8900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgr_dm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoodReadsDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgr_dm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr_dm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 )\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-ee1aaa4e20c2>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdataset_w_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooled_outputs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'success_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_transformer_dataset_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_w_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset_w_embeddings' is not defined"]}]},{"cell_type":"code","metadata":{"id":"g4si01f3xWIL"},"source":["for a, b in enumerate(valloader):\n","  src_key_padding_mask = b['src_key_padding_mask']\n","  grouped_pooled_outs = b['grouped_pooled_outs']\n","  targets = b['success_label']\n","  print(b['src_key_padding_mask'])\n","  input(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VYc2Snw0Ylp"},"source":["src_key_padding_mask[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYy5rGeI0no9"},"source":["import torch.nn as nn\n","encoder_layers = nn.TransformerEncoderLayer(\n","    d_model=768, nhead=2, dim_feedforward=1024, dropout=0.1, batch_first=True\n",")\n","transformer_encoder = nn.TransformerEncoder(\n","    encoder_layers, num_layers=2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxBh0Cb408EG"},"source":["x = transformer_encoder(grouped_pooled_outs, src_key_padding_mask=src_key_padding_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6gNGIRS2Slg"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAwVQJYN1C_M"},"source":["x[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x78V6QE11gU-"},"source":["q = torch.unsqueeze(1-src_key_padding_mask,2)*x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWDo6z-q2aCR"},"source":["q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hqlqquI1iAR"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ht7ZOcIm1nJY"},"source":["x = x.sum(dim=1)/(1-src_key_padding_mask).sum(dim=1).unsqueeze(1) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbH15zou1oYS"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MljeefweL45b"},"source":["# def get_batch_mask(max_seq_len, book_lens):\n","#   mask = torch.zeros(len(book_lens),max_seq_len+1) # batch_size, seq_len\n","#   mask[(torch.arange(len(book_lens)),book_lens)] = 1\n","#   mask = mask.cumsum(dim=1)[:, :-1]\n","#   return mask\n","\n","# def my_collate1(batches):\n","#   # for some reason, the only dictionary values making it here are 'grouped_pooled_outs', 'success_label', and 'genre'\n","#   max_seq_len = len(batches[0]['grouped_pooled_outs']) # all sequences were previously padded to the max length\n","#   src_key_padding_mask = get_batch_mask(max_seq_len, [x['book_lengths'] for x in batches])\n","#   return {\n","#       'src_key_padding_mask': src_key_padding_mask,\n","#       'grouped_pooled_outs': torch.stack([torch.stack(x['grouped_pooled_outs']) for x in batches]),\n","#       'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","#   }\n","\n","# # valset = Dataset.from_dict(convert_to_transformer_dataset_sub(dataset_w_embeddings['validation']))\n","# # valset.set_format('pytorch', columns=['book_lengths', 'grouped_pooled_outs', 'success_label', 'genre'])\n","# # valloader = torch.utils.data.DataLoader(valset, batch_size=64, collate_fn=my_collate1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5ZefHNqMGkE"},"source":["# for batch_idx, batch in enumerate(valloader):\n","#   print(batch['success_label'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfbeQQk-Jt5V"},"source":["### Defining Model"]},{"cell_type":"markdown","metadata":{"id":"81y1MGXaPXPs"},"source":["The Embedding layer:\n","nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n","uses a vector of size 768. Implies that TransformerEncoderLayer works with embeddings of length 768."]},{"cell_type":"code","metadata":{"id":"RWG4yn50LLJA"},"source":["import torch.nn as nn\n","import pytorch_lightning as pl\n","from scipy.special import softmax\n","from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, roc_curve\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbmUNU_cGaMI"},"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, d_model)\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zg46HoiZ-3nK"},"source":["# d_model = 768, nhead = 2, d_hid = 200, dropout = 0.1, nlayers = 2\n","class LightningToBERT(pl.LightningModule):\n","    def __init__(\n","        self,\n","        d_model=768,\n","        nhead=2,\n","        nhid=512,\n","        num_layers=2,\n","        dropout=0.1,\n","        classifier_dropout=0.1,\n","        # max_len=256,\n","    ):\n","\n","        super().__init__()\n","\n","        # self.d_model = embeddings.size(1)\n","        assert (\n","            d_model % nhead == 0\n","        ), \"nheads must divide evenly into d_model\"\n","\n","        # self.emb = nn.Embedding.from_pretrained(embeddings, freeze=False)\n","        # self.pos_encoder = PositionalEncoding(\n","        #     self.d_model, dropout=dropout, max_len=embeddings.size(0)\n","        # )\n","        self.pos_encoder = PositionalEncoding(\n","            d_model, dropout=dropout, max_len=200\n","        )\n","\n","        encoder_layers = nn.TransformerEncoderLayer(\n","            d_model=d_model, nhead=nhead, dim_feedforward=nhid, dropout=dropout, batch_first=True\n","        )\n","        self.transformer_encoder = nn.TransformerEncoder(\n","            encoder_layers, num_layers=num_layers\n","        )\n","\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.pre_classifier = nn.Linear(d_model, d_model)\n","        self.classifier = nn.Linear(d_model, 2)\n","        # self.classifier = nn.Sequential(\n","        #     # Other layers to go here if needed once things seem to be working\n","        #     nn.Linear(d_model, 2),\n","        # )\n","\n","        self.softmaxer = nn.Softmax(dim=1)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=5e-6)\n","        return optimizer\n","\n","    def cross_entropy_loss(self, logits, labels):\n","        loss = nn.CrossEntropyLoss()\n","        return loss(logits, labels)\n","\n","    def forward(self, x, src_key_padding_mask):\n","        # x = self.emb(x) * math.sqrt(self.d_model)\n","        # x = self.pos_encoder(x)\n","        x = self.pos_encoder(x)\n","        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)  # self.src_mask)\n","\n","        # calculates mean taking into account the padding\n","        x = torch.unsqueeze(1-src_key_padding_mask,2)*x\n","        x = x.sum(dim=1)/(1-src_key_padding_mask).sum(dim=1).unsqueeze(1)  \n","\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        return self.classifier(x)\n","        # x = self.dropout(x) RIGHT AFTER x = x.sum(dim=1)/(1-src_key_padding_mask).sum(dim=1).unsqueeze(1) \n","        # return self.classifier(x)\n","\n","    def training_step(self, train_batch, batch_idx):\n","        grouped_pooled_outs = train_batch['grouped_pooled_outs']\n","        src_key_padding_mask = train_batch['src_key_padding_mask']\n","        targets = train_batch['success_label']\n","\n","        logits = self.forward(grouped_pooled_outs, src_key_padding_mask)\n","        loss = self.cross_entropy_loss(logits, targets)\n","\n","        self.log('train_loss', loss, prog_bar=True)\n","        log_dict = {'loss': loss}\n","        return {'loss': loss, 'log': log_dict}\n","\n","    def validation_step(self, val_batch, batch_idx):\n","        grouped_pooled_outs = val_batch['grouped_pooled_outs']\n","        src_key_padding_mask = val_batch['src_key_padding_mask']\n","        targets = val_batch['success_label']\n","\n","        logits = self.forward(grouped_pooled_outs, src_key_padding_mask)\n","        y_prob = self.softmaxer(logits)[:, 1]\n","        y_pred = (y_prob>0.5).float()\n","\n","        loss = self.cross_entropy_loss(logits, targets)\n","        return {'val_loss': loss, 'preds': y_pred, 'targets': targets.tolist()}\n","\n","    def test_step(self, batch, batch_idx, dataloader_idx = None):\n","        grouped_pooled_outs = batch['grouped_pooled_outs']\n","        src_key_padding_mask = batch['src_key_padding_mask']\n","        targets = batch['success_label']\n","\n","        logits = self.forward(grouped_pooled_outs, src_key_padding_mask)\n","        y_probs = self.softmaxer(logits)[:, 1]\n","        return {'class_probs': y_probs, 'targets': targets.tolist()}\n","\n","    def test_epoch_end(self, test_step_outputs):\n","        y_probs = []\n","        y_true = []\n","\n","        for x in test_step_outputs:\n","          y_probs.extend(x['class_probs'].tolist())\n","          y_true.extend(x['targets'])\n","\n","        f1_res = f1_score(y_true, y_pred, average = 'weighted')\n","        return {'f1': f1_res}\n","\n","\n","    def validation_epoch_end(self, val_step_outputs):\n","        y_pred = []\n","        y_true = []\n","\n","        for x in val_step_outputs:\n","          y_pred.extend(x['preds'].tolist())\n","          y_true.extend(x['targets'])\n","\n","        f1_res = f1_score(y_true, y_pred, average = 'weighted')\n","        avg_val_loss = torch.tensor([x['val_loss'] for x in val_step_outputs]).mean()\n","\n","        log_dict = {\n","            'val_loss': avg_val_loss,\n","            'val_f1': f1_res\n","        }\n","\n","        self.log('val_loss', avg_val_loss, prog_bar=True)\n","        self.log('val_f1', f1_res, prog_bar=True)\n","        return {'val_loss': avg_val_loss, 'log': log_dict}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWMns1fFjhiR"},"source":["hit 0.781 with nhead=2, num_layers=2, lr=1e-5 max_epochs=20 at epoch 19\n","\n","0.7632 with nhead=8, num_layers=1, lr=1e-5 max_epochs=20 at epoch 50\n","\n","0.772 with nhead=2, num_layers=1, lr=3e-5 max_epochs=20 at epoch 19"]},{"cell_type":"code","metadata":{"id":"hKOxNw5RWhNN"},"source":["model = LightningToBERT()\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath='/content/version_1_logs', \n","    filename='{epoch}-{val_loss:.2f}-{val_f1:.2f}',\n","    monitor=\"val_loss\", \n","    every_n_epochs=1,\n","    save_top_k=3\n",")\n","trainer = pl.Trainer(resume_from_checkpoint=\"/content/lightning_logs/epoch=5-val_loss=0.45-val_f1=0.79.ckpt\", max_epochs=50, callbacks=[checkpoint_callback])\n","\n","# automatically restores model, epoch, step, LR schedulers, apex, etc...\n","datamodule = GoodReadsDataModule()\n","trainer.fit(model, datamodule)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqUtiIWFxiYw"},"source":["# '/content/lightning_logs/version_' + str(v_num) + '/checkpoints'\n","\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath='/content/lightning_logs', \n","    filename='{epoch}-{val_loss:.2f}-{val_f1:.2f}',\n","    monitor=\"val_loss\", \n","    every_n_train_steps=1,\n","    save_top_k=3\n",")\n","\n","trainer = pl.Trainer(log_every_n_steps=1, gpus=1, max_epochs=20, callbacks=[checkpoint_callback], num_sanity_val_steps=0)\n","model = LightningToBERT(nhead=2, num_layers=2, dropout=0.15)\n","\n","datamodule = GoodReadsDataModule()\n","trainer.fit(model, datamodule)\n","# v_num+=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyyVFRkws4Dt"},"source":["#### Test Model"]},{"cell_type":"code","metadata":{"id":"GRbMH-kNu7kT"},"source":["gr_dm = GoodReadsDataModule()\n","gr_dm.prepare_data()\n","testloader = gr_dm.test_dataloader()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8okrNzxsztu"},"source":["model  = LightningToBERT.load_from_checkpoint(\"/content/lightning_logs/epoch=5-val_loss=0.45-val_f1=0.79.ckpt\")\n","model.to('cuda')\n","\n","y_hats = []\n","y_trues = []\n","with torch.no_grad():\n","  for a, b in enumerate(testloader):\n","    src_key_padding_mask = b['src_key_padding_mask'].to('cuda')\n","    grouped_pooled_outs = b['grouped_pooled_outs'].to('cuda')\n","    targets = b['success_label'].tolist()\n","\n","    y_hat = model(grouped_pooled_outs, src_key_padding_mask).to('cpu').tolist()\n","    y_hats.extend(y_hat)\n","    y_trues.extend(targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBGLAi5BxxuZ"},"source":["probabilities_per_book = softmax(y_hats, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sh-A4G-Qx4bh"},"source":["y_score = probabilities_per_book[:,1].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jjk142vD6uvi"},"source":["y_pred = [math.floor(input) if input < 0.5 else math.ceil(input) for input in y_score]\n","f1_res = f1_score(y_trues, y_pred, average = 'weighted')\n","print(f1_res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfotfjnEypZK"},"source":["def get_f1_for_validation(y_score, y_true, l_th = 0.4, u_th = 0.8):\n","  thresholds = np.arange(l_th, u_th, 0.01)\n","  f1_scores = []\n","  for th in thresholds:\n","    y_pred = [math.floor(input) if input < th else math.ceil(input) for input in y_score]\n","    f1_res = f1_score(y_true, y_pred, average = 'weighted')\n","    f1_scores.append(f1_res)\n","  max_f1 = max(f1_scores)\n","  max_f1_index = f1_scores.index(max_f1)\n","  # self.validated_threshold = thresholds[max_f1_index]\n","  f1_scores_and_thresholds = {'thresholds': thresholds, 'f1_scores': f1_scores, 'max_f1_index': max_f1_index}\n","  return f1_scores_and_thresholds\n","\n","f1_scores_and_thresholds = get_f1_for_validation(y_score, y_trues)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5Aqjg5JzR96"},"source":["import matplotlib\n","matplotlib.pyplot.plot(f1_scores_and_thresholds['thresholds'], f1_scores_and_thresholds['f1_scores'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9ixDqsgvOd9"},"source":["y_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZ8fGjdURkx8"},"source":["# test_model.eval()\n","# y_hats = []\n","\n","# for a, b in enumerate(valloader):\n","#   src_key_padding_mask = b['src_key_padding_mask']\n","#   grouped_pooled_outs = b['grouped_pooled_outs']\n","#   targets = b['success_label']\n","\n","#   y_hat = test_model(grouped_pooled_outs, src_key_padding_mask)\n","#   targets.append(targets.tolist())\n","#   y_hats.append(y_hat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E6ag2yhGs9r9"},"source":["#### View TensorBoard"]},{"cell_type":"code","metadata":{"id":"EaBQdH1UlbbZ"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","%tensorboard --logdir ./lightning_logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST5uYHfbK2mL"},"source":["# class ToBERT(nn.Module):\n","\n","#   def __init__(self, d_model, nhead, dropout, d_hid, nlayers, nclasses):\n","#       # d_model = 768, nhead = 2, d_hid = 200, dropout = 0.1, nlayers = 2\n","#       encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n","#       self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","#       self.classifier = nn.Linear(d_model, nclasses)\n","\n","#   def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n","#       \"\"\"\n","#       Args:\n","#           src: Tensor, shape [seq_len, embedding_dim, batch_size]\n","#           src_mask: Tensor, shape [seq_len, seq_len]\n","\n","#       seq_len should be the max number of segments a book has in our dataset\n","#       embedding_dim will be 768 (from BERT)\n","\n","#       src_mask is necessary because we will need to pad shorter books to have as many segments\n","#       as the longest book. Obviously we do not want our model to attend to the padded tokens in\n","#       these cases.\n","#       \"\"\"\n","#       output = self.transformer_encoder(src, src_mask) \n","#       output = self.classifier(output)\n","#       return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coGEf4xLdjnj"},"source":["### Training (Pytorch)"]},{"cell_type":"code","metadata":{"id":"j04rGEarmw0S"},"source":["from torch.optim import AdamW\n","import time\n","\n","def loss_fun(outputs, targets):\n","    loss = nn.CrossEntropyLoss()\n","    return loss(outputs, targets)\n","\n","def transformer_train_fun1(config, checkpoint_dir='/tmp/ToBERTModels'):\n","\n","  model = ToBERT(768, config[\"nhead\"], config[\"nhid\"], config[\"num_layers\"], config[\"dropout\"], config[\"dropout\"])\n","  model.train()\n","  device = \"cpu\"\n","\n","  if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","    if torch.cuda.device_count() > 1:\n","        model = nn.DataParallel(model)\n","  # print(type(model))\n","  model.to(device)\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    # epoch_steps = 0\n","    for tr_batch_idx, tr_batch in enumerate(trainloader):\n","      grouped_pooled_outs = tr_batch['grouped_pooled_outs'].to(device) # .to(device)\n","      src_key_padding_mask = tr_batch['src_key_padding_mask'].to(device)\n","      targets = tr_batch['success_label'].to(device) #.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = model(grouped_pooled_outs, src_key_padding_mask)\n","      loss = loss_fun(outputs, targets)\n","      loss.backward()\n","      # model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      # epoch_steps += 1\n","\n","      print('[%d, %5d] loss: %.3f' %\n","            (epoch + 1, tr_batch_idx + 1, running_loss))\n","      running_loss = 0.0\n","\n","      val_loss = 0.0\n","      val_steps = 0\n","\n","      all_predictions = torch.tensor([], dtype=torch.long, device=device) # np.array([])\n","      all_labels = torch.tensor([], dtype=torch.long, device=device)# np.array([])\n","\n","      with torch.no_grad():\n","          for val_batch_idx, val_batch in enumerate(valloader, 0):\n","\n","              grouped_pooled_outs = val_batch['grouped_pooled_outs'].to(device) # .to(device)\n","              src_key_padding_mask = val_batch['src_key_padding_mask'].to(device)\n","              targets = val_batch['success_label'].to(device) # .to(device)\n","\n","              outputs = model(grouped_pooled_outs, src_key_padding_mask)\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              # all_predictions = np.append(all_predictions, predicted.numpy())\n","              # all_labels = np.append(all_labels, targets.numpy())\n","              all_predictions = torch.cat((all_predictions, predicted), 0)\n","              all_labels = torch.cat((all_labels, targets), 0)\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      # with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","      #     print(\"saving in checkpoint dir\")\n","      #     path = os.path.join(checkpoint_dir, \"checkpoint\")\n","      #     torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      all_labels = all_labels.cpu().numpy()  \n","      all_predictions = all_predictions.cpu().numpy()\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      print('[%d, %5d] loss: %.4f; f1: %.4f; precision: %.4f; recall: %.4f' % (epoch + 1, tr_batch_idx + 1, val_loss, s_f1, s_precision, s_recall))\n","      # tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUg4XxkTi1-R"},"source":["sample_config = {\n","    \"lr\": 0.0001, #tune.loguniform(5e-4, 5e-2),\n","    \"nhead\": 2,\n","    \"nhid\": 200,\n","    \"num_layers\": 1,\n","    \"dropout\": 0.1,\n","    \"batch_size\": 128,\n","    \"num_epochs\": 30,\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7Wv8ISPjFtp"},"source":["transformer_train_fun1(sample_config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWU6ZQlIhYNO"},"source":["#### Hyperparameter Search"]},{"cell_type":"code","metadata":{"id":"iY5xAPojpx8d"},"source":["def test_results(net, device=\"cpu\"):\n","  testset = load_test_data()\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=8, collate_fn=my_collate1)\n","\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.eval()\n","  with torch.no_grad():\n","    for test_batch_idx, test_batch in enumerate(testloader, 0):\n","        grouped_pooled_outs = test_batch['grouped_pooled_outs'].to(device) # .to(device)\n","        src_key_padding_mask = test_batch['src_key_padding_mask'].to(device)\n","        targets = test_batch['success_label'].to(device) # .to(device)\n","\n","        outputs = net(grouped_pooled_outs, src_key_padding_mask)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWAPmvCnqU4-"},"source":["def main(num_samples = 6, max_num_epochs = 30):\n","# config[\"nhead\"], config[\"nhid\"], config[\"num_layers\"], config[\"dropout\"], config[\"dropout\"]\n","  tune_config = {\n","    \"lr\": tune.choice([0.00001]), #tune.loguniform(5e-4, 5e-2),\n","    \"nhead\": tune.choice([2]),\n","    \"nhid\": tune.choice([200]),\n","    \"num_layers\": tune.choice([1]),\n","    \"dropout\": tune.choice([0.1]),\n","    \"batch_size\": tune.choice([128]),\n","    \"num_epochs\": tune.choice([30]),\n","    \"wandb\": {\n","      \"project\": \"ToBERTClassifier\",\n","      \"api_key\": config['WandB']['api_key'],\n","      \"log_config\": True\n","    }\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(transformer_train_fun1, checkpoint_dir='/tmp/ToBERTModels'),\n","    config = tune_config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"ToBERTClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = ToBERT(768, best_trial.config[\"nhead\"], \n","                        best_trial.config[\"nhid\"], best_trial.config[\"num_layers\"], \n","                        best_trial.config[\"dropout\"], best_trial.config[\"dropout\"])\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/LSTMModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRZGw1UvveKj"},"source":["test_results = main(num_samples = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mj191iy1JxE3"},"source":["### Playground"]},{"cell_type":"markdown","metadata":{"id":"JejUCDYSO5MS"},"source":["we can see that the values of the 1st and 2nd tensors didnt change when we applied the masking properly"]},{"cell_type":"code","metadata":{"id":"YwtfJrmL3vAk"},"source":["def get_batch_mask(max_seq_len, book_lens):\n","  mask = torch.zeros(len(book_lens),max_seq_len+1) # batch_size, seq_len\n","  mask[(torch.arange(len(book_lens)),book_lens)] = 1\n","  mask = mask.cumsum(dim=1)[:, :-1]\n","  return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JY8jZC4D4yzg"},"source":["book_lens = torch.LongTensor([2,4,5])\n","max_seq_len = 6\n","src_key_padding_mask = get_batch_mask(max_seq_len, book_lens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HK8N_vPh6riY"},"source":["src_key_padding_mask.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQT6yRnu6Uxq"},"source":["import torch, torch.nn as nn\n","q = torch.randn(3, 6, 10) # batch size 3, source sequence length 6, embedding size 10\n","attn = nn.MultiheadAttention(10, 1, batch_first=True) # embedding size 10, one head\n","\n","ay = attn(q, q, q, key_padding_mask=src_key_padding_mask) # self attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1eC_4ao_X4P"},"source":["src_key_padding_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_KWWEgoD7v8"},"source":["y = torch.unsqueeze(1-src_key_padding_mask,2)*ay[0]\n","y.sum(dim=1)/(1-src_key_padding_mask).sum(dim=1).unsqueeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kw2weKz-lg6"},"source":["# MultiModal"]},{"cell_type":"markdown","metadata":{"id":"6c612HylViH5"},"source":["### Defining the Model"]},{"cell_type":"code","metadata":{"id":"ZRbTSOAuasG1"},"source":["import torch.nn as nn\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gVjETdBcQi2"},"source":["Our model will be composed of three separate modules:\n","\n","1. (Normalizer) Responsible for taking all the inputs of various dimensions and feeding them each through their own linear layer to project them into a space with all the same dimensions\n","\n","In essence, it is responsible for eq (1) in the paper $h_i=selu(W_{h_i} x_i + b_h)$\n","\n","\n","2. (GenreAwareAttention) This is where most of the meat of the model is. It is responsible for performing these 3 equations. \n","\n","$score(h_i, g) = v^T selu(W_a h_i + W_g g + b_a)$\n","\n","$\\alpha_i = \\frac{exp(score(h_i,g))}{\\sum_{i'}exp(score(h_{i'},g)}$\n","\n","$r=\\sum_i \\alpha_i h_i$\n","\n","3. (ClassOutput) The last layer is simply responsible for projecting the book representation to class probabilities.\n","\n","$\\hat{p}=\\sigma(W_c r + b_c)$"]},{"cell_type":"code","metadata":{"id":"U0vC_Exg-nTp"},"source":["class Normalizer(nn.Module):\n","  def __init__(self, c5g_size, bf_size, std_dims):\n","    super(Normalizer, self).__init__()\n","\n","    self.c5g_linear = nn.Linear(c5g_size, std_dims)\n","    self.bf_linear = nn.Linear(bf_size, std_dims)\n","\n","  def forward(self, x_c5g, x_bf):\n","    # x_c5g ~ (BATCH_SIZE, C5G_FEATURE_SIZE)\n","    # x_bf ~ (BATCH_SIZE, BF_FEATURE_SIZE)\n","    # # split features into char_5_gram and bert_features\n","    # char_5_grams = None\n","    # bert_features = None\n","\n","    c5g_normed = self.c5g_linear(x_c5g)\n","    bf_normed = self.bf_linear(x_bf)\n","\n","    # concatenate c5g_normed and bf_normed\n","    return torch.stack([c5g_normed, bf_normed], 1) # (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzj5ackHOUQu"},"source":["class GenreAwareAttention(nn.Module):\n","  def __init__(self, std_dims, num_units, do_rate):\n","    super(GenreAwareAttention, self).__init__()\n","    self.activation = nn.SELU()\n","    self.nn_softmax = nn.Softmax(dim=1)\n","\n","    self.v = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,1)),\n","        requires_grad=True\n","    )\n","\n","    self.Wa = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(std_dims,num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.b = nn.parameter.Parameter(\n","        nn.init.ones_(torch.empty(num_units,)),\n","        requires_grad=True\n","    )\n","\n","    self.Wg = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(8, num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.dropout = nn.Dropout(p=do_rate)\n","\n","  def forward(self, x, g):\n","    # x ~ (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)\n","    # g ~ (BATCH_SIZE, 1, GENRE_EMBED_SIZE)\n","    \n","    # calculate scores\n","    atten_g = torch.mm(g, self.Wg).unsqueeze(dim=1)\n","    et = self.activation(torch.matmul(x, self.Wa) + atten_g + self.b)\n","    et = self.dropout(et)\n","    \n","    et = torch.matmul(et, self.v)\n","\n","    at = self.nn_softmax(et)\n","\n","    # at = torch.unsqueeze(at, axis=-1)\n","\n","    # print('at:', at.size())\n","    # print('x:', x.size())\n","    ot = at * x # canot multiply at: torch.Size([4, 2, 1, 1]) x: torch.Size([4, 2, 100])\n","\n","    return torch.sum(ot, axis=1) # BATCH_SIZE, EMBED_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxcCXEr4iPWB"},"source":["class ClassifierOut(nn.Module):\n","  def __init__(self, std_dims):\n","    super(ClassifierOut, self).__init__()\n","    self.classifier = nn.Linear(std_dims, 2)\n","  \n","  def forward(self, r): # r ~ BATCH_SIZE, EMBED_SIZE\n","    r_out = self.classifier(r) # BATCH_SIZE, 2\n","    return torch.sigmoid(r_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sb7CIVGIGD-G"},"source":["# class FullModel(nn.Module): # may want to consider also adding a dropout layer before classification\n","#   def __init__(self, c5g_size, bf_size, std_dims, num_units, do_rate):\n","#     super(FullModel,self).__init__()\n","#     self.normalizer = Normalizer(c5g_size, bf_size, std_dims)\n","#     self.genre_aware_attention = GenreAwareAttention(std_dims, num_units, do_rate)\n","#     self.classifier_out = ClassifierOut(std_dims)\n","\n","#   def forward(self, x_c5g, x_bf, genre):\n","#     x_normed = self.normalizer(x_c5g, x_bf)\n","#     g_a_a = self.genre_aware_attention(x_normed, genre)\n","#     return self.classifier_out(g_a_a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QUwEU-H1PPr"},"source":["# d_model = 768, nhead = 2, d_hid = 200, dropout = 0.1, nlayers = 2\n","class FullModel(pl.LightningModule):\n","    def __init__(self, c5g_size, bf_size, std_dims, num_units, do_rate):\n","      super(FullModel,self).__init__()\n","      self.normalizer = Normalizer(c5g_size, bf_size, std_dims)\n","      self.genre_aware_attention = GenreAwareAttention(std_dims, num_units, do_rate)\n","      self.classifier_out = ClassifierOut(std_dims)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)\n","        return optimizer\n","\n","    def cross_entropy_loss(self, logits, labels):\n","        loss = nn.CrossEntropyLoss()\n","        return loss(logits, labels)\n","\n","    def forward(self, x_c5g, x_bf, genre):\n","      x_normed = self.normalizer(x_c5g, x_bf)\n","      g_a_a = self.genre_aware_attention(x_normed, genre)\n","      return self.classifier_out(g_a_a)\n","\n","    def training_step(self, train_batch, batch_idx):\n","        c5g_f = batch['c5g_f']\n","        bert_f = batch['bert_f']\n","        genre = batch['genre']\n","        targets = batch['label']\n","\n","        outputs = self.forward(c5g_f, bert_f, genre)\n","        loss = self.cross_entropy_loss(outputs, targets)\n","\n","        self.log('train_loss', loss, prog_bar=True)\n","        log_dict = {'loss': loss}\n","        return {'loss': loss, 'log': log_dict}\n","\n","    def validation_step(self, val_batch, batch_idx):\n","        c5g_f = batch['c5g_f']\n","        bert_f = batch['bert_f']\n","        genre = batch['genre']\n","        targets = batch['label']\n","\n","        logits = self.forward(c5g_f, bert_f, genre)\n","        y_prob = self.softmaxer(logits)[:, 1]\n","        y_pred = (y_prob>0.5).float()\n","\n","        loss = self.cross_entropy_loss(logits, targets)\n","        return {'val_loss': loss, 'preds': y_pred, 'targets': targets.tolist()}\n","\n","    def validation_epoch_end(self, val_step_outputs):\n","        y_pred = []\n","        y_true = []\n","\n","        for x in val_step_outputs:\n","          y_pred.extend(x['preds'].tolist())\n","          y_true.extend(x['targets'])\n","\n","        f1_res = f1_score(y_true, y_pred, average = 'weighted')\n","        avg_val_loss = torch.tensor([x['val_loss'] for x in val_step_outputs]).mean()\n","\n","        log_dict = {\n","            'val_loss': avg_val_loss,\n","            'val_f1': f1_res\n","        }\n","\n","        self.log('val_loss', avg_val_loss, prog_bar=True)\n","        self.log('val_f1', f1_res, prog_bar=True)\n","        return {'val_loss': avg_val_loss, 'log': log_dict}\n","\n","    def test_step(self, batch, batch_idx, dataloader_idx = None):\n","        c5g_f = batch['c5g_f']\n","        bert_f = batch['bert_f']\n","        genre = batch['genre']\n","        targets = batch['label']\n","\n","        logits = self.forward(grouped_pooled_outs, src_key_padding_mask)\n","        y_probs = self.softmaxer(logits)[:, 1]\n","        y_pred = (y_prob>0.5).float()\n","\n","        return {'preds': y_pred, 'targets': targets.tolist()}\n","\n","    def test_epoch_end(self, test_step_outputs):\n","        y_preds = []\n","        y_true = []\n","\n","        for x in test_step_outputs:\n","          y_preds.extend(x['preds'].tolist())\n","          y_true.extend(x['targets'])\n","\n","        f1_res = f1_score(y_true, y_pred, average = 'weighted')\n","        return {'f1': f1_res}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQO49DFsVmUr"},"source":["### Getting the Data"]},{"cell_type":"code","metadata":{"id":"r62qM0beUEER"},"source":["from MultimodalGoodreadsDataset import MultimodalGoodreadsDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma0olfGhVnaI","executionInfo":{"status":"ok","timestamp":1630037199279,"user_tz":420,"elapsed":20585,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"8d671574-0a58-42fe-a3d3-50f5d11c82bd"},"source":["dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'\n","\n","ds = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)\n","\n","def my_collate_fn(batches, f1_len, f2_len):\n","    return {\n","        'c5g_f': torch.tensor([x['text_features'].toarray()[0][0:f1_len] for x in batches]), # dtype = Float?\n","        'bert_f': torch.tensor([x['text_features'].toarray()[0][f1_len:f1_len+f2_len] for x in batches]), \n","        'genre': torch.tensor([x['genre'] for x in batches]),\n","        'label': torch.tensor([x['label'] for x in batches])\n","    }\n","\n","# c5g_len = ds.f_lengths[0]\n","# bf_len = ds.f_lengths[1]\n","\n","# train_dataloader = DataLoader(ds.train, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))\n","# val_dataloader = DataLoader(ds.val, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))\n","# test_dataloader = DataLoader(ds.test, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Path to yaml: /content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed/train_test_val_80_20_split_goodreads.yaml\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/readers/corpus.py:61: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  data = yaml.load(stream)\n"],"name":"stderr"},{"output_type":"stream","text":["Total test instances: 290, validation instances: 139, and Training instances: 555\n","Total unique books: 984\n","Training instances (555,), Val instances (139,), Test instances (290,)\n","extracting feature: char_5_gram\n","Using cached features\n","extracting feature: bert_features\n","Using cached features\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeNghsJ-4M9v"},"source":["# from pydrive.auth import GoogleAuth\n","# from pydrive.drive import GoogleDrive\n","# from google.colab import auth\n","# from oauth2client.client import GoogleCredentials\n","\n","# import pickle\n","\n","# # 1. Authenticate and create the PyDrive client.\n","# auth.authenticate_user()\n","# gauth = GoogleAuth()\n","# gauth.credentials = GoogleCredentials.get_application_default()\n","# drive = GoogleDrive(gauth)  \n","\n","# with open('train_dataset.pkl', 'wb') as output_file:\n","#   pickle.dump(ds.train, output_file)\n","\n","# with open('val_dataset.pkl', 'wb') as output_file:\n","#   pickle.dump(ds.val, output_file)\n","\n","# with open('test_dataset.pkl', 'wb') as output_file:\n","#   pickle.dump(ds.test, output_file)\n","\n","# folder_id = '1q2IGZrQ9oNwP-CqttWUuiYcenb8vmWUg'\n","# # get the folder id where you want to save your file\n","# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","# file.SetContentFile('train_dataset.pkl')\n","# file.Upload() \n","\n","# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","# file.SetContentFile('val_dataset.pkl')\n","# file.Upload() \n","\n","# # get the folder id where you want to save your file\n","# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","# file.SetContentFile('test_dataset.pkl')\n","# file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_f7F3QWf6Gw"},"source":["from datasets import Dataset\n","class MultimodalGoodreadsDatasetSplit(Dataset):\n","\n","    def __init__(self, X, genres, Y):\n","        self.X = X\n","        self.genres = genres\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.Y)\n","\n","    def __getitem__(self, idx):\n","        return {'text_features': self.X[idx], 'genre': self.genres[idx], 'label': self.Y[idx]}\n","\n","mmgrds_train = MultimodalGoodreadsDatasetSplit(ds.train.X, ds.train.genres, ds.train.Y)\n","mmgrds_val = MultimodalGoodreadsDatasetSplit(ds.val.X, ds.val.genres, ds.val.Y)\n","mmgrds_test = MultimodalGoodreadsDatasetSplit(ds.test.X, ds.test.genres, ds.test.Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eTMCXp0Y8PA"},"source":["from torch.utils.data import DataLoader\n","import torch\n","from functools import partial\n","\n","def my_collate_fn(batches, f1_len, f2_len):\n","    return {\n","        'c5g_f': torch.tensor([x['text_features'].toarray()[0][0:f1_len] for x in batches]), # dtype = Float?\n","        'bert_f': torch.tensor([x['text_features'].toarray()[0][f1_len:f1_len+f2_len] for x in batches]), \n","        'genre': torch.tensor([x['genre'] for x in batches]),\n","        'label': torch.tensor([x['label'] for x in batches])\n","    }\n","\n","def load_data():\n","  return mmgrds_train, mmgrds_val\n","\n","def load_test_data():\n","  return mmgrds_test\n","# def load_data():\n","#   dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","#   cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'\n","\n","#   ds = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)\n","\n","#   return ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZezPhKzsZd6H"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"kRTXe1tzWUAz"},"source":["from torch.optim import AdamW\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support \n","\n","def mm_train_fun1(config, checkpoint_dir='/tmp/MultiModalModels'):\n","\n","  train_dataset, val_dataset = load_data()\n","  model = FullModel(311595, 768, config['std_dims'], config['num_units'], config['do_rate']).to('cuda')\n","  model.train()\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","  # [311595, 768]\n","  train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=311595, f2_len=768))\n","  val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=311595, f2_len=768))\n","  print(\"len(train_dataloader)\",len(train_dataloader))\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(train_dataloader):\n","\n","\n","      c5g_f = batch['c5g_f'].to('cuda')\n","      bert_f = batch['bert_f'].to('cuda')\n","      genre = batch['genre'].to('cuda')\n","      targets = batch['label'].to('cuda')\n","\n","      optimizer.zero_grad()\n","      outputs = model(c5g_f.float(), bert_f.float(), genre.float())\n","      loss = criterion(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      model.eval()\n","      with torch.no_grad():\n","          for i, batch_v in enumerate(val_dataloader, 0):\n","\n","              c5g_f = batch_v['c5g_f'].to('cuda')\n","              bert_f = batch_v['bert_f'].to('cuda')\n","              genre = batch_v['genre'].to('cuda')\n","              targets = batch_v['label'].to('cuda')\n","\n","              outputs = model(c5g_f.float(), bert_f.float(), genre.float())\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.cpu().numpy())\n","              all_labels = np.append(all_labels, targets.cpu().numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      model.train()\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      print('s_precision:', s_precision, 's_recall:', s_recall, 's_f1:', s_f1)\n","      tune.report(loss = loss.item(), epoch = epoch + batch_idx / len(train_dataloader), eval_loss=(val_loss / val_steps), eval_f1=s_f1, eval_precision=s_precision, eval_recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nM6WLGVqxIDY"},"source":["def test_results(net, test_dataloader, device=\"cpu\"):\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.to(device)\n","  net.eval()\n","  with torch.no_grad():\n","    for i, batch_test in enumerate(test_dataloader, 0):\n","        c5g_f = batch_test['c5g_f']\n","        bert_f = batch_test['bert_f']\n","        genre = batch_test['genre']\n","        targets = batch_test['label']\n","\n","        outputs = net(c5g_f.float(), bert_f.float(), genre.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIqVS_dlBVbK"},"source":["from ray import tune\n","from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger, WandbLoggerCallback\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","import os\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","\n","  tune_config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([32,64,128]),\n","    \"num_epochs\": tune.choice([1]),#,3,5,7,9]),\n","    \"std_dims\": tune.sample_from(lambda _: np.random.randint(50,300)),\n","    \"num_units\": tune.sample_from(lambda spec: np.random.randint(25,spec.config.std_dims)),\n","    \"do_rate\": tune.uniform(0, 0.5),\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(mm_train_fun1, checkpoint_dir='/tmp/MMModels'),\n","    config = tune_config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'eval_loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"MultiModalClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  best_trial = result.get_best_trial(metric=\"eval_f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"eval_loss\"]))\n","  print(\"Best trial final validation weighted f1: {}\".format(\n","      best_trial.last_result[\"eval_f1\"]))\n","  \n","  best_trained_model = FullModel(311595, 768, best_trial.config['std_dims'], best_trial.config['num_units'], best_trial.config['do_rate'])\n","  device = \"cpu\"\n","\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/classifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","\n","  test_ds = load_test_data()\n","  test_dataloader = DataLoader(test_ds, batch_size=best_trial.config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=311595, f2_len=768))\n","  return test_results(best_trained_model, test_dataloader, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"px4KGuLsFyX7","executionInfo":{"status":"ok","timestamp":1630037377597,"user_tz":420,"elapsed":139824,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"56496666-3d8f-42de-e0e2-9980cf57d372"},"source":["test_scores = main(num_samples = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-08-27 04:07:20,293\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n","2021-08-27 04:07:21,489\tWARNING experiment.py:296 -- No name detected on trainable. Using DEFAULT.\n","2021-08-27 04:07:21,491\tINFO registry.py:67 -- Detected unknown callable for trainable. Converting to class.\n","2021-08-27 04:07:26,729\tWARNING worker.py:1189 -- Warning: The actor ImplicitFunc has size 205760930 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 2.9/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m len(train_dataloader) 9\n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-07-44\n","  done: false\n","  epoch: 0.0\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6705350081125895\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 1\n","  loss: 0.6717736721038818\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 14.890439748764038\n","  time_this_iter_s: 14.890439748764038\n","  time_total_s: 14.890439748764038\n","  timestamp: 1630037264\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 47f9e_00000\n","  \n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 47f9e_00000 with eval_loss=0.6705350081125895 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.8904</td><td style=\"text-align: right;\">0.671774</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   0.670535</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-07-53\n","  done: false\n","  epoch: 0.1111111111111111\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6434538960456848\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 2\n","  loss: 0.6600186824798584\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 24.534753799438477\n","  time_this_iter_s: 9.644314050674438\n","  time_total_s: 24.534753799438477\n","  timestamp: 1630037273\n","  timesteps_since_restore: 0\n","  training_iteration: 2\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.9/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6434538960456848 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         24.5348</td><td style=\"text-align: right;\">0.660019</td><td style=\"text-align: right;\">0.111111</td><td style=\"text-align: right;\">   0.643454</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-02\n","  done: false\n","  epoch: 0.2222222222222222\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6265470385551453\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 3\n","  loss: 0.596293568611145\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 33.513720989227295\n","  time_this_iter_s: 8.978967189788818\n","  time_total_s: 33.513720989227295\n","  timestamp: 1630037282\n","  timesteps_since_restore: 0\n","  training_iteration: 3\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.9/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6265470385551453 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         33.5137</td><td style=\"text-align: right;\">0.596294</td><td style=\"text-align: right;\">0.222222</td><td style=\"text-align: right;\">   0.626547</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-12\n","  done: false\n","  epoch: 0.3333333333333333\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.609357754389445\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 4\n","  loss: 0.5180160403251648\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 42.739635705947876\n","  time_this_iter_s: 9.225914716720581\n","  time_total_s: 42.739635705947876\n","  timestamp: 1630037292\n","  timesteps_since_restore: 0\n","  training_iteration: 4\n","  trial_id: 47f9e_00000\n","  \n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 5.0/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.609357754389445 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         42.7396</td><td style=\"text-align: right;\">0.518016</td><td style=\"text-align: right;\">0.333333</td><td style=\"text-align: right;\">   0.609358</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-21\n","  done: false\n","  epoch: 0.4444444444444444\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6256746848424276\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 5\n","  loss: 0.5705971121788025\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 51.93508315086365\n","  time_this_iter_s: 9.195447444915771\n","  time_total_s: 51.93508315086365\n","  timestamp: 1630037301\n","  timesteps_since_restore: 0\n","  training_iteration: 5\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 5.0/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6256746848424276 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         51.9351</td><td style=\"text-align: right;\">0.570597</td><td style=\"text-align: right;\">0.444444</td><td style=\"text-align: right;\">   0.625675</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"stream","text":["2021-08-27 04:08:22,312\tWARNING util.py:164 -- The `process_trial_save` operation took 1.046 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-30\n","  done: false\n","  epoch: 0.5555555555555556\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6256352265675863\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 6\n","  loss: 0.5698477625846863\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 61.139683961868286\n","  time_this_iter_s: 9.204600811004639\n","  time_total_s: 61.139683961868286\n","  timestamp: 1630037310\n","  timesteps_since_restore: 0\n","  training_iteration: 6\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6256352265675863 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         61.1397</td><td style=\"text-align: right;\">0.569848</td><td style=\"text-align: right;\">0.555556</td><td style=\"text-align: right;\">   0.625635</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"stream","text":["2021-08-27 04:08:31,686\tWARNING util.py:164 -- The `process_trial_save` operation took 1.212 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-39\n","  done: false\n","  epoch: 0.6666666666666666\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6286422610282898\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 7\n","  loss: 0.5830590724945068\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 70.21444392204285\n","  time_this_iter_s: 9.07475996017456\n","  time_total_s: 70.21444392204285\n","  timestamp: 1630037319\n","  timesteps_since_restore: 0\n","  training_iteration: 7\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6286422610282898 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         70.2144</td><td style=\"text-align: right;\">0.583059</td><td style=\"text-align: right;\">0.666667</td><td style=\"text-align: right;\">   0.628642</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"stream","text":["2021-08-27 04:08:40,743\tWARNING util.py:164 -- The `process_trial_save` operation took 1.182 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-48\n","  done: false\n","  epoch: 0.7777777777777778\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6708032290140787\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 8\n","  loss: 0.7416834235191345\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 79.05911254882812\n","  time_this_iter_s: 8.844668626785278\n","  time_total_s: 79.05911254882812\n","  timestamp: 1630037328\n","  timesteps_since_restore: 0\n","  training_iteration: 8\n","  trial_id: 47f9e_00000\n","  \n","\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.9/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: -0.6708032290140787 | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4)<br>Current best trial: 47f9e_00000 with eval_loss=0.6708032290140787 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         79.0591</td><td style=\"text-align: right;\">0.741683</td><td style=\"text-align: right;\">0.777778</td><td style=\"text-align: right;\">   0.670803</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["2021-08-27 04:08:49,581\tWARNING util.py:164 -- The `process_trial_save` operation took 1.190 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-56\n","  done: false\n","  epoch: 0.8888888888888888\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6199080546696981\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  hostname: c391a45958ff\n","  iterations_since_restore: 9\n","  loss: 0.5679579973220825\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 87.24576449394226\n","  time_this_iter_s: 8.186651945114136\n","  time_total_s: 87.24576449394226\n","  timestamp: 1630037336\n","  timesteps_since_restore: 0\n","  training_iteration: 9\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: -0.6708032290140787 | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6199080546696981 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         87.2458</td><td style=\"text-align: right;\">0.567958</td><td style=\"text-align: right;\">0.888889</td><td style=\"text-align: right;\">   0.619908</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=1118)\u001b[0m s_precision: 0.4192329589565757 s_recall: 0.6474820143884892 s_f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"stream","text":["2021-08-27 04:08:57,705\tWARNING util.py:164 -- The `process_trial_save` operation took 1.130 s, which may be a performance bottleneck.\n","2021-08-27 04:09:07,733\tWARNING util.py:164 -- The `process_trial_result` operation took 10.020 s, which may be a performance bottleneck.\n","2021-08-27 04:09:07,735\tWARNING util.py:164 -- Processing trial results took 10.022 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2021-08-27 04:09:07,736\tWARNING util.py:164 -- The `process_trial` operation took 10.024 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["Result for DEFAULT_47f9e_00000:\n","  date: 2021-08-27_04-08-56\n","  done: true\n","  epoch: 0.8888888888888888\n","  eval_f1: 0.508937827903616\n","  eval_loss: 0.6199080546696981\n","  eval_precision: 0.4192329589565757\n","  eval_recall: 0.6474820143884892\n","  experiment_id: c615d98a797d49db9ff8dc72d56ca556\n","  experiment_tag: 0_batch_size=64,do_rate=0.46321,lr=0.0052587,num_epochs=1,num_units=55,std_dims=86\n","  hostname: c391a45958ff\n","  iterations_since_restore: 9\n","  loss: 0.5679579973220825\n","  node_ip: 172.28.0.2\n","  pid: 1118\n","  should_checkpoint: true\n","  time_since_restore: 87.24576449394226\n","  time_this_iter_s: 8.186651945114136\n","  time_total_s: 87.24576449394226\n","  timestamp: 1630037336\n","  timesteps_since_restore: 0\n","  training_iteration: 9\n","  trial_id: 47f9e_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: -0.6708032290140787 | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 1.0/4 CPUs, 1.0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6199080546696981 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>RUNNING </td><td>172.28.0.2:1118</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         87.2458</td><td style=\"text-align: right;\">0.567958</td><td style=\"text-align: right;\">0.888889</td><td style=\"text-align: right;\">   0.619908</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["2021-08-27 04:09:08,852\tWARNING util.py:164 -- The `process_trial_save` operation took 1.090 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 4.9/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: -0.6708032290140787 | Iter 4.000: -0.609357754389445 | Iter 2.000: -0.6434538960456848 | Iter 1.000: -0.6705350081125895<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/14.8 GiB heap, 0.0/7.4 GiB objects (0.0/1.0 CPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 GPU_group_0_993febdf1a134f69a5456714368ccdd3, 0.0/1.0 CPU_group_0_993febdf1a134f69a5456714368ccdd3)<br>Current best trial: 47f9e_00000 with eval_loss=0.6199080546696981 and parameters={'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-27_04-07-22<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">   epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_47f9e_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.463209</td><td style=\"text-align: right;\">0.00525874</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         55</td><td style=\"text-align: right;\">        86</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         87.2458</td><td style=\"text-align: right;\">0.567958</td><td style=\"text-align: right;\">0.888889</td><td style=\"text-align: right;\">   0.619908</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["2021-08-27 04:09:08,977\tINFO tune.py:550 -- Total run time: 107.49 seconds (105.69 seconds for the tuning loop).\n"],"name":"stderr"},{"output_type":"stream","text":["Best trial config: {'lr': 0.005258735065638649, 'batch_size': 64, 'num_epochs': 1, 'std_dims': 86, 'num_units': 55, 'do_rate': 0.463208753763992}\n","Best trial final validation loss: 0.6199080546696981\n","Best trial final validation weighted f1: 0.508937827903616\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ququ0pe__LHo"},"source":["Learning the genre vectors from Wg, try to understand if some genres are near each other are not using some distance metric (euclidean or manhattan). Can also do PCA."]},{"cell_type":"code","metadata":{"id":"rdLIKbL_jmpi"},"source":["from sklearn.metrics import precision_recall_fscore_support "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7poTQSxv30G"},"source":["s_f1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nD_NTt5_RWbR"},"source":["# Archived Code"]},{"cell_type":"code","metadata":{"id":"spDVHd2QRYIK"},"source":["# def getAveragePooledOutputs(model, encoded_dataset):\n","#   book_embeddings_dataset = {'meaned_pooled_output': [], 'book_title': [], 'genre': [], 'labels': []}\n","\n","#   book_changes = get_book_changes_idx(encoded_dataset['book_title'])\n","\n","#   for i in range(len(book_changes)):\n","#     print(i)\n","#     start = book_changes[i]\n","#     end = None\n","#     if i != len(book_changes) - 1:\n","#       end = book_changes[i+1]\n","#     else:\n","#       end = len(encoded_dataset['input_ids'])\n","\n","#     input_ids = th.LongTensor(encoded_dataset['input_ids'][start:end])\n","#     attention_mask = th.BoolTensor(encoded_dataset['attention_mask'][start:end])\n","\n","#     with torch.no_grad():\n","#       embeddings = transformer_model.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0][:,0] # Pooled output\n","#       book_embeddings = th.mean(embeddings, dim=0) # Takes the mean of the pooled output\n","#     book_embeddings_dataset['meaned_pooled_output'].append(book_embeddings)\n","#     book_embeddings_dataset['book_title'].append(encoded_dataset['book_title'][start])\n","#     book_embeddings_dataset['genre'].append(encoded_dataset['genre'][start])\n","#     book_embeddings_dataset['labels'].append(encoded_dataset['labels'][start])\n","  \n","#   return book_embeddings_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2bvuTrvRZL9"},"source":[""],"execution_count":null,"outputs":[]}]}