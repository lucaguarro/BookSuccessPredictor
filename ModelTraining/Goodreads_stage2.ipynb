{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Goodreads_stage2.ipynb","provenance":[],"collapsed_sections":["_6gF1bvclYVR","oHUvF9GXrF_0","p95ei-lSdr9i","YgoOFdif7bbj","5NgXCJLhiz_4","TK4ZZFsWjZAU","BQXU-G-cjEkk","lavLmaCjjGfG","Ydr19fUDjo9i","K3kxvLBJj70p","nvjGT-vJkO2x","z8195Rq9rb6I","Xh-WGSe1doKZ"],"authorship_tag":"ABX9TyMJmxv6iwO7V5f2O9FHuo2g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BjfCjM2xh9dE"},"source":["In this chapter we used already fine-tuned BERT models to extract the chunk embeddings of our books. The chunk embeddings correspond to either the meaned embeddings of all the words in the sequence or the embedding of the [CLS] token. We will explore the results of both. We will then run a variety of classifiers over these embeddings directly. \n","\n","1.   Meaned pooled output --> single layer NN\n","2.   RoBERT\n","3.   ToBERT ?"]},{"cell_type":"markdown","metadata":{"id":"_6gF1bvclYVR"},"source":["# Get Transformer Model from Stage 1"]},{"cell_type":"code","metadata":{"id":"Sot-nVzelfLN"},"source":["import wandb\n","run = wandb.init()\n","\n","model_name = 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","artifact = None\n","if (model_name == 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings'):\n","  artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-2giwtwvy:v0', type='model')\n","elif (model_name == 'DistilBERT_multitask_overlap50_dataset_embeddings'):\n","  artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-nlpbosie:v0', type='model')\n","\n","artifact_dir = artifact.download()\n","\n","transformer_model = DistilBERTForMultipleSequenceClassification.from_pretrained(artifact_dir, num_labels1 = 2, num_labels2 = 8)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHUvF9GXrF_0"},"source":["# Getting the Data"]},{"cell_type":"markdown","metadata":{"id":"p95ei-lSdr9i"},"source":["## Getting the Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"idsWCeYad34q"},"source":["### Creating the Dataset"]},{"cell_type":"markdown","metadata":{"id":"YgoOFdif7bbj"},"source":["#### From script"]},{"cell_type":"code","metadata":{"id":"IjBdxGKsFyZO"},"source":["import torch as th\n","import time\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def getPooledOutputs(model, encoded_dataset, batch_size = 32):\n","  model.eval()\n","\n","  # pooled_outputs = []\n","  pooled_outputs = torch.empty([0,768]).cuda()\n","\n","  num_iters = (len(encoded_dataset['input_ids']) - 1)//batch_size + 1\n","  print(\"total number of iters \", num_iters)\n","  \n","  for i in range(num_iters):\n","    print(i)\n","    up_to = i*batch_size + batch_size\n","    if len(encoded_dataset['input_ids']) < up_to:\n","      up_to = len(encoded_dataset['input_ids'])\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to]).cuda()\n","    attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to]).cuda()\n","\n","    with torch.no_grad():\n","      embeddings = model.forward(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)['hidden_states'][-1][:,0] # Pooled output\n","      pooled_outputs = th.cat([pooled_outputs, embeddings],0)\n","      th.cuda.empty_cache()\n","\n","  return pooled_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEVzj6kH8q14","executionInfo":{"elapsed":162,"status":"ok","timestamp":1625007321937,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"5189df82-5abc-4f64-9b52-b682a13a8581"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'book_title', 'input_ids', 'labels', 'token_type_ids'],\n","        num_rows: 15752\n","    })\n","    validation: Dataset({\n","        features: ['attention_mask', 'book_title', 'input_ids', 'labels', 'token_type_ids'],\n","        num_rows: 11023\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'book_title', 'input_ids', 'labels', 'token_type_ids'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"f-AigWJRlMyx"},"source":["train_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjtsYx1YnBL3"},"source":["val_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xYnBmEAnHwp"},"source":["test_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyWQ8O-eEc1D"},"source":["from datasets import Dataset\n","train_set_embeddings = Dataset.from_dict({'pooled_outputs': train_set_embeddings})\n","val_set_embeddings = Dataset.from_dict({'pooled_outputs': val_set_embeddings})\n","test_set_embeddings = Dataset.from_dict({'pooled_outputs': test_set_embeddings})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8agtlOpzEigf"},"source":["from datasets import concatenate_datasets\n","dataset_w_embeddings = DatasetDict({\n","    'train': concatenate_datasets([chunked_encoded_dataset['train'], train_set_embeddings], axis = 1), \n","    'validation': concatenate_datasets([chunked_encoded_dataset['validation'], val_set_embeddings], axis = 1), \n","    'test': concatenate_datasets([chunked_encoded_dataset['test'], test_set_embeddings], axis = 1)\n","})\n","dataset_w_embeddings = dataset_w_embeddings.remove_columns(['attention_mask', 'input_ids', 'token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kK_Uf0Au3RFd","executionInfo":{"elapsed":233,"status":"ok","timestamp":1625072541980,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"6b95b14b-584c-4b94-c03c-5f6c4267034c"},"source":["dataset_w_embeddings"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 15752\n","    })\n","    validation: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 11023\n","    })\n","    test: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"8VhVV2CdEo56"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('train_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['train'], output_file)\n","\n","with open('val_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['validation'], output_file)\n","\n","with open('test_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['test'], output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKxTgPtQ7gn5"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"C2e0wfqflvz_"},"source":["base_path = Path(\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MP8MbdD07vOm","executionInfo":{"elapsed":4621,"status":"ok","timestamp":1625072647437,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"e7685c06-741b-490c-bf91-83bf136133d3"},"source":["from datasets import DatasetDict\n","\n","if model_name == 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings':\n","  full_directory = base_path / 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings'\n","elif model_name == 'DistilBERT_multitask_overlap50_dataset_embeddings':\n","  full_directory = base_path / 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","with open(full_directory / 'train_ds.pkl', \"rb\") as input_file:\n","  train_set_embeddings = pickle.load(input_file)\n","\n","with open(full_directory / 'val_ds.pkl', \"rb\") as input_file:\n","  val_set_embeddings = pickle.load(input_file)\n","\n","with open(full_directory / 'test_ds.pkl', \"rb\") as input_file:\n","  test_set_embeddings = pickle.load(input_file)\n","\n","dataset_w_embeddings = DatasetDict({'train': train_set_embeddings, 'validation': val_set_embeddings, 'test': test_set_embeddings})\n","dataset_w_embeddings"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ay\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 15752\n","    })\n","    validation: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 11023\n","    })\n","    test: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"5NgXCJLhiz_4"},"source":["## Average Pooled Outputs for Shallow Neural Network and SVM"]},{"cell_type":"markdown","metadata":{"id":"TK4ZZFsWjZAU"},"source":["### Generating the Data from Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"BQXU-G-cjEkk"},"source":["#### From Script"]},{"cell_type":"code","metadata":{"id":"G_TnU36ei2eh"},"source":["def getAveragePooledOutputs(model, encoded_dataset):\n","  book_embeddings_dataset = {'meaned_pooled_output': [], 'book_title': [], 'genre': [], 'labels': []}\n","\n","  book_changes = get_book_changes_idx(encoded_dataset['book_title'])\n","\n","  for i in range(len(book_changes)):\n","    print(i)\n","    start = book_changes[i]\n","    end = None\n","    if i != len(book_changes) - 1:\n","      end = book_changes[i+1]\n","    else:\n","      end = len(encoded_dataset['input_ids'])\n","\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][start:end])\n","    attention_mask = th.BoolTensor(encoded_dataset['attention_mask'][start:end])\n","\n","    with torch.no_grad():\n","      embeddings = transformer_model.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0][:,0] # Pooled output\n","      book_embeddings = th.mean(embeddings, dim=0) # Takes the mean of the pooled output\n","    book_embeddings_dataset['meaned_pooled_output'].append(book_embeddings)\n","    book_embeddings_dataset['book_title'].append(encoded_dataset['book_title'][start])\n","    book_embeddings_dataset['genre'].append(encoded_dataset['genre'][start])\n","    book_embeddings_dataset['labels'].append(encoded_dataset['labels'][start])\n","  \n","  return book_embeddings_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"161S_UpRi5Xq"},"source":["avg_pld_outs_ds = getAveragePooledOutputs(dataset_w_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQXjx63ljRis"},"source":["from datasets import DatasetDict, Dataset\n","avg_pld_outs_hf_ds = DatasetDict({'train': Dataset.from_dict(avg_pld_outs_ds['train']), 'validation': Dataset.from_dict(avg_pld_outs_ds['validation']), 'test': Dataset.from_dict(avg_pld_outs_ds['test'])})\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('avg_pld_outs_hf_ds.pkl', 'wb') as output_file:\n","  pickle.dump(avg_pld_outs_hf_ds, output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('avg_pld_outs_hf_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lavLmaCjjGfG"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"zRmZRxEGjWJT"},"source":["from datasets import DatasetDict\n","with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","  avg_pld_outs_hf_ds = pickle.load(input_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ydr19fUDjo9i"},"source":["# Simple Shallow Neural Network"]},{"cell_type":"code","metadata":{"id":"Aw4ZV-yjjvYe"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","class Net(nn.Module):\n","\n","    def __init__(self, pre_classifier_init, classifier_init):\n","        super(Net, self).__init__()\n","\n","        self.pre_classifier = nn.Linear(768, 768)\n","        self.classifier = nn.Linear(768, 2)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.pre_classifier.weight.data.copy_(pre_classifier_init.weight.data)\n","        self.classifier.weight.data.copy_(classifier_init.weight.data)\n","\n","        # print(pre_classifier_init.bias.data)\n","        self.pre_classifier.bias.data.copy_(pre_classifier_init.bias.data)\n","        self.classifier.bias.data.copy_(classifier_init.bias.data)\n","\n","        # DOUBLE CHECK IF BIASES ARE BEING SET AS WELL\n","\n","    def forward(self, x, labels = None):\n","        # Max pooling over a (2, 2) window\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        logits = self.classifier(x)\n","\n","        loss = None\n","        if labels is not None:\n","          loss_fct = CrossEntropyLoss()\n","          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","        return SequenceClassifierOutput(\n","            loss = loss,\n","            logits = logits\n","        )\n","\n","net = Net(model.pre_classifier, model.classifier1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3kxvLBJj70p"},"source":["#### Results with no Training"]},{"cell_type":"code","metadata":{"id":"2nhYqvBgkAkH"},"source":["net.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecoQ7GOOj_ml"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['validation']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['validation']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6v9W_FydkEOR"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['test']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['test']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvjGT-vJkO2x"},"source":["#### Training w Hyperparameter Tuning and Results"]},{"cell_type":"code","metadata":{"id":"DSHANKmHkSzp"},"source":["def load_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  trainset = avg_pld_outs_hf_ds['train']\n","  valset = avg_pld_outs_hf_ds['validation']\n","  return trainset, valset\n","\n","def load_test_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  testset = avg_pld_outs_hf_ds['test']\n","  return testset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kM4cHtBkqaf"},"source":["from ray import tune\n","# from ray.tune.integration.wandb import wandb_mixin\n","# '''@wandb_mixin\n","# run = wandb.init()\n","\n","def train_nn(config, checkpoint_dir, data_dir=None):\n","  net = Net(model.pre_classifier, model.classifier1, config['do_rate'])\n","  net.train()\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      if torch.cuda.device_count() > 1:\n","          net = nn.DataParallel(net)\n","  print(type(net))\n","  net.to(device)\n","  # net.cuda()\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], shuffle=True)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","    for i, data in enumerate(trainloader, 0):\n","\n","      inputs = data['meaned_pooled_output']\n","      labels = data['success_label']\n","\n","      inputs, labels = inputs.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = net(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if i % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      net.eval()\n","      with torch.no_grad():\n","        for i, data in enumerate(valloader, 0):\n","\n","          inputs_cpu = data['meaned_pooled_output']\n","          labels_cpu = data['success_label']\n","\n","          inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","          # inputs.cuda()\n","          # labels.cuda()\n","\n","          outputs = net(inputs)\n","          _, predicted = torch.max(outputs.data, 1)\n","\n","          all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","          all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","          loss = criterion(outputs, labels)\n","          val_loss += loss.cpu().numpy()\n","          val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((net.state_dict(), optimizer.state_dict()), path)\n","\n","      net.train()\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      # s_acc = accuracy_score(all_labels, all_predictions)\n","      # wandb.log({\"val_loss\": val_loss / val_steps, \"val_accuracy\": correct / total})\n","      tune.report(loss=(val_loss / val_steps), accuracy=correct / total, f1=s_f1, precision=s_precision, recall=s_recall)\n","  print(\"Finished Training\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBaWnJvfkwCR"},"source":["def test_results(net, device=\"cpu\"):\n","    testset = load_test_data()\n","\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n","\n","    all_predictions = np.array([])\n","    all_labels = np.array([])\n","\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader, 0):\n","            inputs_cpu = data['meaned_pooled_output']\n","            labels_cpu = data['success_label']\n","\n","            inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","            all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","    s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","    return {\n","        'precision': s_precision,\n","        'recall': s_recall,\n","        'f1': s_f1\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aom_a9B_kUL7"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.tune.integration.wandb import WandbLoggerCallback\n","import torch.optim as optim\n","from functools import partial\n","\n","def main(num_samples = 15, max_num_epochs = 10):\n","  config = {\n","      \"lr\": tune.loguniform(1e-4, 1e-1),\n","      \"batch_size\": tune.choice([16,32,64,128]),\n","      \"num_epochs\": tune.choice([1,2]),#,2,3]),#,2,3,5,10,20]),\n","      \"do_rate\": tune.uniform(0.1, 0.5),\n","      \"wandb\": {\n","        \"project\": \"AvgPooledOutputClassifier\",\n","        \"api_key\": \"46cb1981ae15765be5bfb5e7c3257d0315a95a1b\"\n","      }\n","    }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(train_nn, checkpoint_dir='/tmp/ShallowNNModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"AvgPooledOutputClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=\"46cb1981ae15765be5bfb5e7c3257d0315a95a1b\",\n","        log_config=True\n","    )])\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"accuracy\"]))\n","  \n","  best_trained_model = Net(model.pre_classifier, model.classifier1, best_trial.config['do_rate'])\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","\n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/ShallowNNModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANZwhwsqk4C9"},"source":["main(num_samples=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8195Rq9rb6I"},"source":["# SVM"]},{"cell_type":"code","metadata":{"id":"Psd_tsr7rwkw"},"source":["from sklearn import svm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RWgJonqrzzi"},"source":["cs = np.arange(0.9, 1.5, 0.02).tolist()\n","best_clf = None\n","best_score = 0\n","best_c = None\n","for c in cs:\n","  clf = svm.SVC(kernel='rbf', gamma='scale', C=c)\n","  clf.fit(avg_pld_outs_hf_ds['train']['meaned_pooled_output'], avg_pld_outs_hf_ds['train']['success_label'])\n","  predictions = clf.predict(avg_pld_outs_hf_ds['validation']['meaned_pooled_output'])\n","  (_, pred_counts) = np.unique(predictions, return_counts=True)\n","  val_score = f1_score(avg_pld_outs_hf_ds['validation']['success_label'], predictions, average = 'weighted')\n","  print('Clf with C = {} obtained val-score of {}'.format(c, val_score))\n","  if (val_score > best_score):\n","    best_score = val_score\n","    best_clf = clf\n","    best_c = c\n","\n","print('\\nBest C: {}; Val-score: {}'.format(best_c, best_score))\n","test_predictions = best_clf.predict(avg_pld_outs_hf_ds['test']['meaned_pooled_output'])\n","test_score = f1_score(avg_pld_outs_hf_ds['test']['success_label'], test_predictions, average = 'weighted')\n","print('Yields score of {} on test set'.format(test_score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xh-WGSe1doKZ"},"source":["# RoBERT"]},{"cell_type":"code","metadata":{"id":"XaGB0mqPdTCr"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNINcDJopd1-"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  # book_changes_idx = np.append(book_changes_idx, len(book_titles))\n","  # book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def convert_to_LSTM_dataset_full(dataset):\n","  full_ds = {}\n","  full_ds['train'] = convert_to_LSTM_dataset_sub(dataset['train'])\n","  full_ds['validation'] = convert_to_LSTM_dataset_sub(dataset['validation'])\n","  full_ds['test'] = convert_to_LSTM_dataset_sub(dataset['test'])\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  # full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  return full_ds\n","\n","def convert_to_LSTM_dataset_sub(dataset):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  book_start_idx = get_book_changes_idx(dataset['book_title'])\n","  book_start_idx_w_end = np.append(book_start_idx, len(dataset['book_title']))\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = dataset['pooled_outputs'].split_with_sizes(list(book_lengths))\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  # print(type(book_grouped_embeddings))\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = np.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = np.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_slJ4X3Bpf_Q"},"source":["class RoBERT_Model(nn.Module):\n","\n","    def __init__(self, layer_size = 100):\n","        self.layer_size = layer_size\n","        super(RoBERT_Model, self).__init__()\n","        self.lstm = nn.LSTM(768, layer_size, num_layers=1, bidirectional=False)\n","        self.out = nn.Linear(layer_size, 2)\n","\n","    def forward(self, grouped_pooled_outs):\n","        \"\"\" Define how to performed each call\n","        Parameters\n","        __________\n","        pooled_output: array\n","            -\n","        lengt: int\n","            -\n","        Returns:\n","        _______\n","        -\n","        \"\"\"\n","        # chunks_emb = pooled_out.split_with_sizes(lengt) # splits the input tensor into a list of tensors where the length of each sublist is determined by lengt\n","\n","        seq_lengths = torch.LongTensor([x for x in map(len, grouped_pooled_outs)]) # gets the length of each sublist in chunks_emb and returns it as an array\n","\n","        batch_emb_pad = nn.utils.rnn.pad_sequence(grouped_pooled_outs, padding_value=-91, batch_first=True) # pads each sublist in chunks_emb to the largest sublist with value -91\n","        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n","        lstm_input = nn.utils.rnn.pack_padded_sequence(batch_emb, seq_lengths, batch_first=False, enforce_sorted=False) # seq_lengths.cpu().numpy()\n","\n","        packed_output, (h_t, h_c) = self.lstm(lstm_input, )  # (h_t, h_c))\n","#         output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=-91)\n","\n","        h_t = h_t.view(-1, self.layer_size) # (-1, 100)\n","\n","        return self.out(h_t) # logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEPdCiU6phS2"},"source":["def my_collate1(batches):\n","  # for batch in batches:\n","  #   print(type(batch['grouped_pooled_outs']), len(batch['grouped_pooled_outs']))\n","  #   print(type(torch.FloatTensor(batch['grouped_pooled_outs'])))\n","    return {\n","        'grouped_pooled_outs': [torch.stack(x['grouped_pooled_outs']) for x in batches],\n","        'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTNSTUQkpnQp"},"source":["from transformers import AdamW\n","import time\n","\n","def load_test_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  testset = full_ds['test']\n","  return testset\n","\n","def load_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  trainset = full_ds['train']\n","  valset = full_ds['validation']\n","  return trainset, valset\n","\n","def loss_fun(outputs, targets):\n","    loss = nn.CrossEntropyLoss()\n","    return loss(outputs, targets)\n","\n","def rnn_train_fun1(config, checkpoint_dir='/tmp/LSTMModels'):\n","  model = RoBERT_Model(config[\"layer_size\"])\n","  model.train()\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #   device = \"cuda:0\"\n","  #   if torch.cuda.device_count() > 1:\n","  #       model = nn.DataParallel(model)\n","  # # print(type(model))\n","  # model.to(device)\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(trainloader):\n","      grouped_pooled_outs = batch['grouped_pooled_outs'] # .to(device)\n","      targets = batch['success_label'] #.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(grouped_pooled_outs)\n","      loss = loss_fun(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if batch_idx % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      with torch.no_grad():\n","          for i, data in enumerate(valloader, 0):\n","\n","              grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","              targets = data['success_label'] # .to(device)\n","\n","              outputs = model(grouped_pooled_outs)\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.numpy())\n","              all_labels = np.append(all_labels, targets.numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6beZxWFXpsXW"},"source":["def test_results(net, device=\"cpu\"):\n","  testset = load_test_data()\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=8, collate_fn=my_collate1)\n","\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.eval()\n","  with torch.no_grad():\n","    for i, data in enumerate(testloader, 0):\n","        grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","        targets = data['success_label'] # .to(device)\n","\n","        outputs = net(grouped_pooled_outs)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxp-MTSnpvZN"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","  config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([16,32,64]),\n","    \"num_epochs\": tune.choice([1,2,3,5]),\n","    \"layer_size\": tune.choice([100]),\n","    \"wandb\": {\n","      \"project\": \"LSTMClassifier\",\n","      \"api_key\": \"46cb1981ae15765be5bfb5e7c3257d0315a95a1b\",\n","      \"log_config\": True\n","    }\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(rnn_train_fun1, checkpoint_dir='/tmp/LSTMModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"LSTMClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=\"46cb1981ae15765be5bfb5e7c3257d0315a95a1b\",\n","        log_config=True\n","    )])\n","\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = RoBERT_Model(best_trial.config['layer_size'])\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #     device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/LSTMModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETTb9Z0vpy00"},"source":["test_results = main()"],"execution_count":null,"outputs":[]}]}