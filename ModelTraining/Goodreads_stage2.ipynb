{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Goodreads_stage2.ipynb","provenance":[],"collapsed_sections":["_6gF1bvclYVR","oHUvF9GXrF_0","Ydr19fUDjo9i","z8195Rq9rb6I","Xh-WGSe1doKZ"],"mount_file_id":"1wBVNh_5-j0lep-7H-6-hnXYKB4cztf3_","authorship_tag":"ABX9TyNBM94rx4/Urjhsfj/KP8fh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BjfCjM2xh9dE"},"source":["In this chapter we used already fine-tuned BERT models to extract the chunk embeddings of our books. The chunk embeddings correspond to either the meaned embeddings of all the words in the sequence or the embedding of the [CLS] token. We will explore the results of both. We will then run a variety of classifiers over these embeddings directly. \n","\n","1.   Meaned pooled output --> single layer NN\n","2.   RoBERT\n","3.   ToBERT ?"]},{"cell_type":"markdown","metadata":{"id":"XLXM0vhe5MQ8"},"source":["# Installs, Imports, Configuration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttrgrV6NN3nB","executionInfo":{"status":"ok","timestamp":1627497370647,"user_tz":420,"elapsed":6633,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"77f217b1-21fa-4328-fefa-fc0968c6cea9"},"source":["!pip install datasets"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n","\u001b[K     |████████████████████████████████| 542 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 40.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 54.3 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting tqdm>=4.42\n","  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: tqdm, xxhash, huggingface-hub, fsspec, datasets\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","Successfully installed datasets-1.10.2 fsspec-2021.7.0 huggingface-hub-0.0.15 tqdm-4.61.2 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7vwrYLDGtF-","executionInfo":{"status":"ok","timestamp":1627497374379,"user_tz":420,"elapsed":161,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"228372b0-5e27-435d-cf1e-f1017e946432"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QiCafSeXFWzf","executionInfo":{"status":"ok","timestamp":1627497374861,"user_tz":420,"elapsed":3,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from pathlib import Path\n","import sys"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA5cgP2i5PB9","executionInfo":{"status":"ok","timestamp":1627497389070,"user_tz":420,"elapsed":148,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["import configparser\n","\n","config = configparser.ConfigParser()\n","config.read('/content/drive/MyDrive/Thesis/BookSuccessPredictor/config.ini')\n","\n","drive_base_path = Path(config['Drive']['drive_base_path'])\n","\n","# sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / '_utils'))\n","sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / 'datasets' / 'goodreads_maharjan_super' / 'MultiModal' / 'dataset_loader'))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4INA9MmFyFk","executionInfo":{"status":"ok","timestamp":1627497400693,"user_tz":420,"elapsed":10914,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from MultimodalGoodreadsDataset import MultimodalGoodreadsDataset"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLtfG87-M7pS","executionInfo":{"status":"ok","timestamp":1627497400694,"user_tz":420,"elapsed":18,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6Zdrj_6FbPj","executionInfo":{"status":"ok","timestamp":1627497420220,"user_tz":420,"elapsed":18267,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"dbe459bf-5734-44e0-947c-3113f6f3e11c"},"source":["a = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Total test instances: 290, validation instances: 290, and Training instances: 404\n","Total unique books: 984\n","Training instances (404,), Val instances (290,), Test instances (290,)\n","extracting feature: char_5_gram\n","Using cached features\n","extracting feature: bert_features\n","Using cached features\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BV8x5SVQXOkl","executionInfo":{"status":"ok","timestamp":1627497425716,"user_tz":420,"elapsed":162,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["sample = a.train[0]['text_features']"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Q8YHzr2XW5t","executionInfo":{"status":"ok","timestamp":1627497427572,"user_tz":420,"elapsed":188,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"40c8bfc8-f71b-4108-d070-fd0da89937f5"},"source":["sample.shape # [0:312363 - 768] correspond to c5g features; [312363-768:312363] correspond to bert features;"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 312363)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"_6gF1bvclYVR"},"source":["# Get Transformer Model from Stage 1"]},{"cell_type":"code","metadata":{"id":"Sot-nVzelfLN"},"source":["import wandb\n","run = wandb.init()\n","\n","model_name = 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","if config['Model']['name'] == 'distilbert-base-uncased':\n","  if (config['Tokenizer']['overlap']):\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-nlpbosie:v0', type='model')\n","  else:\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-2giwtwvy:v0', type='model')\n","    \n","artifact_dir = artifact.download()\n","\n","transformer_model = DistilBERTForMultipleSequenceClassification.from_pretrained(artifact_dir, num_labels1 = 2, num_labels2 = 8)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHUvF9GXrF_0"},"source":["# Getting the Data"]},{"cell_type":"markdown","metadata":{"id":"p95ei-lSdr9i"},"source":["## Getting the Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"idsWCeYad34q"},"source":["### Creating the Dataset"]},{"cell_type":"markdown","metadata":{"id":"YgoOFdif7bbj"},"source":["#### From script"]},{"cell_type":"code","metadata":{"id":"IjBdxGKsFyZO"},"source":["import torch as th\n","import time\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def getPooledOutputs(model, encoded_dataset, batch_size = 32):\n","  model.eval()\n","\n","  # pooled_outputs = []\n","  pooled_outputs = torch.empty([0,768]).cuda()\n","\n","  num_iters = (len(encoded_dataset['input_ids']) - 1)//batch_size + 1\n","  print(\"total number of iters \", num_iters)\n","  \n","  for i in range(num_iters):\n","    print(i)\n","    up_to = i*batch_size + batch_size\n","    if len(encoded_dataset['input_ids']) < up_to:\n","      up_to = len(encoded_dataset['input_ids'])\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to]).cuda()\n","    attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to]).cuda()\n","\n","    with torch.no_grad():\n","      embeddings = model.forward(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)['hidden_states'][-1][:,0] # Pooled output\n","      pooled_outputs = th.cat([pooled_outputs, embeddings],0)\n","      th.cuda.empty_cache()\n","\n","  return pooled_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEVzj6kH8q14","executionInfo":{"elapsed":162,"status":"ok","timestamp":1625007321937,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"5189df82-5abc-4f64-9b52-b682a13a8581"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'book_title', 'input_ids', 'labels', 'token_type_ids'],\n","        num_rows: 15752\n","    })\n","    validation: Dataset({\n","        features: ['attention_mask', 'book_title', 'input_ids', 'labels', 'token_type_ids'],\n","        num_rows: 11023\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'book_title', 'input_ids', 'labels', 'token_type_ids'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"f-AigWJRlMyx"},"source":["train_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjtsYx1YnBL3"},"source":["val_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xYnBmEAnHwp"},"source":["test_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyWQ8O-eEc1D"},"source":["from datasets import Dataset\n","train_set_embeddings = Dataset.from_dict({'pooled_outputs': train_set_embeddings})\n","val_set_embeddings = Dataset.from_dict({'pooled_outputs': val_set_embeddings})\n","test_set_embeddings = Dataset.from_dict({'pooled_outputs': test_set_embeddings})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8agtlOpzEigf"},"source":["from datasets import concatenate_datasets\n","dataset_w_embeddings = DatasetDict({\n","    'train': concatenate_datasets([chunked_encoded_dataset['train'], train_set_embeddings], axis = 1), \n","    'validation': concatenate_datasets([chunked_encoded_dataset['validation'], val_set_embeddings], axis = 1), \n","    'test': concatenate_datasets([chunked_encoded_dataset['test'], test_set_embeddings], axis = 1)\n","})\n","dataset_w_embeddings = dataset_w_embeddings.remove_columns(['attention_mask', 'input_ids', 'token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kK_Uf0Au3RFd","executionInfo":{"elapsed":233,"status":"ok","timestamp":1625072541980,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"6b95b14b-584c-4b94-c03c-5f6c4267034c"},"source":["dataset_w_embeddings"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 15752\n","    })\n","    validation: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 11023\n","    })\n","    test: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"8VhVV2CdEo56"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('train_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['train'], output_file)\n","\n","with open('val_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['validation'], output_file)\n","\n","with open('test_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['test'], output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKxTgPtQ7gn5"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"C2e0wfqflvz_"},"source":["base_path = Path(\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MP8MbdD07vOm","executionInfo":{"elapsed":4621,"status":"ok","timestamp":1625072647437,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"},"user_tz":420},"outputId":"e7685c06-741b-490c-bf91-83bf136133d3"},"source":["from datasets import DatasetDict\n","\n","if model_name == 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings':\n","  full_directory = base_path / 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings'\n","elif model_name == 'DistilBERT_multitask_overlap50_dataset_embeddings':\n","  full_directory = base_path / 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","with open(full_directory / 'train_ds.pkl', \"rb\") as input_file:\n","  train_set_embeddings = pickle.load(input_file)\n","\n","with open(full_directory / 'val_ds.pkl', \"rb\") as input_file:\n","  val_set_embeddings = pickle.load(input_file)\n","\n","with open(full_directory / 'test_ds.pkl', \"rb\") as input_file:\n","  test_set_embeddings = pickle.load(input_file)\n","\n","dataset_w_embeddings = DatasetDict({'train': train_set_embeddings, 'validation': val_set_embeddings, 'test': test_set_embeddings})\n","dataset_w_embeddings"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ay\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 15752\n","    })\n","    validation: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 11023\n","    })\n","    test: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 10816\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"5NgXCJLhiz_4"},"source":["## Average Pooled Outputs for Shallow Neural Network and SVM"]},{"cell_type":"markdown","metadata":{"id":"TK4ZZFsWjZAU"},"source":["### Generating the Data from Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"BQXU-G-cjEkk"},"source":["#### From Script"]},{"cell_type":"code","metadata":{"id":"G_TnU36ei2eh"},"source":["def getAveragePooledOutputs(model, encoded_dataset):\n","  book_embeddings_dataset = {'meaned_pooled_output': [], 'book_title': [], 'genre': [], 'labels': []}\n","\n","  book_changes = get_book_changes_idx(encoded_dataset['book_title'])\n","\n","  for i in range(len(book_changes)):\n","    print(i)\n","    start = book_changes[i]\n","    end = None\n","    if i != len(book_changes) - 1:\n","      end = book_changes[i+1]\n","    else:\n","      end = len(encoded_dataset['input_ids'])\n","\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][start:end])\n","    attention_mask = th.BoolTensor(encoded_dataset['attention_mask'][start:end])\n","\n","    with torch.no_grad():\n","      embeddings = transformer_model.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0][:,0] # Pooled output\n","      book_embeddings = th.mean(embeddings, dim=0) # Takes the mean of the pooled output\n","    book_embeddings_dataset['meaned_pooled_output'].append(book_embeddings)\n","    book_embeddings_dataset['book_title'].append(encoded_dataset['book_title'][start])\n","    book_embeddings_dataset['genre'].append(encoded_dataset['genre'][start])\n","    book_embeddings_dataset['labels'].append(encoded_dataset['labels'][start])\n","  \n","  return book_embeddings_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"161S_UpRi5Xq"},"source":["avg_pld_outs_ds = getAveragePooledOutputs(dataset_w_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQXjx63ljRis"},"source":["from datasets import DatasetDict, Dataset\n","avg_pld_outs_hf_ds = DatasetDict({'train': Dataset.from_dict(avg_pld_outs_ds['train']), 'validation': Dataset.from_dict(avg_pld_outs_ds['validation']), 'test': Dataset.from_dict(avg_pld_outs_ds['test'])})\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('avg_pld_outs_hf_ds.pkl', 'wb') as output_file:\n","  pickle.dump(avg_pld_outs_hf_ds, output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('avg_pld_outs_hf_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lavLmaCjjGfG"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"MHOd6dPpk-X-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626913121552,"user_tz":420,"elapsed":6968,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"1049e087-ce79-4882-f48a-7c0fac2df2a7"},"source":["!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-1.10.0-py3-none-any.whl (542 kB)\n","\u001b[K     |████████████████████████████████| 542 kB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Collecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.14-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 52.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 55.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n","Successfully installed datasets-1.10.0 fsspec-2021.7.0 huggingface-hub-0.0.14 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zRmZRxEGjWJT","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"error","timestamp":1626913126761,"user_tz":420,"elapsed":728,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"8ef7a5f6-f30f-4798-d4eb-dd2626ee69bb"},"source":["from datasets import DatasetDict\n","with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","  avg_pld_outs_hf_ds = pickle.load(input_file)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2daaad476c9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_dataset_embeddings/avg_pld_outs_hf_ds.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mavg_pld_outs_hf_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_classification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/tasks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautomatic_speech_recognition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutomaticSpeechRecognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaskTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerateMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_bucket_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmock_download_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMockDownloadManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/download_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from .file_utils import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcached_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mposixpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm.contrib.concurrent'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ydr19fUDjo9i"},"source":["# Simple Shallow Neural Network"]},{"cell_type":"code","metadata":{"id":"Aw4ZV-yjjvYe"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","class Net(nn.Module):\n","\n","    def __init__(self, pre_classifier_init, classifier_init):\n","        super(Net, self).__init__()\n","\n","        self.pre_classifier = nn.Linear(768, 768)\n","        self.classifier = nn.Linear(768, 2)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.pre_classifier.weight.data.copy_(pre_classifier_init.weight.data)\n","        self.classifier.weight.data.copy_(classifier_init.weight.data)\n","\n","        # print(pre_classifier_init.bias.data)\n","        self.pre_classifier.bias.data.copy_(pre_classifier_init.bias.data)\n","        self.classifier.bias.data.copy_(classifier_init.bias.data)\n","\n","        # DOUBLE CHECK IF BIASES ARE BEING SET AS WELL\n","\n","    def forward(self, x, labels = None):\n","        # Max pooling over a (2, 2) window\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        logits = self.classifier(x)\n","\n","        loss = None\n","        if labels is not None:\n","          loss_fct = CrossEntropyLoss()\n","          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","        return SequenceClassifierOutput(\n","            loss = loss,\n","            logits = logits\n","        )\n","\n","net = Net(model.pre_classifier, model.classifier1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3kxvLBJj70p"},"source":["#### Results with no Training"]},{"cell_type":"code","metadata":{"id":"2nhYqvBgkAkH"},"source":["net.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecoQ7GOOj_ml"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['validation']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['validation']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6v9W_FydkEOR"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['test']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['test']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvjGT-vJkO2x"},"source":["#### Training w Hyperparameter Tuning and Results"]},{"cell_type":"code","metadata":{"id":"DSHANKmHkSzp"},"source":["def load_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  trainset = avg_pld_outs_hf_ds['train']\n","  valset = avg_pld_outs_hf_ds['validation']\n","  return trainset, valset\n","\n","def load_test_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  testset = avg_pld_outs_hf_ds['test']\n","  return testset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kM4cHtBkqaf"},"source":["from ray import tune\n","# from ray.tune.integration.wandb import wandb_mixin\n","# '''@wandb_mixin\n","# run = wandb.init()\n","\n","def train_nn(config, checkpoint_dir, data_dir=None):\n","  net = Net(model.pre_classifier, model.classifier1, config['do_rate'])\n","  net.train()\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      if torch.cuda.device_count() > 1:\n","          net = nn.DataParallel(net)\n","  print(type(net))\n","  net.to(device)\n","  # net.cuda()\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], shuffle=True)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","    for i, data in enumerate(trainloader, 0):\n","\n","      inputs = data['meaned_pooled_output']\n","      labels = data['success_label']\n","\n","      inputs, labels = inputs.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = net(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if i % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      net.eval()\n","      with torch.no_grad():\n","        for i, data in enumerate(valloader, 0):\n","\n","          inputs_cpu = data['meaned_pooled_output']\n","          labels_cpu = data['success_label']\n","\n","          inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","          # inputs.cuda()\n","          # labels.cuda()\n","\n","          outputs = net(inputs)\n","          _, predicted = torch.max(outputs.data, 1)\n","\n","          all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","          all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","          loss = criterion(outputs, labels)\n","          val_loss += loss.cpu().numpy()\n","          val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((net.state_dict(), optimizer.state_dict()), path)\n","\n","      net.train()\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      # s_acc = accuracy_score(all_labels, all_predictions)\n","      # wandb.log({\"val_loss\": val_loss / val_steps, \"val_accuracy\": correct / total})\n","      tune.report(loss=(val_loss / val_steps), accuracy=correct / total, f1=s_f1, precision=s_precision, recall=s_recall)\n","  print(\"Finished Training\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBaWnJvfkwCR"},"source":["def test_results(net, device=\"cpu\"):\n","    testset = load_test_data()\n","\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n","\n","    all_predictions = np.array([])\n","    all_labels = np.array([])\n","\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader, 0):\n","            inputs_cpu = data['meaned_pooled_output']\n","            labels_cpu = data['success_label']\n","\n","            inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","            all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","    s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","    return {\n","        'precision': s_precision,\n","        'recall': s_recall,\n","        'f1': s_f1\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aom_a9B_kUL7"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.tune.integration.wandb import WandbLoggerCallback\n","import torch.optim as optim\n","from functools import partial\n","\n","def main(num_samples = 15, max_num_epochs = 10):\n","  config = {\n","      \"lr\": tune.loguniform(1e-4, 1e-1),\n","      \"batch_size\": tune.choice([16,32,64,128]),\n","      \"num_epochs\": tune.choice([1,2]),#,2,3]),#,2,3,5,10,20]),\n","      \"do_rate\": tune.uniform(0.1, 0.5),\n","      \"wandb\": {\n","        \"project\": \"AvgPooledOutputClassifier\",\n","        \"api_key\": config['WandB']['api_key']\n","      }\n","    }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(train_nn, checkpoint_dir='/tmp/ShallowNNModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"AvgPooledOutputClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"accuracy\"]))\n","  \n","  best_trained_model = Net(model.pre_classifier, model.classifier1, best_trial.config['do_rate'])\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","\n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/ShallowNNModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANZwhwsqk4C9"},"source":["main(num_samples=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8195Rq9rb6I"},"source":["# SVM"]},{"cell_type":"code","metadata":{"id":"Psd_tsr7rwkw"},"source":["from sklearn import svm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RWgJonqrzzi"},"source":["cs = np.arange(0.9, 1.5, 0.02).tolist()\n","best_clf = None\n","best_score = 0\n","best_c = None\n","for c in cs:\n","  clf = svm.SVC(kernel='rbf', gamma='scale', C=c)\n","  clf.fit(avg_pld_outs_hf_ds['train']['meaned_pooled_output'], avg_pld_outs_hf_ds['train']['success_label'])\n","  predictions = clf.predict(avg_pld_outs_hf_ds['validation']['meaned_pooled_output'])\n","  (_, pred_counts) = np.unique(predictions, return_counts=True)\n","  val_score = f1_score(avg_pld_outs_hf_ds['validation']['success_label'], predictions, average = 'weighted')\n","  print('Clf with C = {} obtained val-score of {}'.format(c, val_score))\n","  if (val_score > best_score):\n","    best_score = val_score\n","    best_clf = clf\n","    best_c = c\n","\n","print('\\nBest C: {}; Val-score: {}'.format(best_c, best_score))\n","test_predictions = best_clf.predict(avg_pld_outs_hf_ds['test']['meaned_pooled_output'])\n","test_score = f1_score(avg_pld_outs_hf_ds['test']['success_label'], test_predictions, average = 'weighted')\n","print('Yields score of {} on test set'.format(test_score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xh-WGSe1doKZ"},"source":["# RoBERT"]},{"cell_type":"code","metadata":{"id":"XaGB0mqPdTCr"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNINcDJopd1-"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  # book_changes_idx = np.append(book_changes_idx, len(book_titles))\n","  # book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def convert_to_LSTM_dataset_full(dataset):\n","  full_ds = {}\n","  full_ds['train'] = convert_to_LSTM_dataset_sub(dataset['train'])\n","  full_ds['validation'] = convert_to_LSTM_dataset_sub(dataset['validation'])\n","  full_ds['test'] = convert_to_LSTM_dataset_sub(dataset['test'])\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  # full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  return full_ds\n","\n","def convert_to_LSTM_dataset_sub(dataset):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  book_start_idx = get_book_changes_idx(dataset['book_title'])\n","  book_start_idx_w_end = np.append(book_start_idx, len(dataset['book_title']))\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = dataset['pooled_outputs'].split_with_sizes(list(book_lengths))\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  # print(type(book_grouped_embeddings))\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = np.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = np.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_slJ4X3Bpf_Q"},"source":["class RoBERT_Model(nn.Module):\n","\n","    def __init__(self, layer_size = 100):\n","        self.layer_size = layer_size\n","        super(RoBERT_Model, self).__init__()\n","        self.lstm = nn.LSTM(768, layer_size, num_layers=1, bidirectional=False)\n","        self.out = nn.Linear(layer_size, 2)\n","\n","    def forward(self, grouped_pooled_outs):\n","        \"\"\" Define how to performed each call\n","        Parameters\n","        __________\n","        pooled_output: array\n","            -\n","        lengt: int\n","            -\n","        Returns:\n","        _______\n","        -\n","        \"\"\"\n","        # chunks_emb = pooled_out.split_with_sizes(lengt) # splits the input tensor into a list of tensors where the length of each sublist is determined by lengt\n","\n","        seq_lengths = torch.LongTensor([x for x in map(len, grouped_pooled_outs)]) # gets the length of each sublist in chunks_emb and returns it as an array\n","\n","        batch_emb_pad = nn.utils.rnn.pad_sequence(grouped_pooled_outs, padding_value=-91, batch_first=True) # pads each sublist in chunks_emb to the largest sublist with value -91\n","        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n","        lstm_input = nn.utils.rnn.pack_padded_sequence(batch_emb, seq_lengths, batch_first=False, enforce_sorted=False) # seq_lengths.cpu().numpy()\n","\n","        packed_output, (h_t, h_c) = self.lstm(lstm_input, )  # (h_t, h_c))\n","#         output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=-91)\n","\n","        h_t = h_t.view(-1, self.layer_size) # (-1, 100)\n","\n","        return self.out(h_t) # logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEPdCiU6phS2"},"source":["def my_collate1(batches):\n","  # for batch in batches:\n","  #   print(type(batch['grouped_pooled_outs']), len(batch['grouped_pooled_outs']))\n","  #   print(type(torch.FloatTensor(batch['grouped_pooled_outs'])))\n","    return {\n","        'grouped_pooled_outs': [torch.stack(x['grouped_pooled_outs']) for x in batches],\n","        'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTNSTUQkpnQp"},"source":["from transformers import AdamW\n","import time\n","\n","def load_test_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  testset = full_ds['test']\n","  return testset\n","\n","def load_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  trainset = full_ds['train']\n","  valset = full_ds['validation']\n","  return trainset, valset\n","\n","def loss_fun(outputs, targets):\n","    loss = nn.CrossEntropyLoss()\n","    return loss(outputs, targets)\n","\n","def rnn_train_fun1(config, checkpoint_dir='/tmp/LSTMModels'):\n","  model = RoBERT_Model(config[\"layer_size\"])\n","  model.train()\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #   device = \"cuda:0\"\n","  #   if torch.cuda.device_count() > 1:\n","  #       model = nn.DataParallel(model)\n","  # # print(type(model))\n","  # model.to(device)\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(trainloader):\n","      grouped_pooled_outs = batch['grouped_pooled_outs'] # .to(device)\n","      targets = batch['success_label'] #.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(grouped_pooled_outs)\n","      loss = loss_fun(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if batch_idx % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      with torch.no_grad():\n","          for i, data in enumerate(valloader, 0):\n","\n","              grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","              targets = data['success_label'] # .to(device)\n","\n","              outputs = model(grouped_pooled_outs)\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.numpy())\n","              all_labels = np.append(all_labels, targets.numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6beZxWFXpsXW"},"source":["def test_results(net, device=\"cpu\"):\n","  testset = load_test_data()\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=8, collate_fn=my_collate1)\n","\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.eval()\n","  with torch.no_grad():\n","    for i, data in enumerate(testloader, 0):\n","        grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","        targets = data['success_label'] # .to(device)\n","\n","        outputs = net(grouped_pooled_outs)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxp-MTSnpvZN"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","  config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([16,32,64]),\n","    \"num_epochs\": tune.choice([1,2,3,5]),\n","    \"layer_size\": tune.choice([100]),\n","    \"wandb\": {\n","      \"project\": \"LSTMClassifier\",\n","      \"api_key\": config['WandB']['api_key'],\n","      \"log_config\": True\n","    }\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(rnn_train_fun1, checkpoint_dir='/tmp/LSTMModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"LSTMClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = RoBERT_Model(best_trial.config['layer_size'])\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #     device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/LSTMModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETTb9Z0vpy00"},"source":["test_results = main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kw2weKz-lg6"},"source":["# MultiModal"]},{"cell_type":"markdown","metadata":{"id":"2gVjETdBcQi2"},"source":["Our model will be composed of three separate modules:\n","\n","1. (Normalizer) Responsible for taking all the inputs of various dimensions and feeding them each through their own linear layer to project them into a space with all the same dimensions\n","\n","In essence, it is responsible for eq (1) in the paper $h_i=selu(W_{h_i} x_i + b_h)$\n","\n","\n","2. (GenreAwareAttention) This is where most of the meat of the model is. It is responsible for performing these 3 equations. \n","\n","$score(h_i, g) = v^T selu(W_a h_i + W_g g + b_a)$\n","\n","$\\alpha_i = \\frac{exp(score(h_i,g))}{\\sum_{i'}exp(score(h_{i'},g)}$\n","\n","$r=\\sum_i \\alpha_i h_i$\n","\n","3. (ClassOutput) The last layer is simply responsible for projecting the book representation to class probabilities.\n","\n","$\\hat{p}=\\sigma(W_c r + b_c)$"]},{"cell_type":"code","metadata":{"id":"1wOJErUkSZmz","executionInfo":{"status":"ok","timestamp":1627428468547,"user_tz":420,"elapsed":132,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["x_a = torch.ones(5, 100)\n","x_b = torch.zeros(5, 100)"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWqFM6CaShgA","executionInfo":{"status":"ok","timestamp":1627428504239,"user_tz":420,"elapsed":121,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"076960ba-1a2c-45a1-faf7-bfe90b570fb5"},"source":["torch.stack([x_a, x_b],1).size()"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 2, 100])"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"U0vC_Exg-nTp","executionInfo":{"status":"ok","timestamp":1627425685108,"user_tz":420,"elapsed":117,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["class Normalizer(nn.Module):\n","  def __init__(self, c5g_size, bf_size, std_dims):\n","    super(Normalizer, self).__init__()\n","\n","    self.c5g_linear = nn.Linear(c5g_size, std_dims)\n","    self.bf_linear = nn.Linear(bf_size, std_dims)\n","\n","  def forward(self, x_c5g, x_bf):\n","    # x_c5g ~ (BATCH_SIZE, C5G_FEATURE_SIZE)\n","    # x_bf ~ (BATCH_SIZE, BF_FEATURE_SIZE)\n","    # # split features into char_5_gram and bert_features\n","    # char_5_grams = None\n","    # bert_features = None\n","\n","    c5g_normed = self.c5g_linear(char_5_grams)\n","    bf_normed = self.bf_linear(bert_features)\n","\n","    # concatenate c5g_normed and bf_normed\n","    return torch.stack([c5g_normed, bf_normed], 1) # (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzj5ackHOUQu"},"source":["class GenreAwareAttention(nn.Module):\n","  def __init__(self, std_dims, num_units):\n","    super(GenreAwareAttention, self).__init__()\n","    self.activation = nn.SELU()\n","    self.nn_softmax = nn.Softmax(dim=1)\n","\n","    self.v = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,1)),\n","        requires_grad=True\n","    )\n","\n","    self.Wa = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(std_dims,num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.b = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,1)),\n","        requires_grad=True\n","    )\n","\n","    self.Wg = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(8, num_units)), \n","        requires_grad=True\n","    )\n","\n","  def forward(self, x, g):\n","    # x ~ (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)\n","    # g ~ (BATCH_SIZE, 1, GENRE_EMBED_SIZE)\n","    \n","    # calculate scores\n","    atten_g = torch.mm(g, self.Wg)\n","    et = self.activation(torch.matmul(x, self.Wa) + atten_g + self.b)\n","    et = torch.matmul(et, v)\n","\n","    at = self.nn_softmax(et)\n","\n","    at = torch.unsqueeze(at, axis=-1)\n","    ot = at * x\n","\n","    return torch.sum(ot, axis=1) # BATCH_SIZE, EMBED_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxcCXEr4iPWB","executionInfo":{"status":"ok","timestamp":1627425689410,"user_tz":420,"elapsed":118,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["class ClassifierOut(nn.Module):\n","  def __init__(self):\n","    super(ClassifierOut, self).__init__()\n","    self.classifier = nn.Linear(100, 2)\n","  \n","  def forward(self, r): # r ~ BATCH_SIZE, EMBED_SIZE\n","    r_out = self.classifier(r) # BATCH_SIZE, 2\n","    return torch.sigmoid(r_out)"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sb7CIVGIGD-G"},"source":["class FullModel(nn.Module)\n","  def __init__(self):\n","    super(Model,self).__init__()\n","    self.normalizer = Normalizer(None, None, 100)\n","    self.genre_aware_attention = GenreAwareAttention()\n","    self.classifier_out = ClassifierOut()\n","\n","  def forward(self, input):\n","    n = self.normalizer(input)\n","    g_a_a = self.genre_aware_attention(n)\n","    return self.classifier_out(g_a_a)"],"execution_count":null,"outputs":[]}]}