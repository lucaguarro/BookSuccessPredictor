{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Goodreads_stage2.ipynb","provenance":[],"collapsed_sections":["_6gF1bvclYVR","YgoOFdif7bbj","BQXU-G-cjEkk","lavLmaCjjGfG","Ydr19fUDjo9i","z8195Rq9rb6I","Xh-WGSe1doKZ"],"mount_file_id":"1wBVNh_5-j0lep-7H-6-hnXYKB4cztf3_","authorship_tag":"ABX9TyOrl75jIeLhozhurkTY7CAe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BjfCjM2xh9dE"},"source":["In this chapter we used already fine-tuned BERT models to extract the chunk embeddings of our books. The chunk embeddings correspond to either the meaned embeddings of all the words in the sequence or the embedding of the [CLS] token. We will explore the results of both. We will then run a variety of classifiers over these embeddings directly. \n","\n","1.   Meaned pooled output --> single layer NN\n","2.   RoBERT\n","3.   ToBERT ?"]},{"cell_type":"markdown","metadata":{"id":"XLXM0vhe5MQ8"},"source":["# Installs, Imports, Configuration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttrgrV6NN3nB","executionInfo":{"status":"ok","timestamp":1629217285179,"user_tz":420,"elapsed":35468,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"ac7ac69c-b34c-4fde-a143-45c8547459d9"},"source":["!pip install datasets\n","!pip install \"ray[default]\"\n","!pip install wandb\n","!pip install tensorboardX\n","\n","!pip install httplib2==0.15.0\n","!pip install google-api-python-client==1.6"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 66.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 60.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.3)\n","Collecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n","Successfully installed datasets-1.11.0 fsspec-2021.7.0 huggingface-hub-0.0.15 xxhash-2.0.2\n","Collecting ray[default]\n","  Downloading ray-1.5.2-cp37-cp37m-manylinux2014_x86_64.whl (51.0 MB)\n","\u001b[K     |████████████████████████████████| 51.0 MB 63 kB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.0.12)\n","Collecting gpustat\n","  Downloading gpustat-0.6.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 9.5 MB/s \n","\u001b[?25hCollecting py-spy>=0.2.0\n","  Downloading py_spy-0.3.8-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 62.1 MB/s \n","\u001b[?25hCollecting opencensus\n","  Downloading opencensus-0.7.13-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 68.2 MB/s \n","\u001b[?25hCollecting aiohttp-cors\n","  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 41.0 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.34.1)\n","Collecting redis>=3.5.0\n","  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 780 kB/s \n","\u001b[?25hCollecting aioredis<2\n","  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 5.1 MB/s \n","\u001b[?25hCollecting pydantic>=1.8\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 69.9 MB/s \n","\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (0.11.0)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.0.2)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.19.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.13)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.17.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.23.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.6.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (7.1.2)\n","Collecting colorful\n","  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 67.3 MB/s \n","\u001b[?25hCollecting async-timeout\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Collecting hiredis\n","  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[default]) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8->ray[default]) (3.7.4.3)\n","Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]) (3.0.4)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 71.7 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 61.9 MB/s \n","\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[default]) (2.10)\n","Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]) (7.352.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]) (5.4.8)\n","Collecting blessings>=1.6\n","  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n","Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]) (1.26.3)\n","Collecting opencensus-context==0.1.2\n","  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (21.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (57.4.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (2018.9)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (1.53.0)\n","Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (1.34.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (4.7.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (2021.5.30)\n","Building wheels for collected packages: gpustat\n","  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=bb018d54eed25e889b8cf38a33eee7e70b715d10fbde4ba78d2284b379f77a54\n","  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n","Successfully built gpustat\n","Installing collected packages: multidict, yarl, async-timeout, opencensus-context, hiredis, blessings, aiohttp, redis, pydantic, py-spy, opencensus, gpustat, colorama, aioredis, aiohttp-cors, ray, colorful\n","Successfully installed aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 gpustat-0.6.0 hiredis-2.0.0 multidict-5.1.0 opencensus-0.7.13 opencensus-context-0.1.2 py-spy-0.3.8 pydantic-1.8.2 ray-1.5.2 redis-3.5.3 yarl-1.6.3\n","Collecting wandb\n","  Downloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 3.8 MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n","\u001b[K     |████████████████████████████████| 170 kB 65.5 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 9.1 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 69.7 MB/s \n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n","Collecting smmap<5,>=3.0.1\n","  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=f7fbbe177bd074b857e6381e9f010f072a51a4d2abe3bfd10b841193d25441fd\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=53e7de5fb9e1a3f47ce8ec4aa97f763ba485601c83ab425775736b864ec9fae8\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n","Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.0\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4\n","Collecting httplib2==0.15.0\n","  Downloading httplib2-0.15.0-py3-none-any.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n","\u001b[?25hInstalling collected packages: httplib2\n","  Attempting uninstall: httplib2\n","    Found existing installation: httplib2 0.17.4\n","    Uninstalling httplib2-0.17.4:\n","      Successfully uninstalled httplib2-0.17.4\n","Successfully installed httplib2-0.15.0\n","Collecting google-api-python-client==1.6\n","  Downloading google_api_python_client-1.6.0-py2.py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 974 kB/s \n","\u001b[?25hRequirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (4.1.3)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (0.15.0)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (1.15.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.6) (3.0.1)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (4.7.2)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.4.8)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.2.8)\n","Installing collected packages: google-api-python-client\n","  Attempting uninstall: google-api-python-client\n","    Found existing installation: google-api-python-client 1.12.8\n","    Uninstalling google-api-python-client-1.12.8:\n","      Successfully uninstalled google-api-python-client-1.12.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","firebase-admin 4.4.0 requires google-api-python-client>=1.7.8, but you have google-api-python-client 1.6.0 which is incompatible.\n","earthengine-api 0.1.277 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.6.0 which is incompatible.\u001b[0m\n","Successfully installed google-api-python-client-1.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7vwrYLDGtF-","executionInfo":{"status":"ok","timestamp":1629217285181,"user_tz":420,"elapsed":37,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"0f215f51-2104-4843-c0f8-9fdeac0cacc6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QiCafSeXFWzf","executionInfo":{"status":"ok","timestamp":1629217285183,"user_tz":420,"elapsed":20,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["from pathlib import Path\n","import sys\n","import pickle"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA5cgP2i5PB9","executionInfo":{"status":"ok","timestamp":1629217286064,"user_tz":420,"elapsed":898,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["import configparser\n","\n","config = configparser.ConfigParser()\n","config.read('/content/drive/MyDrive/Thesis/BookSuccessPredictor/config.ini')\n","\n","drive_base_path = Path(config['Drive']['drive_base_path'])\n","\n","# sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / '_utils'))\n","sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / 'datasets' / 'goodreads_maharjan_super' / 'MultiModal' / 'dataset_loader'))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6gF1bvclYVR"},"source":["# Get Transformer Model from Stage 1"]},{"cell_type":"code","metadata":{"id":"Sot-nVzelfLN"},"source":["import wandb\n","run = wandb.init()\n","\n","model_name = 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","if config['Model']['name'] == 'distilbert-base-uncased':\n","  if (config['Tokenizer']['overlap']):\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-nlpbosie:v0', type='model')\n","  else:\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-2giwtwvy:v0', type='model')\n","    \n","artifact_dir = artifact.download()\n","\n","transformer_model = DistilBERTForMultipleSequenceClassification.from_pretrained(artifact_dir, num_labels1 = 2, num_labels2 = 8)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHUvF9GXrF_0"},"source":["# Getting the Data"]},{"cell_type":"markdown","metadata":{"id":"p95ei-lSdr9i"},"source":["## Getting the Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"idsWCeYad34q"},"source":["### Creating the Dataset"]},{"cell_type":"markdown","metadata":{"id":"YgoOFdif7bbj"},"source":["#### From script"]},{"cell_type":"code","metadata":{"id":"IjBdxGKsFyZO"},"source":["import torch as th\n","import time\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def getPooledOutputs(model, encoded_dataset, batch_size = 32):\n","  model.eval()\n","\n","  # pooled_outputs = []\n","  pooled_outputs = torch.empty([0,768]).cuda()\n","\n","  num_iters = (len(encoded_dataset['input_ids']) - 1)//batch_size + 1\n","  print(\"total number of iters \", num_iters)\n","  \n","  for i in range(num_iters):\n","    print(i)\n","    up_to = i*batch_size + batch_size\n","    if len(encoded_dataset['input_ids']) < up_to:\n","      up_to = len(encoded_dataset['input_ids'])\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to]).cuda()\n","    attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to]).cuda()\n","\n","    with torch.no_grad():\n","      embeddings = model.forward(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)['hidden_states'][-1][:,0] # Pooled output\n","      pooled_outputs = th.cat([pooled_outputs, embeddings],0)\n","      th.cuda.empty_cache()\n","\n","  return pooled_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"AEVzj6kH8q14","executionInfo":{"status":"error","timestamp":1627520230029,"user_tz":420,"elapsed":271,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"639e4e67-6e42-4430-b11c-c3ea61bcab42"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-dc826b7d6e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunked_encoded_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'chunked_encoded_dataset' is not defined"]}]},{"cell_type":"code","metadata":{"id":"f-AigWJRlMyx"},"source":["train_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjtsYx1YnBL3"},"source":["val_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xYnBmEAnHwp"},"source":["test_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyWQ8O-eEc1D"},"source":["from datasets import Dataset\n","train_set_embeddings = Dataset.from_dict({'pooled_outputs': train_set_embeddings})\n","val_set_embeddings = Dataset.from_dict({'pooled_outputs': val_set_embeddings})\n","test_set_embeddings = Dataset.from_dict({'pooled_outputs': test_set_embeddings})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8agtlOpzEigf"},"source":["from datasets import concatenate_datasets\n","dataset_w_embeddings = DatasetDict({\n","    'train': concatenate_datasets([chunked_encoded_dataset['train'], train_set_embeddings], axis = 1), \n","    'validation': concatenate_datasets([chunked_encoded_dataset['validation'], val_set_embeddings], axis = 1), \n","    'test': concatenate_datasets([chunked_encoded_dataset['test'], test_set_embeddings], axis = 1)\n","})\n","dataset_w_embeddings = dataset_w_embeddings.remove_columns(['attention_mask', 'input_ids', 'token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kK_Uf0Au3RFd"},"source":["dataset_w_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VhVV2CdEo56"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('train_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['train'], output_file)\n","\n","with open('val_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['validation'], output_file)\n","\n","with open('test_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['test'], output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKxTgPtQ7gn5"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"C2e0wfqflvz_","executionInfo":{"status":"ok","timestamp":1629217292170,"user_tz":420,"elapsed":285,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}}},"source":["base_path = Path(\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/60_40/DistilBERT_multitask_sentence_tokenized_dataset_embeddings\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MP8MbdD07vOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217302269,"user_tz":420,"elapsed":9762,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"e2e42a88-8a3e-48eb-a942-4dfe7e0dff27"},"source":["from datasets import DatasetDict\n","with open(base_path / 'train_ds.pkl', \"rb\") as input_file:\n","  train_set_embeddings = pickle.load(input_file)\n","\n","with open(base_path / 'val_ds.pkl', \"rb\") as input_file:\n","  val_set_embeddings = pickle.load(input_file)\n","\n","with open(base_path / 'test_ds.pkl', \"rb\") as input_file:\n","  test_set_embeddings = pickle.load(input_file)\n","\n","dataset_w_embeddings = DatasetDict({'train': train_set_embeddings, 'validation': val_set_embeddings, 'test': test_set_embeddings})\n","dataset_w_embeddings"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 14074\n","    })\n","    validation: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 9872\n","    })\n","    test: Dataset({\n","        features: ['book_title', 'genre', 'success_label', 'pooled_outputs'],\n","        num_rows: 9712\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"5NgXCJLhiz_4"},"source":["## Average Pooled Outputs for Shallow Neural Network and SVM"]},{"cell_type":"markdown","metadata":{"id":"TK4ZZFsWjZAU"},"source":["### Generating the Data from Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"BQXU-G-cjEkk"},"source":["#### From Script"]},{"cell_type":"code","metadata":{"id":"G_TnU36ei2eh"},"source":["def getAveragePooledOutputs(model, encoded_dataset):\n","  book_embeddings_dataset = {'meaned_pooled_output': [], 'book_title': [], 'genre': [], 'labels': []}\n","\n","  book_changes = get_book_changes_idx(encoded_dataset['book_title'])\n","\n","  for i in range(len(book_changes)):\n","    print(i)\n","    start = book_changes[i]\n","    end = None\n","    if i != len(book_changes) - 1:\n","      end = book_changes[i+1]\n","    else:\n","      end = len(encoded_dataset['input_ids'])\n","\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][start:end])\n","    attention_mask = th.BoolTensor(encoded_dataset['attention_mask'][start:end])\n","\n","    with torch.no_grad():\n","      embeddings = transformer_model.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0][:,0] # Pooled output\n","      book_embeddings = th.mean(embeddings, dim=0) # Takes the mean of the pooled output\n","    book_embeddings_dataset['meaned_pooled_output'].append(book_embeddings)\n","    book_embeddings_dataset['book_title'].append(encoded_dataset['book_title'][start])\n","    book_embeddings_dataset['genre'].append(encoded_dataset['genre'][start])\n","    book_embeddings_dataset['labels'].append(encoded_dataset['labels'][start])\n","  \n","  return book_embeddings_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"161S_UpRi5Xq"},"source":["avg_pld_outs_ds = getAveragePooledOutputs(dataset_w_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQXjx63ljRis"},"source":["from datasets import DatasetDict, Dataset\n","avg_pld_outs_hf_ds = DatasetDict({'train': Dataset.from_dict(avg_pld_outs_ds['train']), 'validation': Dataset.from_dict(avg_pld_outs_ds['validation']), 'test': Dataset.from_dict(avg_pld_outs_ds['test'])})\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('avg_pld_outs_hf_ds.pkl', 'wb') as output_file:\n","  pickle.dump(avg_pld_outs_hf_ds, output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('avg_pld_outs_hf_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lavLmaCjjGfG"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"MHOd6dPpk-X-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629007385053,"user_tz":420,"elapsed":2756,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"50b3d2ea-a39f-4b3f-f376-f444d03ba746"},"source":["!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.11.0)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zRmZRxEGjWJT"},"source":["from datasets import DatasetDict\n","with open(r\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/Pooled_Output/60_40/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","  avg_pld_outs_hf_ds = pickle.load(input_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ydr19fUDjo9i"},"source":["# Simple Shallow Neural Network"]},{"cell_type":"code","metadata":{"id":"Aw4ZV-yjjvYe"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","class Net(nn.Module):\n","\n","    def __init__(self, pre_classifier_init, classifier_init):\n","        super(Net, self).__init__()\n","\n","        self.pre_classifier = nn.Linear(768, 768)\n","        self.classifier = nn.Linear(768, 2)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.pre_classifier.weight.data.copy_(pre_classifier_init.weight.data)\n","        self.classifier.weight.data.copy_(classifier_init.weight.data)\n","\n","        # print(pre_classifier_init.bias.data)\n","        self.pre_classifier.bias.data.copy_(pre_classifier_init.bias.data)\n","        self.classifier.bias.data.copy_(classifier_init.bias.data)\n","\n","        # DOUBLE CHECK IF BIASES ARE BEING SET AS WELL\n","\n","    def forward(self, x, labels = None):\n","        # Max pooling over a (2, 2) window\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        logits = self.classifier(x)\n","\n","        loss = None\n","        if labels is not None:\n","          loss_fct = CrossEntropyLoss()\n","          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","        return SequenceClassifierOutput(\n","            loss = loss,\n","            logits = logits\n","        )\n","\n","net = Net(model.pre_classifier, model.classifier1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3kxvLBJj70p"},"source":["#### Results with no Training"]},{"cell_type":"code","metadata":{"id":"2nhYqvBgkAkH"},"source":["net.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecoQ7GOOj_ml"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['validation']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['validation']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6v9W_FydkEOR"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['test']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['test']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvjGT-vJkO2x"},"source":["#### Training w Hyperparameter Tuning and Results"]},{"cell_type":"code","metadata":{"id":"DSHANKmHkSzp"},"source":["def load_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  trainset = avg_pld_outs_hf_ds['train']\n","  valset = avg_pld_outs_hf_ds['validation']\n","  return trainset, valset\n","\n","def load_test_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  testset = avg_pld_outs_hf_ds['test']\n","  return testset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kM4cHtBkqaf"},"source":["from ray import tune\n","# from ray.tune.integration.wandb import wandb_mixin\n","# '''@wandb_mixin\n","# run = wandb.init()\n","\n","def train_nn(config, checkpoint_dir, data_dir=None):\n","  net = Net(model.pre_classifier, model.classifier1, config['do_rate'])\n","  net.train()\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      if torch.cuda.device_count() > 1:\n","          net = nn.DataParallel(net)\n","  print(type(net))\n","  net.to(device)\n","  # net.cuda()\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], shuffle=True)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","    for i, data in enumerate(trainloader, 0):\n","\n","      inputs = data['meaned_pooled_output']\n","      labels = data['success_label']\n","\n","      inputs, labels = inputs.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = net(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if i % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      net.eval()\n","      with torch.no_grad():\n","        for i, data in enumerate(valloader, 0):\n","\n","          inputs_cpu = data['meaned_pooled_output']\n","          labels_cpu = data['success_label']\n","\n","          inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","          # inputs.cuda()\n","          # labels.cuda()\n","\n","          outputs = net(inputs)\n","          _, predicted = torch.max(outputs.data, 1)\n","\n","          all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","          all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","          loss = criterion(outputs, labels)\n","          val_loss += loss.cpu().numpy()\n","          val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((net.state_dict(), optimizer.state_dict()), path)\n","\n","      net.train()\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      # s_acc = accuracy_score(all_labels, all_predictions)\n","      # wandb.log({\"val_loss\": val_loss / val_steps, \"val_accuracy\": correct / total})\n","      tune.report(loss=(val_loss / val_steps), accuracy=correct / total, f1=s_f1, precision=s_precision, recall=s_recall)\n","  print(\"Finished Training\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBaWnJvfkwCR"},"source":["def test_results(net, device=\"cpu\"):\n","    testset = load_test_data()\n","\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n","\n","    all_predictions = np.array([])\n","    all_labels = np.array([])\n","\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader, 0):\n","            inputs_cpu = data['meaned_pooled_output']\n","            labels_cpu = data['success_label']\n","\n","            inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","            all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","    s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","    return {\n","        'precision': s_precision,\n","        'recall': s_recall,\n","        'f1': s_f1\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aom_a9B_kUL7"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.tune.integration.wandb import WandbLoggerCallback\n","import torch.optim as optim\n","from functools import partial\n","\n","def main(num_samples = 15, max_num_epochs = 10):\n","  config = {\n","      \"lr\": tune.loguniform(1e-4, 1e-1),\n","      \"batch_size\": tune.choice([16,32,64,128]),\n","      \"num_epochs\": tune.choice([1,2]),#,2,3]),#,2,3,5,10,20]),\n","      \"do_rate\": tune.uniform(0.1, 0.5),\n","      \"wandb\": {\n","        \"project\": \"AvgPooledOutputClassifier\",\n","        \"api_key\": config['WandB']['api_key']\n","      }\n","    }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(train_nn, checkpoint_dir='/tmp/ShallowNNModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"AvgPooledOutputClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"accuracy\"]))\n","  \n","  best_trained_model = Net(model.pre_classifier, model.classifier1, best_trial.config['do_rate'])\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","\n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/ShallowNNModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANZwhwsqk4C9"},"source":["main(num_samples=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8195Rq9rb6I"},"source":["# SVM"]},{"cell_type":"code","metadata":{"id":"Psd_tsr7rwkw"},"source":["from sklearn import svm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RWgJonqrzzi"},"source":["cs = np.arange(0.9, 1.5, 0.02).tolist()\n","best_clf = None\n","best_score = 0\n","best_c = None\n","for c in cs:\n","  clf = svm.SVC(kernel='rbf', gamma='scale', C=c)\n","  clf.fit(avg_pld_outs_hf_ds['train']['meaned_pooled_output'], avg_pld_outs_hf_ds['train']['success_label'])\n","  predictions = clf.predict(avg_pld_outs_hf_ds['validation']['meaned_pooled_output'])\n","  (_, pred_counts) = np.unique(predictions, return_counts=True)\n","  val_score = f1_score(avg_pld_outs_hf_ds['validation']['success_label'], predictions, average = 'weighted')\n","  print('Clf with C = {} obtained val-score of {}'.format(c, val_score))\n","  if (val_score > best_score):\n","    best_score = val_score\n","    best_clf = clf\n","    best_c = c\n","\n","print('\\nBest C: {}; Val-score: {}'.format(best_c, best_score))\n","test_predictions = best_clf.predict(avg_pld_outs_hf_ds['test']['meaned_pooled_output'])\n","test_score = f1_score(avg_pld_outs_hf_ds['test']['success_label'], test_predictions, average = 'weighted')\n","print('Yields score of {} on test set'.format(test_score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xh-WGSe1doKZ"},"source":["# RoBERT"]},{"cell_type":"code","metadata":{"id":"XaGB0mqPdTCr"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNINcDJopd1-"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  return book_changes_idx\n","\n","def convert_to_LSTM_dataset_full(dataset):\n","  full_ds = {}\n","  full_ds['train'] = convert_to_LSTM_dataset_sub(dataset['train'])\n","  full_ds['validation'] = convert_to_LSTM_dataset_sub(dataset['validation'])\n","  full_ds['test'] = convert_to_LSTM_dataset_sub(dataset['test'])\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  return full_ds\n","\n","def convert_to_LSTM_dataset_sub(dataset):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  book_start_idx = get_book_changes_idx(dataset['book_title'])\n","  book_start_idx_w_end = np.append(book_start_idx, len(dataset['book_title']))\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = dataset['pooled_outputs'].split_with_sizes(list(book_lengths))\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  # print(type(book_grouped_embeddings))\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = np.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = np.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_slJ4X3Bpf_Q"},"source":["class RoBERT_Model(nn.Module):\n","\n","    def __init__(self, layer_size = 100):\n","        self.layer_size = layer_size\n","        super(RoBERT_Model, self).__init__()\n","        self.lstm = nn.LSTM(768, layer_size, num_layers=1, bidirectional=False)\n","        self.out = nn.Linear(layer_size, 2)\n","\n","    def forward(self, grouped_pooled_outs):\n","        \"\"\" Define how to performed each call\n","        Parameters\n","        __________\n","        pooled_output: array\n","            -\n","        lengt: int\n","            -\n","        Returns:\n","        _______\n","        -\n","        \"\"\"\n","        # chunks_emb = pooled_out.split_with_sizes(lengt) # splits the input tensor into a list of tensors where the length of each sublist is determined by lengt\n","\n","        seq_lengths = torch.LongTensor([x for x in map(len, grouped_pooled_outs)]) # gets the length of each sublist in chunks_emb and returns it as an array\n","\n","        batch_emb_pad = nn.utils.rnn.pad_sequence(grouped_pooled_outs, padding_value=-91, batch_first=True) # pads each sublist in chunks_emb to the largest sublist with value -91\n","        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n","        lstm_input = nn.utils.rnn.pack_padded_sequence(batch_emb, seq_lengths, batch_first=False, enforce_sorted=False) # seq_lengths.cpu().numpy()\n","\n","        packed_output, (h_t, h_c) = self.lstm(lstm_input, )  # (h_t, h_c))\n","        # output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=-91)\n","\n","        h_t = h_t.view(-1, self.layer_size) # (-1, 100)\n","\n","        return self.out(h_t) # logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEPdCiU6phS2"},"source":["def my_collate1(batches):\n","  # for batch in batches:\n","  #   print(type(batch['grouped_pooled_outs']), len(batch['grouped_pooled_outs']))\n","  #   print(type(torch.FloatTensor(batch['grouped_pooled_outs'])))\n","    return {\n","        'grouped_pooled_outs': [torch.stack(x['grouped_pooled_outs']) for x in batches],\n","        'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTNSTUQkpnQp"},"source":["from transformers import AdamW\n","import time\n","\n","def load_test_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  testset = full_ds['test']\n","  return testset\n","\n","def load_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  trainset = full_ds['train']\n","  valset = full_ds['validation']\n","  return trainset, valset\n","\n","# def loss_fun(outputs, targets):\n","#     loss = nn.CrossEntropyLoss()\n","#     return loss(outputs, targets)\n","\n","def rnn_train_fun1(config, checkpoint_dir='/tmp/LSTMModels'):\n","  model = RoBERT_Model(config[\"layer_size\"])\n","  model.train()\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #   device = \"cuda:0\"\n","  #   if torch.cuda.device_count() > 1:\n","  #       model = nn.DataParallel(model)\n","  # # print(type(model))\n","  # model.to(device)\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(trainloader):\n","      grouped_pooled_outs = batch['grouped_pooled_outs'] # .to(device)\n","      targets = batch['success_label'] #.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(grouped_pooled_outs)\n","      loss = loss_fun(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if batch_idx % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      with torch.no_grad():\n","          for i, data in enumerate(valloader, 0):\n","\n","              grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","              targets = data['success_label'] # .to(device)\n","\n","              outputs = model(grouped_pooled_outs)\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.numpy())\n","              all_labels = np.append(all_labels, targets.numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6beZxWFXpsXW"},"source":["def test_results(net, device=\"cpu\"):\n","  testset = load_test_data()\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=8, collate_fn=my_collate1)\n","\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.eval()\n","  with torch.no_grad():\n","    for i, data in enumerate(testloader, 0):\n","        grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","        targets = data['success_label'] # .to(device)\n","\n","        outputs = net(grouped_pooled_outs)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxp-MTSnpvZN"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","  config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([16,32,64]),\n","    \"num_epochs\": tune.choice([1,2,3,5]),\n","    \"layer_size\": tune.choice([100]),\n","    \"wandb\": {\n","      \"project\": \"LSTMClassifier\",\n","      \"api_key\": config['WandB']['api_key'],\n","      \"log_config\": True\n","    }\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(rnn_train_fun1, checkpoint_dir='/tmp/LSTMModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"LSTMClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = RoBERT_Model(best_trial.config['layer_size'])\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #     device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/LSTMModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETTb9Z0vpy00"},"source":["test_results = main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55x8tH3RG2o-"},"source":["# ToBERT"]},{"cell_type":"markdown","metadata":{"id":"2hsJkN0qJhyQ"},"source":["### ToBERT dataset"]},{"cell_type":"code","metadata":{"id":"ZZ3o8-g8fpt_"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])\n","book_start_idx, book_lengths = get_book_starts_and_lengths(dataset_w_embeddings['validation']['book_title'])\n","pld_outs_split = dataset_w_embeddings['validation']['pooled_outputs'].split_with_sizes(list(book_lengths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZ2hgPWDguNd"},"source":["from torch.nn.utils.rnn import pad_sequence\n","ay = pad_sequence(list(dataset_w_embeddings['validation']['pooled_outputs'].split_with_sizes(list(book_lengths))), batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrdbOdjJozx1","executionInfo":{"status":"ok","timestamp":1629179053631,"user_tz":420,"elapsed":523,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"86dae9de-d970-406b-821d-d7984acb5460"},"source":["book_lengths[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["43"]},"metadata":{"tags":[]},"execution_count":161}]},{"cell_type":"code","metadata":{"id":"co1RUp_JoBcJ"},"source":["torch.set_printoptions(edgeitems=44)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMwRvHrHdBcw"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","\n","def get_book_starts_and_lengths(book_titles):\n","  book_start_idx = get_book_changes_idx(book_titles)\n","  book_start_idx_w_end = np.append(book_start_idx, len(book_titles))\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  return book_start_idx, book_lengths\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  return book_changes_idx\n","\n","def convert_to_transformer_dataset_full(dataset):\n","  full_ds = {}\n","\n","  # we want to figure out the book_lengths at this point because we want to know how long we should pad the sequences to.\n","  train_book_start_idx, train_book_lengths = get_book_starts_and_lengths(dataset['train']['book_titles'])\n","  val_book_start_idx, val_book_lengths = get_book_starts_and_lengths(dataset['validation']['book_titles'])\n","  test_book_start_idx, test_book_lengths = get_book_starts_and_lengths(dataset['test']['book_titles'])\n","  max_seq_len = get_max_seq_length(train_book_lengths, val_book_lengths, test_book_lengths)\n","\n","  full_ds['train'] = convert_to_transformer_dataset_sub(dataset['train'], train_book_start_idx, train_book_lengths, max_seq_len)\n","  full_ds['validation'] = convert_to_transformer_dataset_sub(dataset['validation'], val_book_start_idx, val_book_lengths, max_seq_len)\n","  full_ds['test'] = convert_to_transformer_dataset_sub(dataset['test'], test_book_start_idx, test_book_lengths, max_seq_len)\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  return full_ds\n","\n","def convert_to_transformer_dataset_sub(dataset, book_start_idx, book_lengths, max_seq_length):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  # book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = list(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)))\n","  book_grouped_embeddigns = pad_sequence(list(book_grouped_embeddings, batch_first=True)\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","  ds['book_lengths'] = book_lengths\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = torch.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = torch.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds\n","\n","def get_max_seq_length(train_book_lengths, val_book_lengths, test_book_lengths):\n","  return max(max(train_book_lengths),max(val_book_lengths),max(test_book_lengths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgmGXetlecyX"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LdK6SF2ee9v"},"source":["adapted_ds = convert_to_transformer_dataset_full(dataset_w_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfbeQQk-Jt5V"},"source":["### Defining Model"]},{"cell_type":"markdown","metadata":{"id":"81y1MGXaPXPs"},"source":["The Embedding layer:\n","nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n","uses a vector of size 768. Implies that TransformerEncoderLayer works with embeddings of length 768."]},{"cell_type":"code","metadata":{"id":"Zg46HoiZ-3nK"},"source":["import torch.nn as nn\n","\n","# d_model = 768, nhead = 2, d_hid = 200, dropout = 0.1, nlayers = 2\n","class ToBERT(nn.Module):\n","    def __init__(\n","        self,\n","        d_model=768,\n","        nhead=2,\n","        nhid=200,\n","        num_layers=2,\n","        dropout=0.1,\n","        classifier_dropout=0.1,\n","        # max_len=256,\n","    ):\n","\n","        super().__init__()\n","\n","        # self.d_model = embeddings.size(1)\n","        assert (\n","            self.d_model % nhead == 0\n","        ), \"nheads must divide evenly into d_model\"\n","\n","        # self.emb = nn.Embedding.from_pretrained(embeddings, freeze=False)\n","        # self.pos_encoder = PositionalEncoding(\n","        #     self.d_model, dropout=dropout, max_len=embeddings.size(0)\n","        # )\n","\n","        encoder_layers = nn.TransformerEncoderLayer(\n","            d_model=self.d_model, nhead=nhead, dim_feedforward=nhid, dropout=dropout, batch_first=True\n","        )\n","        self.transformer_encoder = nn.TransformerEncoder(\n","            encoder_layers, num_layers=num_layers\n","        )\n","        self.classifier = nn.Sequential(\n","            # Other layers to go here if needed once things seem to be working\n","            nn.Linear(self.d_model, 2),\n","        )\n","\n","    def forward(self, x, src_key_padding_mask):\n","        # x = self.emb(x) * math.sqrt(self.d_model)\n","        # x = self.pos_encoder(x)\n","        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)  # self.src_mask)\n","\n","        # calculates mean taking into account the padding\n","        x = torch.unsqueeze(1-src_key_padding_mask,2)*ay[0]\n","        x = x.sum(dim=1)/(1-src_key_padding_mask).sum(dim=1).unsqueeze(1)      \n","        # x = x.mean(dim=1)\n","        return self.classifier(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57Zmzyu7_pyO"},"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qg3P8DuLUGZX"},"source":["def get_batch_mask(max_seq_len, book_lens):\n","  mask = torch.zeros(len(book_lens),max_seq_len+1) # batch_size, seq_len\n","  mask[(torch.arange(len(book_lens)),book_lens)] = 1\n","  mask = mask.cumsum(dim=1)[:, :-1]\n","  return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYzV_2m0UJuh"},"source":["book_lens = torch.LongTensor([2,4,5])\n","max_seq_len = 6\n","src_key_padding_mask = get_batch_mask(max_seq_len, book_lens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST5uYHfbK2mL"},"source":["# class ToBERT(nn.Module):\n","\n","#   def __init__(self, d_model, nhead, dropout, d_hid, nlayers, nclasses):\n","#       # d_model = 768, nhead = 2, d_hid = 200, dropout = 0.1, nlayers = 2\n","#       encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n","#       self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","#       self.classifier = nn.Linear(d_model, nclasses)\n","\n","#   def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n","#       \"\"\"\n","#       Args:\n","#           src: Tensor, shape [seq_len, embedding_dim, batch_size]\n","#           src_mask: Tensor, shape [seq_len, seq_len]\n","\n","#       seq_len should be the max number of segments a book has in our dataset\n","#       embedding_dim will be 768 (from BERT)\n","\n","#       src_mask is necessary because we will need to pad shorter books to have as many segments\n","#       as the longest book. Obviously we do not want our model to attend to the padded tokens in\n","#       these cases.\n","#       \"\"\"\n","#       output = self.transformer_encoder(src, src_mask) \n","#       output = self.classifier(output)\n","#       return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mj191iy1JxE3"},"source":["### Playground"]},{"cell_type":"markdown","metadata":{"id":"JejUCDYSO5MS"},"source":["we can see that the values of the 1st and 2nd tensors didnt change when we applied the masking properly"]},{"cell_type":"code","metadata":{"id":"YwtfJrmL3vAk"},"source":["def get_batch_mask(max_seq_len, book_lens):\n","  mask = torch.zeros(len(book_lens),max_seq_len+1) # batch_size, seq_len\n","  mask[(torch.arange(len(book_lens)),book_lens)] = 1\n","  mask = mask.cumsum(dim=1)[:, :-1]\n","  return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JY8jZC4D4yzg"},"source":["book_lens = torch.LongTensor([2,4,5])\n","max_seq_len = 6\n","src_key_padding_mask = get_batch_mask(max_seq_len, book_lens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HK8N_vPh6riY","executionInfo":{"status":"ok","timestamp":1629168366321,"user_tz":420,"elapsed":424,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"8797ad6b-4435-4be6-98c0-d4007dc4794e"},"source":["src_key_padding_mask.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 6])"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"FQT6yRnu6Uxq"},"source":["import torch, torch.nn as nn\n","q = torch.randn(3, 6, 10) # batch size 3, source sequence length 6, embedding size 10\n","attn = nn.MultiheadAttention(10, 1, batch_first=True) # embedding size 10, one head\n","\n","ay = attn(q, q, q, key_padding_mask=src_key_padding_mask) # self attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1eC_4ao_X4P","executionInfo":{"status":"ok","timestamp":1629168997574,"user_tz":420,"elapsed":192,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"855add65-d33e-4a03-df3a-519daaada6c6"},"source":["src_key_padding_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_KWWEgoD7v8","executionInfo":{"status":"ok","timestamp":1629169587551,"user_tz":420,"elapsed":211,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"9047e658-d666-4d7d-f1cf-c8e4b8e57c22"},"source":["y = torch.unsqueeze(1-src_key_padding_mask,2)*ay[0]\n","y.sum(dim=1)/(1-src_key_padding_mask).sum(dim=1).unsqueeze(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0207,  0.0189, -0.1383,  0.0378, -0.0212, -0.0739, -0.0113,  0.0461,\n","         -0.0231,  0.1898],\n","        [-0.0730,  0.2946,  0.1850, -0.4192,  0.3180,  0.5532, -0.2360, -0.2962,\n","         -0.1943, -0.3110],\n","        [ 0.2559, -0.1303,  0.3230,  0.0399,  0.0889,  0.0014,  0.0462,  0.0022,\n","          0.3451, -0.1787]], grad_fn=<DivBackward0>)"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"1kw2weKz-lg6"},"source":["# MultiModal"]},{"cell_type":"markdown","metadata":{"id":"6c612HylViH5"},"source":["### Defining the Modal"]},{"cell_type":"code","metadata":{"id":"ZRbTSOAuasG1"},"source":["import torch.nn as nn\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gVjETdBcQi2"},"source":["Our model will be composed of three separate modules:\n","\n","1. (Normalizer) Responsible for taking all the inputs of various dimensions and feeding them each through their own linear layer to project them into a space with all the same dimensions\n","\n","In essence, it is responsible for eq (1) in the paper $h_i=selu(W_{h_i} x_i + b_h)$\n","\n","\n","2. (GenreAwareAttention) This is where most of the meat of the model is. It is responsible for performing these 3 equations. \n","\n","$score(h_i, g) = v^T selu(W_a h_i + W_g g + b_a)$\n","\n","$\\alpha_i = \\frac{exp(score(h_i,g))}{\\sum_{i'}exp(score(h_{i'},g)}$\n","\n","$r=\\sum_i \\alpha_i h_i$\n","\n","3. (ClassOutput) The last layer is simply responsible for projecting the book representation to class probabilities.\n","\n","$\\hat{p}=\\sigma(W_c r + b_c)$"]},{"cell_type":"code","metadata":{"id":"U0vC_Exg-nTp"},"source":["class Normalizer(nn.Module):\n","  def __init__(self, c5g_size, bf_size, std_dims):\n","    super(Normalizer, self).__init__()\n","\n","    self.c5g_linear = nn.Linear(c5g_size, std_dims)\n","    self.bf_linear = nn.Linear(bf_size, std_dims)\n","\n","  def forward(self, x_c5g, x_bf):\n","    # x_c5g ~ (BATCH_SIZE, C5G_FEATURE_SIZE)\n","    # x_bf ~ (BATCH_SIZE, BF_FEATURE_SIZE)\n","    # # split features into char_5_gram and bert_features\n","    # char_5_grams = None\n","    # bert_features = None\n","\n","    c5g_normed = self.c5g_linear(x_c5g)\n","    bf_normed = self.bf_linear(x_bf)\n","\n","    # concatenate c5g_normed and bf_normed\n","    return torch.stack([c5g_normed, bf_normed], 1) # (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzj5ackHOUQu"},"source":["class GenreAwareAttention(nn.Module):\n","  def __init__(self, std_dims, num_units, do_rate):\n","    super(GenreAwareAttention, self).__init__()\n","    self.activation = nn.SELU()\n","    self.nn_softmax = nn.Softmax(dim=1)\n","\n","    self.v = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,1)),\n","        requires_grad=True\n","    )\n","\n","    self.Wa = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(std_dims,num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.b = nn.parameter.Parameter(\n","        nn.init.ones_(torch.empty(num_units,)),\n","        requires_grad=True\n","    )\n","\n","    self.Wg = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(8, num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.dropout = nn.Dropout(p=do_rate)\n","\n","  def forward(self, x, g):\n","    # x ~ (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)\n","    # g ~ (BATCH_SIZE, 1, GENRE_EMBED_SIZE)\n","    \n","    # calculate scores\n","    atten_g = torch.mm(g, self.Wg).unsqueeze(dim=1)\n","    et = self.activation(torch.matmul(x, self.Wa) + atten_g + self.b)\n","    et = self.dropout(et)\n","    \n","    et = torch.matmul(et, self.v)\n","\n","    at = self.nn_softmax(et)\n","\n","    # at = torch.unsqueeze(at, axis=-1)\n","\n","    # print('at:', at.size())\n","    # print('x:', x.size())\n","    ot = at * x # canot multiply at: torch.Size([4, 2, 1, 1]) x: torch.Size([4, 2, 100])\n","\n","    return torch.sum(ot, axis=1) # BATCH_SIZE, EMBED_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxcCXEr4iPWB"},"source":["class ClassifierOut(nn.Module):\n","  def __init__(self, std_dims):\n","    super(ClassifierOut, self).__init__()\n","    self.classifier = nn.Linear(std_dims, 2)\n","  \n","  def forward(self, r): # r ~ BATCH_SIZE, EMBED_SIZE\n","    r_out = self.classifier(r) # BATCH_SIZE, 2\n","    return torch.sigmoid(r_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sb7CIVGIGD-G"},"source":["class FullModel(nn.Module): # may want to consider also adding a dropout layer before classification\n","  def __init__(self, c5g_size, bf_size, std_dims, num_units, do_rate):\n","    super(FullModel,self).__init__()\n","    self.normalizer = Normalizer(c5g_size, bf_size, std_dims)\n","    self.genre_aware_attention = GenreAwareAttention(std_dims, num_units, do_rate)\n","    self.classifier_out = ClassifierOut(std_dims)\n","\n","  def forward(self, x_c5g, x_bf, genre):\n","    x_normed = self.normalizer(x_c5g, x_bf)\n","    g_a_a = self.genre_aware_attention(x_normed, genre)\n","    return self.classifier_out(g_a_a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQO49DFsVmUr"},"source":["### Getting the Data"]},{"cell_type":"code","metadata":{"id":"r62qM0beUEER"},"source":["from MultimodalGoodreadsDataset import MultimodalGoodreadsDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ma0olfGhVnaI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628662514739,"user_tz":420,"elapsed":22081,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"34fed12f-95e6-43de-97fe-d3facd059d2a"},"source":["dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'\n","\n","ds = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)\n","\n","def my_collate_fn(batches, f1_len, f2_len):\n","    return {\n","        'c5g_f': torch.tensor([x['text_features'].toarray()[0][0:f1_len] for x in batches]), # dtype = Float?\n","        'bert_f': torch.tensor([x['text_features'].toarray()[0][f1_len:f1_len+f2_len] for x in batches]), \n","        'genre': torch.tensor([x['genre'] for x in batches]),\n","        'label': torch.tensor([x['label'] for x in batches])\n","    }\n","\n","# c5g_len = ds.f_lengths[0]\n","# bf_len = ds.f_lengths[1]\n","\n","# train_dataloader = DataLoader(ds.train, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))\n","# val_dataloader = DataLoader(ds.val, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))\n","# test_dataloader = DataLoader(ds.test, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total test instances: 290, validation instances: 290, and Training instances: 404\n","Total unique books: 984\n","Training instances (404,), Val instances (290,), Test instances (290,)\n","extracting feature: char_5_gram\n","Using cached features\n","extracting feature: bert_features\n","Using cached features\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeNghsJ-4M9v"},"source":["# from pydrive.auth import GoogleAuth\n","# from pydrive.drive import GoogleDrive\n","# from google.colab import auth\n","# from oauth2client.client import GoogleCredentials\n","\n","# import pickle\n","\n","# # 1. Authenticate and create the PyDrive client.\n","# auth.authenticate_user()\n","# gauth = GoogleAuth()\n","# gauth.credentials = GoogleCredentials.get_application_default()\n","# drive = GoogleDrive(gauth)  \n","\n","# with open('train_dataset.pkl', 'wb') as output_file:\n","#   pickle.dump(ds.train, output_file)\n","\n","# with open('val_dataset.pkl', 'wb') as output_file:\n","#   pickle.dump(ds.val, output_file)\n","\n","# with open('test_dataset.pkl', 'wb') as output_file:\n","#   pickle.dump(ds.test, output_file)\n","\n","# folder_id = '1q2IGZrQ9oNwP-CqttWUuiYcenb8vmWUg'\n","# # get the folder id where you want to save your file\n","# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","# file.SetContentFile('train_dataset.pkl')\n","# file.Upload() \n","\n","# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","# file.SetContentFile('val_dataset.pkl')\n","# file.Upload() \n","\n","# # get the folder id where you want to save your file\n","# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","# file.SetContentFile('test_dataset.pkl')\n","# file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_f7F3QWf6Gw"},"source":["from datasets import Dataset\n","class MultimodalGoodreadsDatasetSplit(Dataset):\n","\n","    def __init__(self, X, genres, Y):\n","        self.X = X\n","        self.genres = genres\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.Y)\n","\n","    def __getitem__(self, idx):\n","        return {'text_features': self.X[idx], 'genre': self.genres[idx], 'label': self.Y[idx]}\n","\n","mmgrds_train = MultimodalGoodreadsDatasetSplit(ds.train.X, ds.train.genres, ds.train.Y)\n","mmgrds_val = MultimodalGoodreadsDatasetSplit(ds.val.X, ds.val.genres, ds.val.Y)\n","mmgrds_test = MultimodalGoodreadsDatasetSplit(ds.test.X, ds.test.genres, ds.test.Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eTMCXp0Y8PA"},"source":["from torch.utils.data import DataLoader\n","import torch\n","from functools import partial\n","\n","def my_collate_fn(batches, f1_len, f2_len):\n","    return {\n","        'c5g_f': torch.tensor([x['text_features'].toarray()[0][0:f1_len] for x in batches]), # dtype = Float?\n","        'bert_f': torch.tensor([x['text_features'].toarray()[0][f1_len:f1_len+f2_len] for x in batches]), \n","        'genre': torch.tensor([x['genre'] for x in batches]),\n","        'label': torch.tensor([x['label'] for x in batches])\n","    }\n","\n","def load_data():\n","  return mmgrds_train, mmgrds_val\n","\n","def load_test_data():\n","  return mmgrds_test\n","# def load_data():\n","#   dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","#   cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'\n","\n","#   ds = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)\n","\n","#   return ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZezPhKzsZd6H"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"kRTXe1tzWUAz"},"source":["from torch.optim import AdamW\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support \n","\n","def mm_train_fun1(config, checkpoint_dir='/tmp/MultiModalModels'):\n","\n","  train_dataset, val_dataset = load_data()\n","  model = FullModel(311595, 768, config['std_dims'], config['num_units'], config['do_rate']).to('cuda')\n","  model.train()\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","  # [311595, 768]\n","  train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=311595, f2_len=768))\n","  val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=311595, f2_len=768))\n","  print(\"len(train_dataloader)\",len(train_dataloader))\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(train_dataloader):\n","\n","\n","      c5g_f = batch['c5g_f'].to('cuda')\n","      bert_f = batch['bert_f'].to('cuda')\n","      genre = batch['genre'].to('cuda')\n","      targets = batch['label'].to('cuda')\n","\n","      optimizer.zero_grad()\n","      outputs = model(c5g_f.float(), bert_f.float(), genre.float())\n","      loss = criterion(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      model.eval()\n","      with torch.no_grad():\n","          for i, batch_v in enumerate(val_dataloader, 0):\n","\n","              c5g_f = batch_v['c5g_f'].to('cuda')\n","              bert_f = batch_v['bert_f'].to('cuda')\n","              genre = batch_v['genre'].to('cuda')\n","              targets = batch_v['label'].to('cuda')\n","\n","              outputs = model(c5g_f.float(), bert_f.float(), genre.float())\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.cpu().numpy())\n","              all_labels = np.append(all_labels, targets.cpu().numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      model.train()\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      print('s_precision:', s_precision, 's_recall:', s_recall, 's_f1:', s_f1)\n","      tune.report(loss = loss.item(), epoch = epoch + batch_idx / len(train_dataloader), eval_loss=(val_loss / val_steps), eval_f1=s_f1, eval_precision=s_precision, eval_recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nM6WLGVqxIDY"},"source":["def test_results(net, test_dataloader, device=\"cpu\"):\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.to(device)\n","  net.eval()\n","  with torch.no_grad():\n","    for i, batch_test in enumerate(test_dataloader, 0):\n","        c5g_f = batch_test['c5g_f']\n","        bert_f = batch_test['bert_f']\n","        genre = batch_test['genre']\n","        targets = batch_test['label']\n","\n","        outputs = net(c5g_f.float(), bert_f.float(), genre.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIqVS_dlBVbK"},"source":["from ray import tune\n","from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger, WandbLoggerCallback\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","import os\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","\n","  tune_config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([32,64,128]),\n","    \"num_epochs\": tune.choice([1]),#,3,5,7,9]),\n","    \"std_dims\": tune.sample_from(lambda _: np.random.randint(50,300)),\n","    \"num_units\": tune.sample_from(lambda spec: np.random.randint(25,spec.config.std_dims)),\n","    \"do_rate\": tune.uniform(0, 0.5),\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(mm_train_fun1, checkpoint_dir='/tmp/MMModels'),\n","    config = tune_config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'eval_loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"MultiModalClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  best_trial = result.get_best_trial(metric=\"eval_f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"eval_loss\"]))\n","  print(\"Best trial final validation weighted f1: {}\".format(\n","      best_trial.last_result[\"eval_f1\"]))\n","  \n","  best_trained_model = FullModel(311595, 768, best_trial.config['std_dims'], best_trial.config['num_units'], best_trial.config['do_rate'])\n","  device = \"cpu\"\n","\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/classifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","\n","  test_ds = load_test_data()\n","  test_dataloader = DataLoader(test_ds, batch_size=best_trial.config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=311595, f2_len=768))\n","  return test_results(best_trained_model, test_dataloader, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"px4KGuLsFyX7","executionInfo":{"status":"error","timestamp":1628664617603,"user_tz":420,"elapsed":370805,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"09f5c7be-7db4-46e4-abe7-e38374bd31e1"},"source":["test_scores = main(num_samples = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-08-11 06:44:04,893\tWARNING experiment.py:296 -- No name detected on trainable. Using DEFAULT.\n","2021-08-11 06:44:04,895\tINFO registry.py:67 -- Detected unknown callable for trainable. Converting to class.\n","2021-08-11 06:44:16,757\tWARNING worker.py:1189 -- Warning: The actor ImplicitFunc has size 204400654 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n","2021-08-11 06:44:18,044\tWARNING util.py:164 -- The `start_trial` operation took 6.849 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 6.5/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=757)\u001b[0m len(train_dataloader) 4\n","\u001b[2m\u001b[36m(pid=757)\u001b[0m saving in checkpoint dir\n","\u001b[2m\u001b[36m(pid=757)\u001b[0m s_precision: 0.7724137931034483 s_recall: 0.6482758620689655 s_f1: 0.5134804850322092\n","Result for DEFAULT_874d9_00000:\n","  date: 2021-08-11_06-45-55\n","  done: false\n","  epoch: 0.0\n","  eval_f1: 0.5134804850322092\n","  eval_loss: 0.6167731881141663\n","  eval_precision: 0.7724137931034483\n","  eval_recall: 0.6482758620689655\n","  experiment_id: 5548763094fa48b0b0f9cbcb377b9523\n","  hostname: 3d258191eee7\n","  iterations_since_restore: 1\n","  loss: 0.6405212879180908\n","  node_ip: 172.28.0.2\n","  pid: 757\n","  should_checkpoint: true\n","  time_since_restore: 87.71400427818298\n","  time_this_iter_s: 87.71400427818298\n","  time_total_s: 87.71400427818298\n","  timestamp: 1628664355\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 874d9_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 9.3/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -0.6167731881141663<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.6167731881141663 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>172.28.0.2:757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          87.714</td><td style=\"text-align: right;\">0.640521</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   0.616773</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:46:00,112\tWARNING util.py:164 -- The `process_trial_save` operation took 4.204 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=757)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_874d9_00000:\n","  date: 2021-08-11_06-47-17\n","  done: false\n","  epoch: 0.25\n","  eval_f1: 0.7056996416828881\n","  eval_loss: 0.5765504439671835\n","  eval_precision: 0.7224287856071964\n","  eval_recall: 0.7275862068965517\n","  experiment_id: 5548763094fa48b0b0f9cbcb377b9523\n","  hostname: 3d258191eee7\n","  iterations_since_restore: 2\n","  loss: 0.6037033200263977\n","  node_ip: 172.28.0.2\n","  pid: 757\n","  should_checkpoint: true\n","  time_since_restore: 168.92903876304626\n","  time_this_iter_s: 81.21503448486328\n","  time_total_s: 168.92903876304626\n","  timestamp: 1628664437\n","  timesteps_since_restore: 0\n","  training_iteration: 2\n","  trial_id: 874d9_00000\n","  \n","\u001b[2m\u001b[36m(pid=757)\u001b[0m s_precision: 0.7224287856071964 s_recall: 0.7275862068965517 s_f1: 0.7056996416828881\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 10.0/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.5765504439671835 | Iter 1.000: -0.6167731881141663<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 accelerator_type:P100, 0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.5765504439671835 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>172.28.0.2:757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         168.929</td><td style=\"text-align: right;\">0.603703</td><td style=\"text-align: right;\">   0.25</td><td style=\"text-align: right;\">    0.57655</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:47:20,749\tWARNING util.py:164 -- The `process_trial_save` operation took 3.610 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=757)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_874d9_00000:\n","  date: 2021-08-11_06-48-37\n","  done: false\n","  epoch: 0.5\n","  eval_f1: 0.711622502002011\n","  eval_loss: 0.5611875454584757\n","  eval_precision: 0.7252861507152862\n","  eval_recall: 0.7310344827586207\n","  experiment_id: 5548763094fa48b0b0f9cbcb377b9523\n","  hostname: 3d258191eee7\n","  iterations_since_restore: 3\n","  loss: 0.552470326423645\n","  node_ip: 172.28.0.2\n","  pid: 757\n","  should_checkpoint: true\n","  time_since_restore: 249.4257116317749\n","  time_this_iter_s: 80.49667286872864\n","  time_total_s: 249.4257116317749\n","  timestamp: 1628664517\n","  timesteps_since_restore: 0\n","  training_iteration: 3\n","  trial_id: 874d9_00000\n","  \n","\u001b[2m\u001b[36m(pid=757)\u001b[0m s_precision: 0.7252861507152862 s_recall: 0.7310344827586207 s_f1: 0.711622502002011\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 10.6/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.5765504439671835 | Iter 1.000: -0.6167731881141663<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.5611875454584757 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>172.28.0.2:757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         249.426</td><td style=\"text-align: right;\">0.55247</td><td style=\"text-align: right;\">    0.5</td><td style=\"text-align: right;\">   0.561188</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:48:42,053\tWARNING util.py:164 -- The `process_trial_save` operation took 4.392 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[2m\u001b[36m(pid=757)\u001b[0m saving in checkpoint dir\n","Result for DEFAULT_874d9_00000:\u001b[2m\u001b[36m(pid=757)\u001b[0m s_precision: 0.7577271236753995 s_recall: 0.7620689655172413 s_f1: 0.7511382341381775\n","  date: 2021-08-11_06-49-39\n","  done: false\n","  epoch: 0.75\n","  eval_f1: 0.7511382341381775\n","  eval_loss: 0.5793119072914124\n","  eval_precision: 0.7577271236753995\n","  eval_recall: 0.7620689655172413\n","  experiment_id: 5548763094fa48b0b0f9cbcb377b9523\n","  hostname: 3d258191eee7\n","  iterations_since_restore: 4\n","  loss: 0.6669412851333618\n","  node_ip: 172.28.0.2\n","  pid: 757\n","  should_checkpoint: true\n","  time_since_restore: 310.88571524620056\n","  time_this_iter_s: 61.46000361442566\n","  time_total_s: 310.88571524620056\n","  timestamp: 1628664579\n","  timesteps_since_restore: 0\n","  training_iteration: 4\n","  trial_id: 874d9_00000\n","  \n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 10.6/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.5793119072914124 | Iter 2.000: -0.5765504439671835 | Iter 1.000: -0.6167731881141663<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.5793119072914124 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>172.28.0.2:757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         310.886</td><td style=\"text-align: right;\">0.666941</td><td style=\"text-align: right;\">   0.75</td><td style=\"text-align: right;\">   0.579312</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:49:45,282\tWARNING util.py:164 -- The `process_trial_save` operation took 6.196 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 10.7/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.5793119072914124 | Iter 2.000: -0.5765504439671835 | Iter 1.000: -0.6167731881141663<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 accelerator_type:P100, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.5793119072914124 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>172.28.0.2:757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         310.886</td><td style=\"text-align: right;\">0.666941</td><td style=\"text-align: right;\">   0.75</td><td style=\"text-align: right;\">   0.579312</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:49:55,344\tWARNING util.py:164 -- The `process_trial_result` operation took 10.026 s, which may be a performance bottleneck.\n","2021-08-11 06:49:55,347\tWARNING util.py:164 -- Processing trial results took 10.030 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n","2021-08-11 06:49:55,350\tWARNING util.py:164 -- The `process_trial` operation took 10.034 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["Result for DEFAULT_874d9_00000:\n","  date: 2021-08-11_06-49-39\n","  done: true\n","  epoch: 0.75\n","  eval_f1: 0.7511382341381775\n","  eval_loss: 0.5793119072914124\n","  eval_precision: 0.7577271236753995\n","  eval_recall: 0.7620689655172413\n","  experiment_id: 5548763094fa48b0b0f9cbcb377b9523\n","  experiment_tag: 0_batch_size=128,do_rate=0.2602,lr=0.0019288,num_epochs=1,num_units=140,std_dims=218\n","  hostname: 3d258191eee7\n","  iterations_since_restore: 4\n","  loss: 0.6669412851333618\n","  node_ip: 172.28.0.2\n","  pid: 757\n","  should_checkpoint: true\n","  time_since_restore: 310.88571524620056\n","  time_this_iter_s: 61.46000361442566\n","  time_total_s: 310.88571524620056\n","  timestamp: 1628664579\n","  timesteps_since_restore: 0\n","  training_iteration: 4\n","  trial_id: 874d9_00000\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 10.1/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.5793119072914124 | Iter 2.000: -0.5765504439671835 | Iter 1.000: -0.6167731881141663<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 accelerator_type:P100, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.5793119072914124 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>RUNNING </td><td>172.28.0.2:757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         310.886</td><td style=\"text-align: right;\">0.666941</td><td style=\"text-align: right;\">   0.75</td><td style=\"text-align: right;\">   0.579312</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:49:59,449\tWARNING util.py:164 -- The `process_trial_save` operation took 3.918 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 10.1/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: -0.5793119072914124 | Iter 2.000: -0.5765504439671835 | Iter 1.000: -0.6167731881141663<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 CPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 accelerator_type:P100, 0.0/1.0 GPU_group_0_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 CPU_group_0ae284d4e0599a9443e979e464a29293, 0.0/1.0 GPU_group_0ae284d4e0599a9443e979e464a29293)<br>Current best trial: 874d9_00000 with eval_loss=0.5793119072914124 and parameters={'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}<br>Result logdir: /root/ray_results/DEFAULT_2021-08-11_06-44-06<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  do_rate</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  eval_loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_874d9_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.260198</td><td style=\"text-align: right;\">0.00192878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">        140</td><td style=\"text-align: right;\">       218</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         310.886</td><td style=\"text-align: right;\">0.666941</td><td style=\"text-align: right;\">   0.75</td><td style=\"text-align: right;\">   0.579312</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-08-11 06:49:59,673\tINFO tune.py:550 -- Total run time: 354.79 seconds (351.12 seconds for the tuning loop).\n"],"name":"stderr"},{"output_type":"stream","text":["Best trial config: {'lr': 0.0019287766765341365, 'batch_size': 128, 'num_epochs': 1, 'std_dims': 218, 'num_units': 140, 'do_rate': 0.26019837129864337}\n","Best trial final validation loss: 0.5793119072914124\n","Best trial final validation weighted f1: 0.7511382341381775\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-53302c092ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-eed42739c4fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_collate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m311595\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"]}]},{"cell_type":"markdown","metadata":{"id":"ququ0pe__LHo"},"source":["Learning the genre vectors from Wg, try to understand if some genres are near each other are not using some distance metric (euclidean or manhattan). Can also do PCA."]},{"cell_type":"code","metadata":{"id":"rdLIKbL_jmpi"},"source":["from sklearn.metrics import precision_recall_fscore_support "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7poTQSxv30G"},"source":["s_f1"],"execution_count":null,"outputs":[]}]}