{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Goodreads_stage2.ipynb","provenance":[],"collapsed_sections":["_6gF1bvclYVR","p95ei-lSdr9i","idsWCeYad34q","YgoOFdif7bbj","5NgXCJLhiz_4","Ydr19fUDjo9i","z8195Rq9rb6I","Xh-WGSe1doKZ"],"mount_file_id":"1wBVNh_5-j0lep-7H-6-hnXYKB4cztf3_","authorship_tag":"ABX9TyPwkNkIUQHwDen08k97NPYH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BjfCjM2xh9dE"},"source":["In this chapter we used already fine-tuned BERT models to extract the chunk embeddings of our books. The chunk embeddings correspond to either the meaned embeddings of all the words in the sequence or the embedding of the [CLS] token. We will explore the results of both. We will then run a variety of classifiers over these embeddings directly. \n","\n","1.   Meaned pooled output --> single layer NN\n","2.   RoBERT\n","3.   ToBERT ?"]},{"cell_type":"markdown","metadata":{"id":"XLXM0vhe5MQ8"},"source":["# Installs, Imports, Configuration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttrgrV6NN3nB","executionInfo":{"status":"ok","timestamp":1628568561517,"user_tz":420,"elapsed":34176,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"9c11718e-106d-4d64-d713-1bfda7ee2356"},"source":["!pip install datasets\n","!pip install \"ray[default]\"\n","!pip install wandb\n","!pip install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting tqdm>=4.42\n","  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 15.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 15.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: tqdm, xxhash, huggingface-hub, fsspec, datasets\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","Successfully installed datasets-1.11.0 fsspec-2021.7.0 huggingface-hub-0.0.15 tqdm-4.62.0 xxhash-2.0.2\n","Collecting ray[default]\n","  Downloading ray-1.5.1-cp37-cp37m-manylinux2014_x86_64.whl (51.5 MB)\n","\u001b[K     |████████████████████████████████| 51.5 MB 40 kB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.17.3)\n","Collecting py-spy>=0.2.0\n","  Downloading py_spy-0.3.8-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 49.8 MB/s \n","\u001b[?25hCollecting opencensus\n","  Downloading opencensus-0.7.13-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 46.3 MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.23.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.6.0)\n","Collecting redis>=3.5.0\n","  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 529 kB/s \n","\u001b[?25hCollecting aioredis<2\n","  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.34.1)\n","Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (0.11.0)\n","Collecting aiohttp-cors\n","  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n","Collecting pydantic>=1.8\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 29.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.0.12)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 52.6 MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting gpustat\n","  Downloading gpustat-0.6.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.13)\n","Collecting colorful\n","  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 40.2 MB/s \n","\u001b[?25hCollecting hiredis\n","  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 4.8 MB/s \n","\u001b[?25hCollecting async-timeout\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[default]) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8->ray[default]) (3.7.4.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 62.9 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]) (3.0.4)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[default]) (2.10)\n","Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]) (7.352.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]) (5.4.8)\n","Collecting blessings>=1.6\n","  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n","Collecting opencensus-context==0.1.2\n","  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n","Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]) (1.26.3)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (57.2.0)\n","Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (1.32.1)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (1.53.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (21.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (2018.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (4.2.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (1.24.3)\n","Building wheels for collected packages: gpustat\n","  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=8978e12e0acf1309ddfcfdbeb8ace1e4e3ff787fdb8b35cf23176ae4b9764b03\n","  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n","Successfully built gpustat\n","Installing collected packages: multidict, yarl, async-timeout, opencensus-context, hiredis, blessings, aiohttp, redis, pydantic, py-spy, opencensus, gpustat, colorama, aioredis, aiohttp-cors, ray, colorful\n","Successfully installed aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 gpustat-0.6.0 hiredis-2.0.0 multidict-5.1.0 opencensus-0.7.13 opencensus-context-0.1.2 py-spy-0.3.8 pydantic-1.8.2 ray-1.5.1 redis-3.5.3 yarl-1.6.3\n","Collecting wandb\n","  Downloading wandb-0.11.2-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting urllib3>=1.26.5\n","  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 56.5 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 63.4 MB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting configparser>=3.8.1\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n","\u001b[K     |████████████████████████████████| 170 kB 63.9 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n","Collecting smmap<5,>=3.0.1\n","  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n","Collecting requests<3,>=2.0.0\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 968 kB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=63b402f1684bf755041cdeef0397097941d8b64d2a4587d0c7031a29d8063c66\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=dd3ea22bb784a6915a460483400a392458977279418f8c6beffa0a576e7155d5\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, urllib3, gitdb, subprocess32, shortuuid, sentry-sdk, requests, pathtools, GitPython, docker-pycreds, configparser, wandb\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 requests-2.26.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 urllib3-1.26.6 wandb-0.11.2\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7vwrYLDGtF-","executionInfo":{"status":"ok","timestamp":1628568561518,"user_tz":420,"elapsed":14,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"c64c26e7-4720-4bb4-c484-552e65690023"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QiCafSeXFWzf"},"source":["from pathlib import Path\n","import sys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA5cgP2i5PB9"},"source":["import configparser\n","\n","config = configparser.ConfigParser()\n","config.read('/content/drive/MyDrive/Thesis/BookSuccessPredictor/config.ini')\n","\n","drive_base_path = Path(config['Drive']['drive_base_path'])\n","\n","# sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / '_utils'))\n","sys.path.append(str(drive_base_path / 'BookSuccessPredictor' / 'datasets' / 'goodreads_maharjan_super' / 'MultiModal' / 'dataset_loader'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6gF1bvclYVR"},"source":["# Get Transformer Model from Stage 1"]},{"cell_type":"code","metadata":{"id":"Sot-nVzelfLN"},"source":["import wandb\n","run = wandb.init()\n","\n","model_name = 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","if config['Model']['name'] == 'distilbert-base-uncased':\n","  if (config['Tokenizer']['overlap']):\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-nlpbosie:v0', type='model')\n","  else:\n","    artifact = run.use_artifact('lucaguarro/goodreads_success_predictor/model-2giwtwvy:v0', type='model')\n","    \n","artifact_dir = artifact.download()\n","\n","transformer_model = DistilBERTForMultipleSequenceClassification.from_pretrained(artifact_dir, num_labels1 = 2, num_labels2 = 8)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHUvF9GXrF_0"},"source":["# Getting the Data"]},{"cell_type":"markdown","metadata":{"id":"p95ei-lSdr9i"},"source":["## Getting the Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"idsWCeYad34q"},"source":["### Creating the Dataset"]},{"cell_type":"markdown","metadata":{"id":"YgoOFdif7bbj"},"source":["#### From script"]},{"cell_type":"code","metadata":{"id":"IjBdxGKsFyZO"},"source":["import torch as th\n","import time\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def getPooledOutputs(model, encoded_dataset, batch_size = 32):\n","  model.eval()\n","\n","  # pooled_outputs = []\n","  pooled_outputs = torch.empty([0,768]).cuda()\n","\n","  num_iters = (len(encoded_dataset['input_ids']) - 1)//batch_size + 1\n","  print(\"total number of iters \", num_iters)\n","  \n","  for i in range(num_iters):\n","    print(i)\n","    up_to = i*batch_size + batch_size\n","    if len(encoded_dataset['input_ids']) < up_to:\n","      up_to = len(encoded_dataset['input_ids'])\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to]).cuda()\n","    attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to]).cuda()\n","\n","    with torch.no_grad():\n","      embeddings = model.forward(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)['hidden_states'][-1][:,0] # Pooled output\n","      pooled_outputs = th.cat([pooled_outputs, embeddings],0)\n","      th.cuda.empty_cache()\n","\n","  return pooled_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"AEVzj6kH8q14","executionInfo":{"status":"error","timestamp":1627520230029,"user_tz":420,"elapsed":271,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"639e4e67-6e42-4430-b11c-c3ea61bcab42"},"source":["chunked_encoded_dataset"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-dc826b7d6e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunked_encoded_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'chunked_encoded_dataset' is not defined"]}]},{"cell_type":"code","metadata":{"id":"f-AigWJRlMyx"},"source":["train_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjtsYx1YnBL3"},"source":["val_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xYnBmEAnHwp"},"source":["test_set_embeddings = getPooledOutputs(transformer_model, chunked_encoded_dataset['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyWQ8O-eEc1D"},"source":["from datasets import Dataset\n","train_set_embeddings = Dataset.from_dict({'pooled_outputs': train_set_embeddings})\n","val_set_embeddings = Dataset.from_dict({'pooled_outputs': val_set_embeddings})\n","test_set_embeddings = Dataset.from_dict({'pooled_outputs': test_set_embeddings})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8agtlOpzEigf"},"source":["from datasets import concatenate_datasets\n","dataset_w_embeddings = DatasetDict({\n","    'train': concatenate_datasets([chunked_encoded_dataset['train'], train_set_embeddings], axis = 1), \n","    'validation': concatenate_datasets([chunked_encoded_dataset['validation'], val_set_embeddings], axis = 1), \n","    'test': concatenate_datasets([chunked_encoded_dataset['test'], test_set_embeddings], axis = 1)\n","})\n","dataset_w_embeddings = dataset_w_embeddings.remove_columns(['attention_mask', 'input_ids', 'token_type_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kK_Uf0Au3RFd"},"source":["dataset_w_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VhVV2CdEo56"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('train_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['train'], output_file)\n","\n","with open('val_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['validation'], output_file)\n","\n","with open('test_ds.pkl', 'wb') as output_file:\n","  pickle.dump(dataset_w_embeddings['test'], output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('train_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('val_ds.pkl')\n","file.Upload() \n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('test_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKxTgPtQ7gn5"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"C2e0wfqflvz_"},"source":["base_path = Path(\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MP8MbdD07vOm"},"source":["from datasets import DatasetDict\n","\n","if model_name == 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings':\n","  full_directory = base_path / 'DistilBERT_multitask_sentence_tokenized_dataset_embeddings'\n","elif model_name == 'DistilBERT_multitask_overlap50_dataset_embeddings':\n","  full_directory = base_path / 'DistilBERT_multitask_overlap50_dataset_embeddings'\n","\n","with open(full_directory / 'train_ds.pkl', \"rb\") as input_file:\n","  train_set_embeddings = pickle.load(input_file)\n","\n","with open(full_directory / 'val_ds.pkl', \"rb\") as input_file:\n","  val_set_embeddings = pickle.load(input_file)\n","\n","with open(full_directory / 'test_ds.pkl', \"rb\") as input_file:\n","  test_set_embeddings = pickle.load(input_file)\n","\n","dataset_w_embeddings = DatasetDict({'train': train_set_embeddings, 'validation': val_set_embeddings, 'test': test_set_embeddings})\n","dataset_w_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5NgXCJLhiz_4"},"source":["## Average Pooled Outputs for Shallow Neural Network and SVM"]},{"cell_type":"markdown","metadata":{"id":"TK4ZZFsWjZAU"},"source":["### Generating the Data from Pooled Outputs"]},{"cell_type":"markdown","metadata":{"id":"BQXU-G-cjEkk"},"source":["#### From Script"]},{"cell_type":"code","metadata":{"id":"G_TnU36ei2eh"},"source":["def getAveragePooledOutputs(model, encoded_dataset):\n","  book_embeddings_dataset = {'meaned_pooled_output': [], 'book_title': [], 'genre': [], 'labels': []}\n","\n","  book_changes = get_book_changes_idx(encoded_dataset['book_title'])\n","\n","  for i in range(len(book_changes)):\n","    print(i)\n","    start = book_changes[i]\n","    end = None\n","    if i != len(book_changes) - 1:\n","      end = book_changes[i+1]\n","    else:\n","      end = len(encoded_dataset['input_ids'])\n","\n","    input_ids = th.LongTensor(encoded_dataset['input_ids'][start:end])\n","    attention_mask = th.BoolTensor(encoded_dataset['attention_mask'][start:end])\n","\n","    with torch.no_grad():\n","      embeddings = transformer_model.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0][:,0] # Pooled output\n","      book_embeddings = th.mean(embeddings, dim=0) # Takes the mean of the pooled output\n","    book_embeddings_dataset['meaned_pooled_output'].append(book_embeddings)\n","    book_embeddings_dataset['book_title'].append(encoded_dataset['book_title'][start])\n","    book_embeddings_dataset['genre'].append(encoded_dataset['genre'][start])\n","    book_embeddings_dataset['labels'].append(encoded_dataset['labels'][start])\n","  \n","  return book_embeddings_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"161S_UpRi5Xq"},"source":["avg_pld_outs_ds = getAveragePooledOutputs(dataset_w_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQXjx63ljRis"},"source":["from datasets import DatasetDict, Dataset\n","avg_pld_outs_hf_ds = DatasetDict({'train': Dataset.from_dict(avg_pld_outs_ds['train']), 'validation': Dataset.from_dict(avg_pld_outs_ds['validation']), 'test': Dataset.from_dict(avg_pld_outs_ds['test'])})\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)  \n","\n","with open('avg_pld_outs_hf_ds.pkl', 'wb') as output_file:\n","  pickle.dump(avg_pld_outs_hf_ds, output_file)\n","\n","folder_id = '176pNJFvgTaclx_dKNGgocd-TvwmqxM7Y'\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile('avg_pld_outs_hf_ds.pkl')\n","file.Upload() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lavLmaCjjGfG"},"source":["#### Load from Drive"]},{"cell_type":"code","metadata":{"id":"MHOd6dPpk-X-"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRmZRxEGjWJT"},"source":["from datasets import DatasetDict\n","with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","  avg_pld_outs_hf_ds = pickle.load(input_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ydr19fUDjo9i"},"source":["# Simple Shallow Neural Network"]},{"cell_type":"code","metadata":{"id":"Aw4ZV-yjjvYe"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","class Net(nn.Module):\n","\n","    def __init__(self, pre_classifier_init, classifier_init):\n","        super(Net, self).__init__()\n","\n","        self.pre_classifier = nn.Linear(768, 768)\n","        self.classifier = nn.Linear(768, 2)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.pre_classifier.weight.data.copy_(pre_classifier_init.weight.data)\n","        self.classifier.weight.data.copy_(classifier_init.weight.data)\n","\n","        # print(pre_classifier_init.bias.data)\n","        self.pre_classifier.bias.data.copy_(pre_classifier_init.bias.data)\n","        self.classifier.bias.data.copy_(classifier_init.bias.data)\n","\n","        # DOUBLE CHECK IF BIASES ARE BEING SET AS WELL\n","\n","    def forward(self, x, labels = None):\n","        # Max pooling over a (2, 2) window\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        logits = self.classifier(x)\n","\n","        loss = None\n","        if labels is not None:\n","          loss_fct = CrossEntropyLoss()\n","          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","        return SequenceClassifierOutput(\n","            loss = loss,\n","            logits = logits\n","        )\n","\n","net = Net(model.pre_classifier, model.classifier1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3kxvLBJj70p"},"source":["#### Results with no Training"]},{"cell_type":"code","metadata":{"id":"2nhYqvBgkAkH"},"source":["net.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecoQ7GOOj_ml"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['validation']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['validation']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6v9W_FydkEOR"},"source":["with torch.no_grad():\n","  logits = net.forward(torch.FloatTensor(avg_pld_outs_hf_ds['test']['meaned_pooled_output']))\n","y_score = softmax(logits['logits'], axis = 1)[:, 1].tolist()\n","y_pred = [math.floor(input) if input < 0.50 else math.ceil(input) for input in y_score]\n","f1_score(avg_pld_outs_hf_ds['test']['success_label'], y_pred, average = 'weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvjGT-vJkO2x"},"source":["#### Training w Hyperparameter Tuning and Results"]},{"cell_type":"code","metadata":{"id":"DSHANKmHkSzp"},"source":["def load_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  trainset = avg_pld_outs_hf_ds['train']\n","  valset = avg_pld_outs_hf_ds['validation']\n","  return trainset, valset\n","\n","def load_test_data():\n","  with open(r\"/content/drive/MyDrive/Thesis/Datasets/goodreads_maharjan_super/Pooled_Output/DistilBERT_multitask_overlap50_dataset_embeddings/avg_pld_outs_hf_ds.pkl\", \"rb\") as input_file:\n","    avg_pld_outs_hf_ds = pickle.load(input_file)\n","  avg_pld_outs_hf_ds.set_format(type='pt', columns=['meaned_pooled_output', 'success_label'])\n","  testset = avg_pld_outs_hf_ds['test']\n","  return testset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kM4cHtBkqaf"},"source":["from ray import tune\n","# from ray.tune.integration.wandb import wandb_mixin\n","# '''@wandb_mixin\n","# run = wandb.init()\n","\n","def train_nn(config, checkpoint_dir, data_dir=None):\n","  net = Net(model.pre_classifier, model.classifier1, config['do_rate'])\n","  net.train()\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      if torch.cuda.device_count() > 1:\n","          net = nn.DataParallel(net)\n","  print(type(net))\n","  net.to(device)\n","  # net.cuda()\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], shuffle=True)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","    for i, data in enumerate(trainloader, 0):\n","\n","      inputs = data['meaned_pooled_output']\n","      labels = data['success_label']\n","\n","      inputs, labels = inputs.to(device), labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = net(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if i % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      net.eval()\n","      with torch.no_grad():\n","        for i, data in enumerate(valloader, 0):\n","\n","          inputs_cpu = data['meaned_pooled_output']\n","          labels_cpu = data['success_label']\n","\n","          inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","          # inputs.cuda()\n","          # labels.cuda()\n","\n","          outputs = net(inputs)\n","          _, predicted = torch.max(outputs.data, 1)\n","\n","          all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","          all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","          loss = criterion(outputs, labels)\n","          val_loss += loss.cpu().numpy()\n","          val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((net.state_dict(), optimizer.state_dict()), path)\n","\n","      net.train()\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      # s_acc = accuracy_score(all_labels, all_predictions)\n","      # wandb.log({\"val_loss\": val_loss / val_steps, \"val_accuracy\": correct / total})\n","      tune.report(loss=(val_loss / val_steps), accuracy=correct / total, f1=s_f1, precision=s_precision, recall=s_recall)\n","  print(\"Finished Training\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBaWnJvfkwCR"},"source":["def test_results(net, device=\"cpu\"):\n","    testset = load_test_data()\n","\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n","\n","    all_predictions = np.array([])\n","    all_labels = np.array([])\n","\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader, 0):\n","            inputs_cpu = data['meaned_pooled_output']\n","            labels_cpu = data['success_label']\n","\n","            inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n","            outputs = net(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions = np.append(all_predictions, predicted.to('cpu').numpy())\n","            all_labels = np.append(all_labels, labels_cpu.numpy())\n","\n","    s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","    return {\n","        'precision': s_precision,\n","        'recall': s_recall,\n","        'f1': s_f1\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aom_a9B_kUL7"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.tune.integration.wandb import WandbLoggerCallback\n","import torch.optim as optim\n","from functools import partial\n","\n","def main(num_samples = 15, max_num_epochs = 10):\n","  config = {\n","      \"lr\": tune.loguniform(1e-4, 1e-1),\n","      \"batch_size\": tune.choice([16,32,64,128]),\n","      \"num_epochs\": tune.choice([1,2]),#,2,3]),#,2,3,5,10,20]),\n","      \"do_rate\": tune.uniform(0.1, 0.5),\n","      \"wandb\": {\n","        \"project\": \"AvgPooledOutputClassifier\",\n","        \"api_key\": config['WandB']['api_key']\n","      }\n","    }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(train_nn, checkpoint_dir='/tmp/ShallowNNModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"AvgPooledOutputClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"accuracy\"]))\n","  \n","  best_trained_model = Net(model.pre_classifier, model.classifier1, best_trial.config['do_rate'])\n","  device = \"cpu\"\n","  if torch.cuda.is_available():\n","      device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","\n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/ShallowNNModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANZwhwsqk4C9"},"source":["main(num_samples=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8195Rq9rb6I"},"source":["# SVM"]},{"cell_type":"code","metadata":{"id":"Psd_tsr7rwkw"},"source":["from sklearn import svm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RWgJonqrzzi"},"source":["cs = np.arange(0.9, 1.5, 0.02).tolist()\n","best_clf = None\n","best_score = 0\n","best_c = None\n","for c in cs:\n","  clf = svm.SVC(kernel='rbf', gamma='scale', C=c)\n","  clf.fit(avg_pld_outs_hf_ds['train']['meaned_pooled_output'], avg_pld_outs_hf_ds['train']['success_label'])\n","  predictions = clf.predict(avg_pld_outs_hf_ds['validation']['meaned_pooled_output'])\n","  (_, pred_counts) = np.unique(predictions, return_counts=True)\n","  val_score = f1_score(avg_pld_outs_hf_ds['validation']['success_label'], predictions, average = 'weighted')\n","  print('Clf with C = {} obtained val-score of {}'.format(c, val_score))\n","  if (val_score > best_score):\n","    best_score = val_score\n","    best_clf = clf\n","    best_c = c\n","\n","print('\\nBest C: {}; Val-score: {}'.format(best_c, best_score))\n","test_predictions = best_clf.predict(avg_pld_outs_hf_ds['test']['meaned_pooled_output'])\n","test_score = f1_score(avg_pld_outs_hf_ds['test']['success_label'], test_predictions, average = 'weighted')\n","print('Yields score of {} on test set'.format(test_score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xh-WGSe1doKZ"},"source":["# RoBERT"]},{"cell_type":"code","metadata":{"id":"XaGB0mqPdTCr"},"source":["dataset_w_embeddings.set_format('pytorch', columns=['pooled_outputs', 'success_label', 'genre'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNINcDJopd1-"},"source":["import numpy as np\n","from datasets import DatasetDict, Dataset\n","\n","def get_book_changes_idx(book_titles):\n","  book_changes_idx = np.where(np.array(book_titles[:-1]) != np.array(book_titles[1:]))[0]\n","  book_changes_idx += 1\n","  # book_changes_idx = np.append(book_changes_idx, len(book_titles))\n","  # book_changes_idx = np.insert(book_changes_idx, 0, 0)\n","  return book_changes_idx\n","\n","def convert_to_LSTM_dataset_full(dataset):\n","  full_ds = {}\n","  full_ds['train'] = convert_to_LSTM_dataset_sub(dataset['train'])\n","  full_ds['validation'] = convert_to_LSTM_dataset_sub(dataset['validation'])\n","  full_ds['test'] = convert_to_LSTM_dataset_sub(dataset['test'])\n","\n","  full_ds = DatasetDict({'train': Dataset.from_dict(full_ds['train']), 'validation': Dataset.from_dict(full_ds['validation']), 'test': Dataset.from_dict(full_ds['test'])})\n","  # full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  return full_ds\n","\n","def convert_to_LSTM_dataset_sub(dataset):\n","  ds = {'grouped_pooled_outs': None, 'success_label': None, 'genre': None}\n","\n","  book_start_idx = get_book_changes_idx(dataset['book_title'])\n","  book_start_idx_w_end = np.append(book_start_idx, len(dataset['book_title']))\n","  book_start_idx_w_zero = np.insert(book_start_idx, 0, 0)\n","\n","  book_lengths = book_start_idx_w_end - np.concatenate((np.array([0]), np.roll(book_start_idx_w_end, 1)[1:]))\n","  # print(type(dataset['pooled_outputs']))\n","  book_grouped_embeddings = dataset['pooled_outputs'].split_with_sizes(list(book_lengths))\n","  # book_grouped_embeddings = torch.stack(dataset['pooled_outputs'].split_with_sizes(list(book_lengths)), dim=0)\n","\n","  # print(type(book_grouped_embeddings))\n","  ds['grouped_pooled_outs'] = book_grouped_embeddings\n","  ds['success_label'] = np.take(dataset['success_label'], book_start_idx_w_zero)\n","  ds['genre'] = np.take(dataset['genre'], book_start_idx_w_zero)\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_slJ4X3Bpf_Q"},"source":["class RoBERT_Model(nn.Module):\n","\n","    def __init__(self, layer_size = 100):\n","        self.layer_size = layer_size\n","        super(RoBERT_Model, self).__init__()\n","        self.lstm = nn.LSTM(768, layer_size, num_layers=1, bidirectional=False)\n","        self.out = nn.Linear(layer_size, 2)\n","\n","    def forward(self, grouped_pooled_outs):\n","        \"\"\" Define how to performed each call\n","        Parameters\n","        __________\n","        pooled_output: array\n","            -\n","        lengt: int\n","            -\n","        Returns:\n","        _______\n","        -\n","        \"\"\"\n","        # chunks_emb = pooled_out.split_with_sizes(lengt) # splits the input tensor into a list of tensors where the length of each sublist is determined by lengt\n","\n","        seq_lengths = torch.LongTensor([x for x in map(len, grouped_pooled_outs)]) # gets the length of each sublist in chunks_emb and returns it as an array\n","\n","        batch_emb_pad = nn.utils.rnn.pad_sequence(grouped_pooled_outs, padding_value=-91, batch_first=True) # pads each sublist in chunks_emb to the largest sublist with value -91\n","        batch_emb = batch_emb_pad.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n","        lstm_input = nn.utils.rnn.pack_padded_sequence(batch_emb, seq_lengths, batch_first=False, enforce_sorted=False) # seq_lengths.cpu().numpy()\n","\n","        packed_output, (h_t, h_c) = self.lstm(lstm_input, )  # (h_t, h_c))\n","#         output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=-91)\n","\n","        h_t = h_t.view(-1, self.layer_size) # (-1, 100)\n","\n","        return self.out(h_t) # logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEPdCiU6phS2"},"source":["def my_collate1(batches):\n","  # for batch in batches:\n","  #   print(type(batch['grouped_pooled_outs']), len(batch['grouped_pooled_outs']))\n","  #   print(type(torch.FloatTensor(batch['grouped_pooled_outs'])))\n","    return {\n","        'grouped_pooled_outs': [torch.stack(x['grouped_pooled_outs']) for x in batches],\n","        'success_label': torch.LongTensor([x['success_label'] for x in batches])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTNSTUQkpnQp"},"source":["from transformers import AdamW\n","import time\n","\n","def load_test_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  testset = full_ds['test']\n","  return testset\n","\n","def load_data():\n","  full_ds = convert_to_LSTM_dataset_full(dataset_w_embeddings)\n","  full_ds.set_format(type='torch', columns = ['grouped_pooled_outs', 'success_label', 'genre'])\n","  trainset = full_ds['train']\n","  valset = full_ds['validation']\n","  return trainset, valset\n","\n","# def loss_fun(outputs, targets):\n","#     loss = nn.CrossEntropyLoss()\n","#     return loss(outputs, targets)\n","\n","def rnn_train_fun1(config, checkpoint_dir='/tmp/LSTMModels'):\n","  model = RoBERT_Model(config[\"layer_size\"])\n","  model.train()\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #   device = \"cuda:0\"\n","  #   if torch.cuda.device_count() > 1:\n","  #       model = nn.DataParallel(model)\n","  # # print(type(model))\n","  # model.to(device)\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainset, valset = load_data()\n","\n","  trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","  valloader = torch.utils.data.DataLoader(valset, batch_size=config[\"batch_size\"], collate_fn=my_collate1)\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(trainloader):\n","      grouped_pooled_outs = batch['grouped_pooled_outs'] # .to(device)\n","      targets = batch['success_label'] #.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(grouped_pooled_outs)\n","      loss = loss_fun(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if batch_idx % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      with torch.no_grad():\n","          for i, data in enumerate(valloader, 0):\n","\n","              grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","              targets = data['success_label'] # .to(device)\n","\n","              outputs = model(grouped_pooled_outs)\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.numpy())\n","              all_labels = np.append(all_labels, targets.numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6beZxWFXpsXW"},"source":["def test_results(net, device=\"cpu\"):\n","  testset = load_test_data()\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=8, collate_fn=my_collate1)\n","\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.eval()\n","  with torch.no_grad():\n","    for i, data in enumerate(testloader, 0):\n","        grouped_pooled_outs = data['grouped_pooled_outs'] # .to(device)\n","        targets = data['success_label'] # .to(device)\n","\n","        outputs = net(grouped_pooled_outs)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxp-MTSnpvZN"},"source":["from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","  config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([16,32,64]),\n","    \"num_epochs\": tune.choice([1,2,3,5]),\n","    \"layer_size\": tune.choice([100]),\n","    \"wandb\": {\n","      \"project\": \"LSTMClassifier\",\n","      \"api_key\": config['WandB']['api_key'],\n","      \"log_config\": True\n","    }\n","  }\n","\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  result = tune.run(\n","    partial(rnn_train_fun1, checkpoint_dir='/tmp/LSTMModels'),\n","    config = config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"LSTMClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","\n","  \n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = RoBERT_Model(best_trial.config['layer_size'])\n","  device = \"cpu\"\n","  # if torch.cuda.is_available():\n","  #     device = \"cuda:0\"\n","      # if gpus_per_trial > 1:\n","      #     best_trained_model = nn.DataParallel(best_trained_model)\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/Models/LSTMModels/yungclassifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETTb9Z0vpy00"},"source":["test_results = main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kw2weKz-lg6"},"source":["# MultiModal"]},{"cell_type":"markdown","metadata":{"id":"6c612HylViH5"},"source":["### Defining the Modal"]},{"cell_type":"code","metadata":{"id":"ZRbTSOAuasG1"},"source":["import torch.nn as nn\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gVjETdBcQi2"},"source":["Our model will be composed of three separate modules:\n","\n","1. (Normalizer) Responsible for taking all the inputs of various dimensions and feeding them each through their own linear layer to project them into a space with all the same dimensions\n","\n","In essence, it is responsible for eq (1) in the paper $h_i=selu(W_{h_i} x_i + b_h)$\n","\n","\n","2. (GenreAwareAttention) This is where most of the meat of the model is. It is responsible for performing these 3 equations. \n","\n","$score(h_i, g) = v^T selu(W_a h_i + W_g g + b_a)$\n","\n","$\\alpha_i = \\frac{exp(score(h_i,g))}{\\sum_{i'}exp(score(h_{i'},g)}$\n","\n","$r=\\sum_i \\alpha_i h_i$\n","\n","3. (ClassOutput) The last layer is simply responsible for projecting the book representation to class probabilities.\n","\n","$\\hat{p}=\\sigma(W_c r + b_c)$"]},{"cell_type":"code","metadata":{"id":"U0vC_Exg-nTp"},"source":["class Normalizer(nn.Module):\n","  def __init__(self, c5g_size, bf_size, std_dims):\n","    super(Normalizer, self).__init__()\n","\n","    self.c5g_linear = nn.Linear(c5g_size, std_dims)\n","    self.bf_linear = nn.Linear(bf_size, std_dims)\n","\n","  def forward(self, x_c5g, x_bf):\n","    # x_c5g ~ (BATCH_SIZE, C5G_FEATURE_SIZE)\n","    # x_bf ~ (BATCH_SIZE, BF_FEATURE_SIZE)\n","    # # split features into char_5_gram and bert_features\n","    # char_5_grams = None\n","    # bert_features = None\n","\n","    c5g_normed = self.c5g_linear(x_c5g)\n","    bf_normed = self.bf_linear(x_bf)\n","\n","    # concatenate c5g_normed and bf_normed\n","    return torch.stack([c5g_normed, bf_normed], 1) # (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzj5ackHOUQu"},"source":["class GenreAwareAttention(nn.Module):\n","  def __init__(self, std_dims, num_units):\n","    super(GenreAwareAttention, self).__init__()\n","    self.activation = nn.SELU()\n","    self.nn_softmax = nn.Softmax(dim=1)\n","\n","    self.v = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(num_units,1)),\n","        requires_grad=True\n","    )\n","\n","    self.Wa = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(std_dims,num_units)), \n","        requires_grad=True\n","    )\n","\n","    self.b = nn.parameter.Parameter(\n","        nn.init.ones_(torch.empty(num_units,)),\n","        requires_grad=True\n","    )\n","\n","    self.Wg = nn.parameter.Parameter(\n","        nn.init.xavier_uniform_(torch.empty(8, num_units)), \n","        requires_grad=True\n","    )\n","\n","  def forward(self, x, g):\n","    # x ~ (BATCH_SIZE, NUM_MODALITIES, EMBED_SIZE)\n","    # g ~ (BATCH_SIZE, 1, GENRE_EMBED_SIZE)\n","    \n","    # calculate scores\n","    atten_g = torch.mm(g, self.Wg).unsqueeze(dim=1)\n","    et = self.activation(torch.matmul(x, self.Wa) + atten_g + self.b)\n","    et = torch.matmul(et, self.v)\n","\n","    at = self.nn_softmax(et)\n","\n","    # at = torch.unsqueeze(at, axis=-1)\n","\n","    # print('at:', at.size())\n","    # print('x:', x.size())\n","    ot = at * x # canot multiply at: torch.Size([4, 2, 1, 1]) x: torch.Size([4, 2, 100])\n","\n","    return torch.sum(ot, axis=1) # BATCH_SIZE, EMBED_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxcCXEr4iPWB"},"source":["class ClassifierOut(nn.Module):\n","  def __init__(self, std_dims):\n","    super(ClassifierOut, self).__init__()\n","    self.classifier = nn.Linear(std_dims, 2)\n","  \n","  def forward(self, r): # r ~ BATCH_SIZE, EMBED_SIZE\n","    r_out = self.classifier(r) # BATCH_SIZE, 2\n","    return torch.sigmoid(r_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sb7CIVGIGD-G"},"source":["class FullModel(nn.Module): # may want to consider also adding a dropout layer before classification\n","  def __init__(self, c5g_size, bf_size, std_dims, num_units):\n","    super(FullModel,self).__init__()\n","    self.normalizer = Normalizer(c5g_size, bf_size, std_dims)\n","    self.genre_aware_attention = GenreAwareAttention(std_dims, num_units)\n","    self.classifier_out = ClassifierOut(std_dims)\n","\n","  def forward(self, x_c5g, x_bf, genre):\n","    x_normed = self.normalizer(x_c5g, x_bf)\n","    g_a_a = self.genre_aware_attention(x_normed, genre)\n","    return self.classifier_out(g_a_a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQO49DFsVmUr"},"source":["### Getting the Data"]},{"cell_type":"code","metadata":{"id":"r62qM0beUEER"},"source":["# from MultimodalGoodreadsDataset import MultimodalGoodreadsDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ma0olfGhVnaI"},"source":["dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'\n","\n","# ds = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)\n","\n","def my_collate_fn(batches, f1_len, f2_len):\n","    return {\n","        'c5g_f': torch.tensor([x['text_features'].toarray()[0][0:f1_len] for x in batches]), # dtype = Float?\n","        'bert_f': torch.tensor([x['text_features'].toarray()[0][f1_len:f1_len+f2_len] for x in batches]), \n","        'genre': torch.tensor([x['genre'] for x in batches]),\n","        'label': torch.tensor([x['label'] for x in batches])\n","    }\n","\n","# c5g_len = ds.f_lengths[0]\n","# bf_len = ds.f_lengths[1]\n","\n","# train_dataloader = DataLoader(ds.train, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))\n","# val_dataloader = DataLoader(ds.val, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))\n","# test_dataloader = DataLoader(ds.test, batch_size=64, shuffle=True, collate_fn=partial(my_collate_fn, f1_len=c5g_len, f2_len=bf_len))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eTMCXp0Y8PA"},"source":["from torch.utils.data import DataLoader\n","import torch\n","from functools import partial\n","\n","def my_collate_fn(batches, f1_len, f2_len):\n","    return {\n","        'c5g_f': torch.tensor([x['text_features'].toarray()[0][0:f1_len] for x in batches]), # dtype = Float?\n","        'bert_f': torch.tensor([x['text_features'].toarray()[0][f1_len:f1_len+f2_len] for x in batches]), \n","        'genre': torch.tensor([x['genre'] for x in batches]),\n","        'label': torch.tensor([x['label'] for x in batches])\n","    }\n","\n","def load_data():\n","  dataset_base_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/raw_preprocessed/goodreads_maharjan_trimmed'\n","  cached_features_dir = '/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/cached_features'\n","\n","  ds = MultimodalGoodreadsDataset(dataset_base_dir, cached_features_dir)\n","\n","  return ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZezPhKzsZd6H"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"k-bC2fOxh_7Y"},"source":["# model = FullModel(c5g_len, bf_len, 100, 50).to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p64cWKuFYxdx","executionInfo":{"status":"ok","timestamp":1628566251190,"user_tz":420,"elapsed":209,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"2a8afdee-59d1-4495-c133-993c2168d1cb"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"k6PlXM8LPMGp","executionInfo":{"status":"error","timestamp":1628568342913,"user_tz":420,"elapsed":282,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"a26fca0a-7a77-48af-8826-a155b1d90dd8"},"source":["# from MultimodalGoodreadsDataset import MultimodalGoodreadsDataset"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-aff1ce7788fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMultimodalGoodreadsDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultimodalGoodreadsDataset2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mImportError\u001b[0m: cannot import name 'MultimodalGoodreadsDataset2' from 'MultimodalGoodreadsDataset' (/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader/MultimodalGoodreadsDataset.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"kRTXe1tzWUAz"},"source":["from torch.optim import AdamW\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support \n","\n","def mm_train_fun1(config, checkpoint_dir='/tmp/MultiModalModels'):\n","  print(\"***test4\")\n","  # sys.path.append('/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader')\n","  sys.path.append('/content/drive/MyDrive/Thesis/BookSuccessPredictor/datasets/goodreads_maharjan_super/MultiModal/dataset_loader')\n","  from MultimodalGoodreadsDataset import MultimodalGoodreadsDataset\n","\n","  ds = load_data()\n","  model = FullModel(ds.f_lengths[0], ds.f_lengths[1], config['std_dims'], config['num_units']).to('cuda')\n","  model.train()\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer=AdamW(model.parameters(), lr=config[\"lr\"])\n","\n","  trainloader = DataLoader(ds.train, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=ds.f_lengths[0], f2_len=ds.f_lengths[1]))\n","  valloader = DataLoader(ds.val, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(my_collate_fn, f1_len=ds.f_lengths[0], f2_len=ds.f_lengths[1]))\n","\n","  for epoch in range(config['num_epochs']):\n","    running_loss = 0.0\n","    epoch_steps = 0\n","\n","    for batch_idx, batch in enumerate(train_dataloader):\n","      c5g_f = batch['c5g_f'].to('cuda')\n","      bert_f = batch['bert_f'].to('cuda')\n","      genre = batch['genre'].to('cuda')\n","      targets = batch['label'].to('cuda')\n","\n","      optimizer.zero_grad()\n","      outputs = model(c5g_f.float(), bert_f.float(), genre.float())\n","      loss = criterion(outputs, targets)\n","      loss.backward()\n","      model.float()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      epoch_steps += 1\n","\n","      if batch_idx % 10 == 9:\n","        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n","                                        running_loss / epoch_steps))\n","        running_loss = 0.0\n","\n","      # Validation loss\n","      val_loss = 0.0\n","      val_steps = 0\n","      total = 0\n","      correct = 0\n","\n","      all_predictions = np.array([])\n","      all_labels = np.array([])\n","\n","      with torch.no_grad():\n","          for i, batch_v in enumerate(val_dataloader, 0):\n","\n","              c5g_f = batch_v['c5g_f'].to('cuda')\n","              bert_f = batch_v['bert_f'].to('cuda')\n","              genre = batch_v['genre'].to('cuda')\n","              targets = batch_v['label'].to('cuda')\n","\n","              outputs = model(c5g_f.float(), bert_f.float(), genre.float())\n","              _, predicted = torch.max(outputs.data, 1)\n","\n","              all_predictions = np.append(all_predictions, predicted.cpu().numpy())\n","              all_labels = np.append(all_labels, targets.cpu().numpy())\n","\n","              loss = criterion(outputs, targets)\n","              val_loss += loss.cpu().numpy()\n","              val_steps += 1\n","\n","      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n","          print(\"saving in checkpoint dir\")\n","          path = os.path.join(checkpoint_dir, \"checkpoint\")\n","          torch.save((model.state_dict(), optimizer.state_dict()), path)\n","\n","      s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","      print('s_precision:', s_precision, 's_recall:', s_recall, 's_f1:', s_f1)\n","      # tune.report(loss=(val_loss / val_steps), f1=s_f1, precision=s_precision, recall=s_recall)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nM6WLGVqxIDY"},"source":["def test_results(net, test_dataloader, device=\"cpu\"):\n","  all_predictions = np.array([])\n","  all_labels = np.array([])\n","\n","  net.to(device)\n","  net.eval()\n","  with torch.no_grad():\n","    for i, batch_test in enumerate(test_dataloader, 0):\n","        c5g_f = batch_test['c5g_f']\n","        bert_f = batch_test['bert_f']\n","        genre = batch_test['genre']\n","        targets = batch_test['label']\n","\n","        outputs = net(c5g_f.float(), bert_f.float(), genre.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions = np.append(all_predictions, predicted.numpy())\n","        all_labels = np.append(all_labels, targets.numpy())\n","\n","  s_precision, s_recall, s_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n","\n","  return {\n","      'precision': s_precision,\n","      'recall': s_recall,\n","      'f1': s_f1\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIqVS_dlBVbK"},"source":["from ray import tune\n","from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.tune.integration.wandb import WandbLogger, WandbLoggerCallback\n","from ray.tune.schedulers import ASHAScheduler\n","from functools import partial\n","\n","def main(num_samples = 6, max_num_epochs = 15):\n","  print(\"***test\")\n","  tune_config = {\n","    \"lr\": tune.loguniform(5e-4, 5e-2),\n","    \"batch_size\": tune.choice([32,64,128]),\n","    \"num_epochs\": tune.choice([1,3,5,7,9]),\n","    \"std_dims\": tune.sample_from(lambda _: np.random.randint(50,300)),\n","    \"num_units\": tune.sample_from(lambda spec: np.random.randint(25,spec.config.std_dims)),\n","    \"wandb\": {\n","      \"project\": \"MultiModalClassifier\",\n","      \"api_key\": config['WandB']['api_key'],\n","      \"log_config\": True\n","    }\n","  }\n","\n","  print(\"***test2\")\n","  scheduler = ASHAScheduler(\n","    max_t=max_num_epochs,\n","    grace_period=1,\n","    reduction_factor=2)\n","\n","  print(\"***test3\")\n","  result = tune.run(\n","    partial(mm_train_fun1, checkpoint_dir='/tmp/MMModels'),\n","    config = tune_config,\n","    resources_per_trial={'gpu': 1},\n","    metric = 'loss',\n","    mode = 'min',\n","    num_samples = num_samples,\n","    scheduler = scheduler,\n","    callbacks=[WandbLoggerCallback(\n","        project=\"MultiModalClassifier\",\n","        group='raytune_hpsearch',\n","        api_key=config['WandB']['api_key'],\n","        log_config=True\n","    )])\n","  \n","  print(\"***test4\")\n","\n","  best_trial = result.get_best_trial(metric=\"f1\", mode=\"max\", scope=\"last\")\n","  print(\"Best trial config: {}\".format(best_trial.config))\n","  print(\"Best trial final validation loss: {}\".format(\n","      best_trial.last_result[\"loss\"]))\n","  print(\"Best trial final validation accuracy: {}\".format(\n","      best_trial.last_result[\"f1\"]))\n","  \n","  best_trained_model = FullModel(311595, 768, best_trial.config['std_dims'], best_trial.config['num_units'])\n","  device = \"cpu\"\n","\n","  best_trained_model.to(device)\n","                        \n","  best_checkpoint_dir = best_trial.checkpoint.value\n","  model_state, optimizer_state = torch.load(os.path.join(\n","      best_checkpoint_dir, \"checkpoint\"))\n","  best_trained_model.load_state_dict(model_state)\n","\n","  # model_save_name = \"yungclassifier.pt\"\n","  path = F\"/content/drive/MyDrive/Thesis/BookSuccessPredictor/saved_models/classifier1.pt\"\n","  torch.save(best_trained_model.state_dict(), path)\n","  return test_results(best_trained_model, test_dataloader, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"px4KGuLsFyX7","executionInfo":{"status":"error","timestamp":1628569136750,"user_tz":420,"elapsed":17564,"user":{"displayName":"Luca Guarro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSVZPJxC-6rJfC9ZK-bceU_7VuBnoBQg1qMN4Fng=s64","userId":"08879576481210952349"}},"outputId":"8aafe93b-3c8c-4e5c-ea31-c37e345f08ba"},"source":["test_results = main(num_samples = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-08-10 04:18:38,809\tWARNING experiment.py:296 -- No name detected on trainable. Using DEFAULT.\n","2021-08-10 04:18:38,813\tINFO registry.py:67 -- Detected unknown callable for trainable. Converting to class.\n"],"name":"stderr"},{"output_type":"stream","text":["***test\n","***test2\n","***test3\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 1.5/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.29 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-10_04-18-38<br>Number of trials: 1/1 (1 PENDING)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_09bb2_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00227609</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         34</td><td style=\"text-align: right;\">        70</td></tr>\n","</tbody>\n","</table><br><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Exception ignored in: <function WandbLoggerCallback.__del__ at 0x7f65cf337830>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/integration/wandb.py\", line 378, in __del__\n","    for trial in self._trial_processes:\n","RuntimeError: dictionary changed size during iteration\n","Exception ignored in: <function WandbLoggerCallback.__del__ at 0x7f65cf337830>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/integration/wandb.py\", line 378, in __del__\n","    for trial in self._trial_processes:\n","RuntimeError: dictionary changed size during iteration\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m 2021-08-10 04:18:45,776\tERROR function_runner.py:254 -- Runner Thread raised error.\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"<ipython-input-16-9b988ec30457>\", line 8, in mm_train_fun1\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m ModuleNotFoundError: No module named 'MultimodalGoodreadsDataset'\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m Exception in thread Thread-2:\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m Traceback (most recent call last):\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     self.run()\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 267, in run\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     raise e\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     self._entrypoint()\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     self._status_reporter.get_checkpoint())\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m     output = fn()\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m   File \"<ipython-input-16-9b988ec30457>\", line 8, in mm_train_fun1\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m ModuleNotFoundError: No module named 'MultimodalGoodreadsDataset'\n","\u001b[2m\u001b[36m(pid=884)\u001b[0m \n","2021-08-10 04:18:45,975\tERROR trial_runner.py:773 -- Trial DEFAULT_09bb2_00000: Error processing event.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\", line 739, in _process_trial\n","    results = self.trial_executor.fetch_result(trial)\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 729, in fetch_result\n","    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 1564, in get\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=884, ip=172.28.0.2)\n","  File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n","  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n","    return method(__ray_actor, *args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 178, in train_buffered\n","    result = self.train()\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 237, in train\n","    result = self.step()\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 366, in step\n","    self._report_thread_runner_error(block=True)\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n","    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n","ray.tune.error.TuneError: Trial raised an exception. Traceback:\n","\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=884, ip=172.28.0.2)\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n","    self._entrypoint()\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n","    self._status_reporter.get_checkpoint())\n","  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n","    output = fn()\n","  File \"<ipython-input-16-9b988ec30457>\", line 8, in mm_train_fun1\n","ModuleNotFoundError: No module named 'MultimodalGoodreadsDataset'\n","2021-08-10 04:18:56,012\tWARNING util.py:164 -- The `process_trial` operation took 10.039 s, which may be a performance bottleneck.\n"],"name":"stderr"},{"output_type":"stream","text":["Result for DEFAULT_09bb2_00000:\n","  {}\n","  \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 1.6/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.29 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-10_04-18-38<br>Number of trials: 1/1 (1 ERROR)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_09bb2_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00227609</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         34</td><td style=\"text-align: right;\">        70</td></tr>\n","</tbody>\n","</table><br>Number of errored trials: 1<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_09bb2_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/DEFAULT_2021-08-10_04-18-38/DEFAULT_09bb2_00000_0_batch_size=32,lr=0.0022761,num_epochs=1,num_units=34,std_dims=70_2021-08-10_04-18-38/error.txt</td></tr>\n","</tbody>\n","</table><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["== Status ==<br>Memory usage on this node: 1.6/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.29 GiB heap, 0.0/3.65 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/DEFAULT_2021-08-10_04-18-38<br>Number of trials: 1/1 (1 ERROR)<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  num_units</th><th style=\"text-align: right;\">  std_dims</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_09bb2_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00227609</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">         34</td><td style=\"text-align: right;\">        70</td></tr>\n","</tbody>\n","</table><br>Number of errored trials: 1<br><table>\n","<thead>\n","<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                        </th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DEFAULT_09bb2_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/DEFAULT_2021-08-10_04-18-38/DEFAULT_09bb2_00000_0_batch_size=32,lr=0.0022761,num_epochs=1,num_units=34,std_dims=70_2021-08-10_04-18-38/error.txt</td></tr>\n","</tbody>\n","</table><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"TuneError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-53302c092ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-bf3b2948fadb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raytune_hpsearch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WandB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlog_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     )])\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [DEFAULT_09bb2_00000])"]}]},{"cell_type":"markdown","metadata":{"id":"ququ0pe__LHo"},"source":["Learning the genre vectors from Wg, try to understand if some genres are near each other are not using some distance metric (euclidean or manhattan). Can also do PCA."]},{"cell_type":"code","metadata":{"id":"rdLIKbL_jmpi"},"source":["from sklearn.metrics import precision_recall_fscore_support "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7poTQSxv30G"},"source":["s_f1"],"execution_count":null,"outputs":[]}]}