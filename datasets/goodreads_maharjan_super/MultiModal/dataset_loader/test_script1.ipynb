{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('virtual_workspace': conda)"
  },
  "interpreter": {
   "hash": "f299ee3012df6080f57de79881e3826058f9cfc9301d8d13f6355475feceabda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from readers.goodreads import GoodreadsReader\r\n",
    "from readers.corpus import Corpus\r\n",
    "from features.utils import fetch_features_vectorized\r\n",
    "\r\n",
    "def label_extractor(book_data):\r\n",
    "    return book_data.success\r\n",
    "\r\n",
    "features = [\"char_5_gram\", \"bert_features\"]\r\n",
    "\r\n",
    "# reader = GoodreadsReader(r'C:\\Users\\lucag\\Google Drive\\Thesis\\UsefulRelatedProjects\\curr_sota\\data\\raw_text')\r\n",
    "reader = GoodreadsReader(r'G:\\My Drive\\Thesis\\UsefulRelatedProjects\\curr_sota\\data\\raw_text')\r\n",
    "\r\n",
    "corpus = Corpus.from_splitfile(reader, '../../raw_text/train_test_val_split_goodreads.yaml', label_extractor)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total test instances: 290, validation instances: 290, and Training instances: 404\n",
      "Total unique books: 984\n",
      "Training instances (404,), Val instances (290,), Test instances (290,)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "g:\\My Drive\\Thesis\\BookSuccessPredictor\\datasets\\goodreads_maharjan_super\\MultiModal\\dataset_loader\\readers\\corpus.py:61: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(stream)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "corpus.X_train[0].genre"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Detective_and_mystery_stories'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "feature_dic = fetch_features_vectorized('./cached_features', features, corpus)\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "Path('./cached_features').exists()\r\n",
    "feature_dic"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bert_features\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<404x377185 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 11006134 stored elements in Compressed Sparse Row format>,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " <290x377185 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 7492197 stored elements in Compressed Sparse Row format>,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]),\n",
       " <290x377185 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 7504316 stored elements in Compressed Sparse Row format>,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(feature_dic[0][0,377184-769:377185])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 2)\t-0.343386173248291\n",
      "  (0, 3)\t0.5202162861824036\n",
      "  (0, 4)\t0.3808419108390808\n",
      "  (0, 5)\t-0.02856644243001938\n",
      "  (0, 6)\t0.4367370009422302\n",
      "  (0, 7)\t0.803038477897644\n",
      "  (0, 8)\t0.0014930168399587274\n",
      "  (0, 9)\t-0.35344067215919495\n",
      "  (0, 10)\t-0.2789946496486664\n",
      "  (0, 11)\t-0.3335440754890442\n",
      "  (0, 12)\t-0.32410508394241333\n",
      "  (0, 13)\t-0.25759559869766235\n",
      "  (0, 14)\t-0.43711477518081665\n",
      "  (0, 15)\t-0.42432713508605957\n",
      "  (0, 16)\t0.5314561128616333\n",
      "  (0, 17)\t0.18932518362998962\n",
      "  (0, 18)\t0.464493066072464\n",
      "  (0, 19)\t-0.36042264103889465\n",
      "  (0, 20)\t0.47384971380233765\n",
      "  (0, 21)\t0.001913672429509461\n",
      "  (0, 22)\t0.7433455586433411\n",
      "  (0, 23)\t-0.047287467867136\n",
      "  (0, 24)\t-0.40754178166389465\n",
      "  (0, 25)\t0.4398742914199829\n",
      "  (0, 26)\t-0.2494587004184723\n",
      "  :\t:\n",
      "  (0, 745)\t0.08691953122615814\n",
      "  (0, 746)\t0.4817403554916382\n",
      "  (0, 747)\t0.375184565782547\n",
      "  (0, 748)\t0.11179772019386292\n",
      "  (0, 749)\t0.4755815863609314\n",
      "  (0, 750)\t0.6935405135154724\n",
      "  (0, 751)\t0.3913550078868866\n",
      "  (0, 752)\t0.4019980728626251\n",
      "  (0, 753)\t0.10883190482854843\n",
      "  (0, 754)\t-0.864895761013031\n",
      "  (0, 755)\t0.38439470529556274\n",
      "  (0, 756)\t0.03394608199596405\n",
      "  (0, 757)\t-0.4514492452144623\n",
      "  (0, 758)\t-0.30156558752059937\n",
      "  (0, 759)\t-0.290010929107666\n",
      "  (0, 760)\t-0.13023939728736877\n",
      "  (0, 761)\t0.12477579712867737\n",
      "  (0, 762)\t-0.22712291777133942\n",
      "  (0, 763)\t0.17329324781894684\n",
      "  (0, 764)\t-0.5236007571220398\n",
      "  (0, 765)\t1.1644737720489502\n",
      "  (0, 766)\t-0.14758647978305817\n",
      "  (0, 767)\t0.7147431373596191\n",
      "  (0, 768)\t0.5103347897529602\n",
      "  (0, 769)\t0.6136907935142517\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from pytorch_dataset_loader import MultimodalGoodreadsDataset\r\n",
    "\r\n",
    "a = MultimodalGoodreadsDataset()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total test instances: 290, validation instances: 290, and Training instances: 404\n",
      "Total unique books: 984\n",
      "Training instances (404,), Val instances (290,), Test instances (290,)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "g:\\My Drive\\Thesis\\BookSuccessPredictor\\datasets\\goodreads_maharjan_super\\MultiModal\\dataset_loader\\readers\\corpus.py:61: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(stream)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-85ec8fece75d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_dataset_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultimodalGoodreadsDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultimodalGoodreadsDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mg:\\My Drive\\Thesis\\BookSuccessPredictor\\datasets\\goodreads_maharjan_super\\MultiModal\\dataset_loader\\pytorch_dataset_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_splitfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../../raw_text/train_test_val_split_goodreads.yaml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_extractor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtrain_genres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_genres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_genres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_genre_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_features_vectorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./cached_features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}